<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>机器学习导论 | hicode365</title>
  <meta name="description" content="引言在传统的编程范式中，我们输入数据和规则，通过计算机得到答案。而机器学习（Machine Learning）则不同，我们向计算机输入数据和答案（即最终的结果），让计算机自己去发现其中的学习模型、发现规律，从而构建能够进行预测的数学模型。简而言之，机器学习是一门研究如何让计算机从数据中自动获取规律，并利用这些规律来预测未知、辅助决策的科学。 机器学习是一门交叉学科，根据不同的视角可以被划分为多种范">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习导论">
<meta property="og:url" content="https://www.hicode365.com/cuidh5iGsdDS3Iq6RawHOWB5J">
<meta property="og:site_name" content="hicode">
<meta property="og:description" content="引言在传统的编程范式中，我们输入数据和规则，通过计算机得到答案。而机器学习（Machine Learning）则不同，我们向计算机输入数据和答案（即最终的结果），让计算机自己去发现其中的学习模型、发现规律，从而构建能够进行预测的数学模型。简而言之，机器学习是一门研究如何让计算机从数据中自动获取规律，并利用这些规律来预测未知、辅助决策的科学。 机器学习是一门交叉学科，根据不同的视角可以被划分为多种范">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2026-02-22T01:35:12.017Z">
<meta property="article:modified_time" content="2026-02-22T07:05:54.959Z">
<meta property="article:author" content="hicode365">
<meta property="article:tag" content="分类">
<meta property="article:tag" content="判别式模型">
<meta property="article:tag" content="回归">
<meta property="article:tag" content="生成式模型">
<meta property="article:tag" content="贝叶斯学派">
<meta property="article:tag" content="频率学派">
<meta name="twitter:card" content="summary">
  <!-- Canonical links -->
  <link rel="canonical" href="https://www.hicode365.com/cuidh5iGsdDS3Iq6RawHOWB5J.html">
  
    <link rel="alternate" href="/atom.xml" title="hicode" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
    <link href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.css" rel="stylesheet">
  
  
<meta name="generator" content="Hexo 8.1.1"></head>


<body class="main-center theme-black# 主题颜色 theme-black theme-blue theme-green theme-purple" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://www.hicode365.com/" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.png" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">hicode</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">Algorithms Engineer</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Peking, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav menu-highlight">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">关于</span>
          </a>
        </li>
        
      </ul>
      
    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/docker/">docker</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/git/">git</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9E%E6%88%98/">工程与实战</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ml/">机器学习</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/SVM/" style="font-size: 13px;">SVM</a> <a href="/tags/docker/" style="font-size: 13px;">docker</a> <a href="/tags/git/" style="font-size: 13px;">git</a> <a href="/tags/python/" style="font-size: 13px;">python</a> <a href="/tags/tfserving/" style="font-size: 13px;">tfserving</a> <a href="/tags/%E5%88%86%E7%B1%BB/" style="font-size: 13px;">分类</a> <a href="/tags/%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B/" style="font-size: 13px;">判别式模型</a> <a href="/tags/%E5%91%BD%E4%BB%A4/" style="font-size: 13px;">命令</a> <a href="/tags/%E5%9B%9E%E5%BD%92/" style="font-size: 13px;">回归</a> <a href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E7%A7%AF/" style="font-size: 13px;">支持向量积</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/" style="font-size: 13px;">数据类型</a> <a href="/tags/%E6%A0%B8%E5%87%BD%E6%95%B0/" style="font-size: 13px;">核函数</a> <a href="/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B/" style="font-size: 13px;">生成式模型</a> <a href="/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E6%B4%BE/" style="font-size: 13px;">贝叶斯学派</a> <a href="/tags/%E8%BD%AF%E9%97%B4%E9%9A%94/" style="font-size: 13px;">软间隔</a> <a href="/tags/%E9%A2%91%E7%8E%87%E5%AD%A6%E6%B4%BE/" style="font-size: 13px;">频率学派</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled ">
        
          <li>
            
            <div class="item-thumb">
              <a href="/cuid9ineUcg_C5I5d5AUzIiOJ" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/ml/">机器学习</a>
              </p>
              <p class="item-title">
                <a href="/cuid9ineUcg_C5I5d5AUzIiOJ" class="title">SVM支持向量机</a>
              </p>
              <p class="item-date" style="display:none;">
                <time datetime="2026-02-22T09:48:39.749Z" itemprop="datePublished">2026-02-22</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-thumb">
              <a href="/cuidh5iGsdDS3Iq6RawHOWB5J" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/ml/">机器学习</a>
              </p>
              <p class="item-title">
                <a href="/cuidh5iGsdDS3Iq6RawHOWB5J" class="title">机器学习导论</a>
              </p>
              <p class="item-date" style="display:none;">
                <time datetime="2026-02-22T01:35:12.017Z" itemprop="datePublished">2026-02-22</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-thumb">
              <a href="/cuidDZEK6Bgd97PUqFs0w-F_G" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9E%E6%88%98/">工程与实战</a>
              </p>
              <p class="item-title">
                <a href="/cuidDZEK6Bgd97PUqFs0w-F_G" class="title">python中的数据类型</a>
              </p>
              <p class="item-date" style="display:none;">
                <time datetime="2023-08-15T02:10:06.000Z" itemprop="datePublished">2023-08-15</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-thumb">
              <a href="/1c41d072cac63725dbd48a1273b2fd97" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/docker/">docker</a>
              </p>
              <p class="item-title">
                <a href="/1c41d072cac63725dbd48a1273b2fd97" class="title">docker常用命令</a>
              </p>
              <p class="item-date" style="display:none;">
                <time datetime="2022-11-27T14:28:39.000Z" itemprop="datePublished">2022-11-27</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-thumb">
              <a href="/cuidrRz6-aJ020Ll6zs2_iuIT" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/git/">git</a>
              </p>
              <p class="item-title">
                <a href="/cuidrRz6-aJ020Ll6zs2_iuIT" class="title">git常用命令</a>
              </p>
              <p class="item-date" style="display:none;">
                <time datetime="2021-09-28T03:05:35.000Z" itemprop="datePublished">2021-09-28</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
  <aside class="sidebar sidebar-toc collapse   in  " id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    <nav id="toc" class="article-toc">
      <h3 class="toc-title">文章目录</h3>
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8C%89%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD%E5%88%92%E5%88%86"><span class="toc-number">2.</span> <span class="toc-text">按统计推断划分</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%91%E7%8E%87%E5%AD%A6%E6%B4%BE"><span class="toc-number">2.1.</span> <span class="toc-text">频率学派</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-number">2.1.1.</span> <span class="toc-text">核心思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="toc-number">2.1.2.</span> <span class="toc-text">最大似然估计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%91%E7%8E%87%E6%B4%BE%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">2.1.3.</span> <span class="toc-text">频率派与机器学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%91%E7%8E%87%E5%AD%A6%E6%B4%BE%E7%9A%84%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1"><span class="toc-number">2.1.4.</span> <span class="toc-text">频率学派的参数估计</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E6%B4%BE"><span class="toc-number">2.2.</span> <span class="toc-text">贝叶斯学派</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-1"><span class="toc-number">2.2.1.</span> <span class="toc-text">核心思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86%E4%B8%8E%E5%90%8E%E9%AA%8C%E5%88%86%E5%B8%83"><span class="toc-number">2.2.2.</span> <span class="toc-text">贝叶斯定理与后验分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1"><span class="toc-number">2.2.3.</span> <span class="toc-text">最大后验估计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E9%A2%84%E6%B5%8B"><span class="toc-number">2.2.4.</span> <span class="toc-text">贝叶斯预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B4%BE%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">2.2.5.</span> <span class="toc-text">贝叶斯派与机器学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E6%B4%BE%E7%9A%84%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1"><span class="toc-number">2.2.6.</span> <span class="toc-text">贝叶斯学派的参数估计</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%91%E7%8E%87%E6%B4%BE%E4%B8%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B4%BE%E5%AF%B9%E6%AF%94"><span class="toc-number">2.3.</span> <span class="toc-text">频率派与贝叶斯派对比</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E9%80%89%E6%8B%A9"><span class="toc-number">2.4.</span> <span class="toc-text">应用场景选择</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8C%89%E5%BB%BA%E6%A8%A1%E7%9B%AE%E6%A0%87%E5%88%92%E5%88%86"><span class="toc-number">3.</span> <span class="toc-text">按建模目标划分</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%A4%E5%88%AB%E5%BC%8F"><span class="toc-number">3.1.</span> <span class="toc-text">判别式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%BC%8F"><span class="toc-number">3.2.</span> <span class="toc-text">生成式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%A4%E5%88%AB%E5%BC%8F%E4%B8%8E%E7%94%9F%E6%88%90%E5%BC%8F%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94"><span class="toc-number">3.3.</span> <span class="toc-text">判别式与生成式的模型对比</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E4%B8%8E%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">3.4.</span> <span class="toc-text">生成与判别式模型的选择</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8C%89%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87%E5%88%92%E5%88%86"><span class="toc-number">4.</span> <span class="toc-text">按学习目标划分</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89"><span class="toc-number">4.1.</span> <span class="toc-text">回归（Regression）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%EF%BC%88Classification%EF%BC%89"><span class="toc-number">4.2.</span> <span class="toc-text">分类（Classification）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87"><span class="toc-number">4.2.1.</span> <span class="toc-text">目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BB%BA%E6%A8%A1"><span class="toc-number">4.2.2.</span> <span class="toc-text">建模</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%A4%E5%88%AB%E5%BC%8F%E5%AD%A6%E4%B9%A0%E5%BB%BA%E6%A8%A1%EF%BC%88Discriminative%EF%BC%89"><span class="toc-number">4.2.2.1.</span> <span class="toc-text">判别式学习建模（Discriminative）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%BC%8F%E5%AD%A6%E4%B9%A0%E5%BB%BA%E6%A8%A1%EF%BC%88Generative%EF%BC%89"><span class="toc-number">4.2.2.2.</span> <span class="toc-text">生成式学习建模（Generative）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1"><span class="toc-number">4.2.3.</span> <span class="toc-text">参数估计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%91%E7%8E%87%E6%B4%BE%E7%9A%84%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%EF%BC%88MLE%EF%BC%89"><span class="toc-number">4.2.3.1.</span> <span class="toc-text">频率派的最大似然估计（MLE）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B4%BE%E7%9A%84%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1%EF%BC%88MAP%EF%BC%89"><span class="toc-number">4.2.3.2.</span> <span class="toc-text">贝叶斯派的最大后验估计（MAP）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B"><span class="toc-number">4.2.4.</span> <span class="toc-text">完整流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%EF%BC%88Clustering%EF%BC%89"><span class="toc-number">4.3.</span> <span class="toc-text">聚类（Clustering）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%8D%E7%BB%B4%EF%BC%88Dimensionality-Reduction%EF%BC%89"><span class="toc-number">4.4.</span> <span class="toc-text">降维（Dimensionality Reduction）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%92%E5%BA%8F%EF%BC%88Ranking%EF%BC%89"><span class="toc-number">4.5.</span> <span class="toc-text">排序（Ranking）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E8%AF%AD"><span class="toc-number">5.</span> <span class="toc-text">结语</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B%E5%88%92%E5%88%86%E4%B8%80%E8%A7%88%E8%A1%A8"><span class="toc-number">6.</span> <span class="toc-text">常见模型划分一览表</span></a></li></ol>
    </nav>
  </div>
</aside>

<main class="main" role="main">
  <div class="content">
  <article id="post-机器学习/机器学习导论" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      机器学习导论
    </h1>
  

      
      <div class="article-meta">
        <!-- <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/cuidh5iGsdDS3Iq6RawHOWB5J" class="article-date">
	  <time datetime="2026-02-22T01:35:12.017Z" itemprop="datePublished">2026-02-22</time>
	</a>
</span> -->
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/ml/">机器学习</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/%E5%88%86%E7%B1%BB/" rel="tag">分类</a>, <a class="article-tag-link-link" href="/tags/%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B/" rel="tag">判别式模型</a>, <a class="article-tag-link-link" href="/tags/%E5%9B%9E%E5%BD%92/" rel="tag">回归</a>, <a class="article-tag-link-link" href="/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B/" rel="tag">生成式模型</a>, <a class="article-tag-link-link" href="/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E6%B4%BE/" rel="tag">贝叶斯学派</a>, <a class="article-tag-link-link" href="/tags/%E9%A2%91%E7%8E%87%E5%AD%A6%E6%B4%BE/" rel="tag">频率学派</a>
  </span>


        

        <!-- <span class="post-comment"><i class="icon icon-comment"></i> <a href="/cuidh5iGsdDS3Iq6RawHOWB5J#comments" class="article-comment-link">评论</a></span> -->
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>在传统的编程范式中，我们输入数据和规则，通过计算机得到答案。而机器学习（Machine Learning）则不同，我们向计算机输入数据和答案（即最终的结果），让计算机自己去发现其中的学习模型、发现规律，从而构建能够进行预测的数学模型。简而言之，<strong>机器学习是一门研究如何让计算机从数据中自动获取规律，并利用这些规律来预测未知、辅助决策的科学。</strong></p>
<p>机器学习是一门交叉学科，根据不同的视角可以被划分为多种范式，如： </p>
<p>从<strong>统计推断的哲学思想</strong>划分，机器学习模型可以划分为：<strong>频率学派与贝叶斯学派</strong>。前者认为模型的参数是固定但未知的“常数”，学习的过程就是通过优化方法寻找这个最优值；而后者则认为参数本身服从某种分布，学习的过程是在观察数据后，对先验信念进行更新。</p>
<p>若从<strong>建模的最终目标</strong>划分，算法模型又可分为<strong>判别式模型与生成式模型</strong>。判别式模型关注的是寻找不同类别之间的决策边界，直接学习如何划分数据；而生成式模型则试图理解数据本身的产生过程，通过学习联合分布来间接进行预测。</p>
<h1 id="按统计推断划分"><a href="#按统计推断划分" class="headerlink" title="按统计推断划分"></a>按统计推断划分</h1><h2 id="频率学派"><a href="#频率学派" class="headerlink" title="频率学派"></a>频率学派</h2><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>频率派（Frequentist）将概率解释为事件在长期重复试验中发生的频率。在这一框架下，模型参数 $ \theta $ 被视为一个确定的未知常量，虽然我们不知道它的具体值，但它本身并不具有随机性。我们只能通过观测数据来估计这个常量的值。</p>
<h3 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h3><p>给定观测数据集 $ X $，所有样本的联合概率为：</p>
<p>$$
p(X|\theta) = \prod_{i=1}^N p(x_i|\theta)
$$</p>
<p>这个联合概率被称为<strong>似然函数</strong>（Likelihood Function），它衡量了在特定参数 $ \theta $ 下观测到当前数据的可能性。频率派的估计方法就是寻找能使似然函数最大化的参数值，即<strong>最大似然估计</strong>（Maximum Likelihood Estimation, MLE）：</p>
<p>$$
\theta_{MLE} = \arg\max_{\theta} p(X|\theta) = \arg\max_{\theta} \sum_{i=1}^N \log p(x_i|\theta)
$$</p>
<p>由于对数函数是单调递增的，且能将连乘转化为连加便于计算，实际中通常使用对数似然。</p>
<h3 id="频率派与机器学习"><a href="#频率派与机器学习" class="headerlink" title="频率派与机器学习"></a>频率派与机器学习</h3><p>频率派的思想深刻影响了<strong>统计机器学习</strong>的发展。许多经典算法都可以从MLE的角度理解：</p>
<ul>
<li><strong>线性回归</strong>：假设误差服从高斯分布时，MLE等价于最小二乘法</li>
<li><strong>逻辑回归</strong>：直接对条件概率 $ p(y|x) $ 进行MLE估计</li>
<li><strong>支持向量机</strong>：可以看作是在特定损失函数下的频率派方法</li>
</ul>
<p>频率派方法的优势在于其理论简洁性、计算效率高，且在大样本条件下具有良好的渐近性质。然而，它也存在明显的局限性：无法融入先验知识，在小样本情况下容易过拟合，且只能给出点估计而非分布估计。</p>
<hr>
<h3 id="频率学派的参数估计"><a href="#频率学派的参数估计" class="headerlink" title="频率学派的参数估计"></a>频率学派的参数估计</h3><table>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>全称</strong></th>
<th><strong>核心思想</strong></th>
<th><strong>适用场景</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>MLE</strong></td>
<td>最大似然估计</td>
<td>$\hat{\theta}_{MLE} = \arg\max P(D \mid \theta)$   寻找使观测数据出现概率最大的参数</td>
<td><strong>最通用</strong>。大样本下具有渐近正态性、一致性。对模型假设敏感。</td>
</tr>
<tr>
<td><strong>MM</strong></td>
<td>矩估计法</td>
<td>令样本矩等于总体矩：$E[X^k] = \frac{1}{n}\sum X_i^k$</td>
<td>简单快速，不需要知道分布的具体形式。但在小样本下往往不是有效估计量。</td>
</tr>
<tr>
<td><strong>LSE</strong></td>
<td>最小二乘估计</td>
<td>$\min \sum (y_i - \hat{y}_i)^2$   最小化残差平方和</td>
<td>线性回归。<strong>不需要概率分布假设</strong>（若误差服从正态分布，则等价于 MLE）。</td>
</tr>
<tr>
<td><strong>GMM</strong></td>
<td>广义矩估计</td>
<td>利用过定矩条件（矩条件多于参数），最小化二次型距离</td>
<td><strong>经济计量学常用</strong>。解决了内生性问题，且不需要对误差分布做强假设。</td>
</tr>
<tr>
<td><strong>M-估计</strong></td>
<td>稳健估计</td>
<td>替换平方损失函数为更抗噪的 $\rho$ 函数（如 Huber 损失）</td>
<td><strong>数据有异常值（Outliers）时</strong>。比 MLE 更鲁棒。</td>
</tr>
<tr>
<td><strong>EM</strong></td>
<td>期望极大算法</td>
<td>交替进行 E-step（求期望）和 M-step（极大化似然）</td>
<td><strong>含有隐变量</strong>（Latent variables）的概率模型，如 GMM 聚类、HMM。</td>
</tr>
</tbody></table>
<h2 id="贝叶斯学派"><a href="#贝叶斯学派" class="headerlink" title="贝叶斯学派"></a>贝叶斯学派</h2><h3 id="核心思想-1"><a href="#核心思想-1" class="headerlink" title="核心思想"></a>核心思想</h3><p>贝叶斯派（Bayesian）将概率解释为对事件发生的不确定性的度量，这种不确定性可以是主观的信念。在这一框架下，参数 $ \theta $ 被视为一个随机变量，服从某个先验分布 $ p(\theta) $，这个先验分布反映了我们在看到数据之前对参数的认知。</p>
<h3 id="贝叶斯定理与后验分布"><a href="#贝叶斯定理与后验分布" class="headerlink" title="贝叶斯定理与后验分布"></a>贝叶斯定理与后验分布</h3><p>当我们观测到数据 $ X $ 后，根据贝叶斯定理，我们可以更新对参数的认知，得到<strong>后验分布</strong>（Posterior Distribution）：</p>
<p>$$
p(\theta|X) = \frac{p(X|\theta) \cdot p(\theta)}{p(X)} = \frac{p(X|\theta) \cdot p(\theta)}{\int p(X|\theta) \cdot p(\theta) \, d\theta}
$$</p>
<p>其中：</p>
<ul>
<li>$ p(X|\theta) $ 是似然函数，与频率派中的定义相同</li>
<li>$ p(\theta) $ 是先验分布，体现了我们对参数的主观先验知识</li>
<li>$ p(X) = \int p(X|\theta)p(\theta)d\theta $ 是边缘似然，也称为证据（Evidence），作为归一化常数确保后验概率之和为1</li>
</ul>
<h3 id="最大后验估计"><a href="#最大后验估计" class="headerlink" title="最大后验估计"></a>最大后验估计</h3><p>虽然贝叶斯派的完整结果是整个后验分布，但在实际应用中有时也需要一个点估计。这时可以采用<strong>最大后验估计</strong>（Maximum A Posteriori, MAP）：</p>
<p>$$
\theta_{MAP} = \arg\max_{\theta} p(\theta|X) = \arg\max_{\theta} p(X|\theta) \cdot p(\theta) \tag{4}
$$</p>
<p>MAP估计巧妙地结合了似然函数和先验信息。从优化的角度看，先验分布起到了<strong>正则化</strong>的作用：例如，高斯先验对应L2正则化，拉普拉斯先验对应L1正则化。</p>
<h3 id="贝叶斯预测"><a href="#贝叶斯预测" class="headerlink" title="贝叶斯预测"></a>贝叶斯预测</h3><p>贝叶斯方法真正的优势在于其能够进行概率预测。当我们得到后验分布后，对于新样本 $ x_{new} $ 的预测分布可以通过对参数空间进行积分得到：</p>
<p>$$
p(x_{new}|X) = \int p(x_{new}|\theta) \cdot p(\theta|X) \, d\theta \tag{5}
$$</p>
<p>这个过程被称为<strong>贝叶斯模型平均</strong>（Bayesian Model Averaging）。与频率派只使用单一最优参数不同，贝叶斯方法考虑了所有可能的参数值，并依据后验概率进行加权平均，从而自然地体现了模型的不确定性。</p>
<h3 id="贝叶斯派与机器学习"><a href="#贝叶斯派与机器学习" class="headerlink" title="贝叶斯派与机器学习"></a>贝叶斯派与机器学习</h3><p>贝叶斯派思想催生了**概率图模型（Probabilistic Graphical Models）**这一重要领域，包括：</p>
<ul>
<li><strong>朴素贝叶斯分类器</strong>：最简单的贝叶斯模型，假设特征条件独立</li>
<li><strong>高斯过程</strong>：非参数贝叶斯方法，用于回归和分类</li>
<li><strong>贝叶斯神经网络</strong>：为神经网络权重赋予先验分布</li>
</ul>
<p>贝叶斯方法的优势在于：能够自然地融合先验知识、防止过拟合、提供不确定性估计、适用于小样本学习。但同时也面临挑战：后验分布的计算通常涉及高维积分，难以解析求解，需要借助近似方法如马尔可夫链蒙特卡洛（MCMC）、变分推断等。</p>
<h3 id="贝叶斯学派的参数估计"><a href="#贝叶斯学派的参数估计" class="headerlink" title="贝叶斯学派的参数估计"></a>贝叶斯学派的参数估计</h3><table>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>核心思想</strong></th>
<th><strong>输出形式</strong></th>
<th><strong>特点</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>MAP</strong></td>
<td>最大后验估计   $\arg\max P(D \mid \theta) P(\theta)$</td>
<td><strong>点估计</strong>   (单个数值)</td>
<td>相当于 <strong>MLE + 先验</strong>。在线性回归中增加高斯先验即等价于 <strong>L2 正则化 (Ridge)</strong>。</td>
</tr>
<tr>
<td><strong>贝叶斯推断</strong></td>
<td>计算完整的后验分布   $P(\theta \mid D) = \frac{P(D \mid \theta) P(\theta)}{\int P(D \mid \theta) P(\theta) d\theta}$</td>
<td><strong>后验分布</strong></td>
<td>提供不确定性度量（均值、方差）。当先验是<strong>共轭先验</strong>时有解析解。</td>
</tr>
<tr>
<td><strong>MCMC</strong></td>
<td>马尔可夫链蒙特卡洛采样 (如 Gibbs 采样)</td>
<td><strong>采样样本点集</strong></td>
<td>通过随机采样模拟复杂的后验分布。<strong>万能但慢</strong>，适用于高维复杂模型。</td>
</tr>
<tr>
<td><strong>变分推断 (VI)</strong></td>
<td>将推断问题转化为<strong>优化问题</strong>   寻找一个简单分布 $q(\theta)$ 最小化 $KL(q \| p$</td>
<td><strong>近似解析分布</strong></td>
<td>牺牲了一定精度换取速度。是大规模深度贝叶斯学习（如 VAE）的核心。</td>
</tr>
</tbody></table>
<h2 id="频率派与贝叶斯派对比"><a href="#频率派与贝叶斯派对比" class="headerlink" title="频率派与贝叶斯派对比"></a>频率派与贝叶斯派对比</h2><p>尽管存在哲学分歧，两大学派在方法论上有着深刻的联系：</p>
<ul>
<li><strong>MLE与MAP的关系</strong>：当先验分布为均匀分布时，MAP估计退化为MLE估计</li>
<li><strong>正则化视角</strong>：MLE + 正则化项 等价于 MAP（特定先验）</li>
<li><strong>大样本一致性</strong>：当样本量趋于无穷时，后验分布会集中在MLE估计附近，贝叶斯估计渐近等价于频率派估计</li>
</ul>
<p>现代机器学习中，两大学派的界限正变得越来越模糊。研究者们开始汲取两者的优势：</p>
<ul>
<li><strong>贝叶斯深度学习</strong>：将贝叶斯思想引入深度神经网络，解决过拟合和不确定性估计问题</li>
<li><strong>集成学习</strong>：结合多个模型的思想与贝叶斯模型平均有异曲同工之妙</li>
<li><strong>概率编程</strong>：提供灵活的框架来表达和求解概率模型</li>
<li><strong>变分自编码器（VAE）</strong>：巧妙结合了深度学习与变分推断</li>
</ul>
<h2 id="应用场景选择"><a href="#应用场景选择" class="headerlink" title="应用场景选择"></a>应用场景选择</h2><p>在实际问题中，选择哪种方法取决于具体需求：</p>
<ul>
<li><strong>数据量巨大</strong>：频率派方法通常计算更高效</li>
<li><strong>小样本或零样本学习</strong>：贝叶斯方法可以利用先验知识</li>
<li><strong>需要不确定性估计</strong>：贝叶斯方法自然提供</li>
<li><strong>模型解释性要求高</strong>：简单频率派模型更易解释</li>
<li><strong>计算资源有限</strong>：频率派方法通常更友好</li>
</ul>
<h1 id="按建模目标划分"><a href="#按建模目标划分" class="headerlink" title="按建模目标划分"></a>按建模目标划分</h1><h2 id="判别式"><a href="#判别式" class="headerlink" title="判别式"></a>判别式</h2><ul>
<li><strong>直接</strong>对后验概率 <code>P(y | x)</code> 进行建模。</li>
<li><strong>关注点</strong>：直接学习决策边界，即不同类别之间的界限。</li>
<li><strong>例子</strong>：逻辑回归、神经网络、支持向量机。</li>
<li><strong>比喻</strong>：学会直接区分狗和猫的图片（只看区别）。</li>
</ul>
<h2 id="生成式"><a href="#生成式" class="headerlink" title="生成式"></a>生成式</h2><ul>
<li><strong>间接</strong>地对<strong>似然 (Likelihood)</strong> <code>P(x | y)</code> 和<strong>先验 (Prior)</strong> <code>P(y)</code> 进行建模。</li>
<li><strong>关注点</strong>：为每个类别单独建模其特征的分布。“生成式”是因为一旦学到了 <code>P(x | y)</code>，就可以为任何类别 <code>y</code> 生成新的样本 <code>x</code>。</li>
<li><strong>步骤</strong>：<ol>
<li>为每个类别 <code>y</code> 假设一个特征分布模型（例如高斯分布）。</li>
<li>从训练数据中估计每个类别分布的参数（如均值、方差）。</li>
<li>利用贝叶斯定理，将学到的 <code>P(x | y)</code> 和 <code>P(y)</code> 转换为最终用于分类的 <code>P(y | x)</code>。</li>
</ol>
</li>
<li><strong>比喻</strong>：分别学习“狗看起来是什么样”和“猫看起来是什么样”的完整模型，然后对于一个新动物，看它更符合哪个模型。</li>
</ul>
<h2 id="判别式与生成式的模型对比"><a href="#判别式与生成式的模型对比" class="headerlink" title="判别式与生成式的模型对比"></a>判别式与生成式的模型对比</h2><p><strong>判别模型</strong> 和 <strong>生成模型</strong> 的根本区别在于它们<strong>解决问题的思路和关注点</strong>不同。</p>
<ul>
<li><p><strong>判别模型</strong> 致力于 <strong>“找到区别”</strong>。</p>
<ul>
<li><strong>思路</strong>：直接学习不同类别数据之间的<strong>决策边界</strong>，而不关心单个类别本身的具体样貌。</li>
<li><strong>目标</strong>：回答“<strong>它更像是猫还是狗？</strong>”</li>
<li><strong>好比</strong>：一个裁判，他不需要会画画，只需要掌握一个关键标准（比如身长）来快速区分两者。</li>
</ul>
</li>
<li><p><strong>生成模型</strong> 致力于 <strong>“理解本质”</strong>。</p>
<ul>
<li><strong>思路</strong>：分别学习每一类数据（如猫、狗）的<strong>整体特征和内部结构</strong>，为每个类别建立一个完整的“概念模型”。</li>
<li><strong>目标</strong>：回答“<strong>猫&#x2F;狗长什么样子？</strong>”</li>
<li><strong>好比</strong>：一个艺术家，他需要透彻地了解猫和狗的骨骼、肌肉、毛发等所有细节，才能把它们画出来。</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th align="left">特征</th>
<th align="left"><strong>判别模型（Discriminative）</strong></th>
<th align="left"><strong>生成模型（Generative）</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>核心思想</strong></td>
<td align="left">学习类别之间的<strong>边界</strong>，找到“差异”。</td>
<td align="left">学习数据本身的<strong>分布</strong>，理解每一类的“本质”。</td>
</tr>
<tr>
<td align="left"><strong>解决的问题</strong></td>
<td align="left">这是X还是Y</td>
<td align="left">什么是X</td>
</tr>
<tr>
<td align="left"><strong>学习内容</strong></td>
<td align="left">条件概率p(y|x)   （x是数据特征，y是数据标签）</td>
<td align="left">联合概率p(x,y)、分布p(x)（无监督的情况）</td>
</tr>
<tr>
<td align="left"><strong>能力</strong></td>
<td align="left">主要用于<strong>分类和回归</strong>，无法生成新数据。</td>
<td align="left">既可以进行分类，也可以<strong>生成新的数据</strong>（如画一只猫）。</td>
</tr>
<tr>
<td align="left"><strong>类比</strong></td>
<td align="left">只学会一个<strong>投机取巧的判别技巧</strong>（如比身长）。</td>
<td align="left">学完后对猫狗有<strong>直观认知</strong>，能画出它们。</td>
</tr>
<tr>
<td align="left"><strong>常见算法</strong></td>
<td align="left">逻辑回归、支持向量机、决策树、CRF、神经网络</td>
<td align="left">朴素贝叶斯、高斯混合、隐马尔可夫模型、VAF、生成对抗网络</td>
</tr>
</tbody></table>
<p><strong>判别模型因为只专注于区分，因此不具备生成能力；而生成模型因为学会了数据的“本质”，所以具备生成新数据的能力。</strong></p>
<p>ChatGPT、Midjourney这类能创作内容（生成文本、图像）的模型，其核心都是<strong>生成模型</strong>。而许多用于图像分类、垃圾邮件过滤等任务的模型，则更多是<strong>判别模型</strong>。</p>
<h2 id="生成与判别式模型的选择"><a href="#生成与判别式模型的选择" class="headerlink" title="生成与判别式模型的选择"></a>生成与判别式模型的选择</h2><ul>
<li>当数据量较少或对数据分布有较强先验知识时，<strong>生成学习方法</strong>可能更有效。<ul>
<li>适用于数据生成、缺失值处理；</li>
<li>在小样本下可能比判别模型（如逻辑回归）更鲁棒。</li>
</ul>
</li>
<li>当数据量充足且计算资源有限时，判别学习方法可能更合适。</li>
</ul>
<h1 id="按学习目标划分"><a href="#按学习目标划分" class="headerlink" title="按学习目标划分"></a>按学习目标划分</h1><h2 id="回归（Regression）"><a href="#回归（Regression）" class="headerlink" title="回归（Regression）"></a>回归（Regression）</h2><ul>
<li>回归旨在建立输入特征与连续型输出变量之间的映射关系，回归任务的目标是预测一个<strong>连续的数值输出</strong> $y \in \mathbb{R}$，</li>
<li><strong>核心思想</strong>：找到一条曲线&#x2F;超平面，使预测值 $\hat{y}$ 尽可能接近真实值 $y$，最小化预测误差: $ \min_\theta \sum_{i=1}^N (y_i - \hat{y}_i)^2 $</li>
<li>应用场景：预测房价、售额预测、股票价格等。</li>
</ul>
<p>常见的回归算法:</p>
<ul>
<li><strong>线性回归</strong>：最简单的回归模型，假设输入与输出呈线性关系</li>
<li><strong>岭回归&#x2F;Lasso</strong>：在线性回归基础上加入L2&#x2F;L1正则化，防止过拟合</li>
<li><strong>决策树回归</strong>：通过树结构分段拟合数据</li>
<li><strong>随机森林回归</strong>：集成多棵决策树，降低方差</li>
<li><strong>梯度提升回归</strong>（GBDT、XGBoost）：通过迭代优化残差</li>
<li><strong>支持向量回归（SVR）</strong>：利用核方法处理非线性关系</li>
<li><strong>神经网络</strong>：能够拟合任意复杂的非线性函数</li>
</ul>
<h2 id="分类（Classification）"><a href="#分类（Classification）" class="headerlink" title="分类（Classification）"></a>分类（Classification）</h2><ul>
<li>目标是预测离散的类别标签（Y）。</li>
<li>例如：判断邮件是否为垃圾邮件（类别：垃圾邮件&#x2F;非垃圾邮件）、识别图像中的物体类别（类别：猫&#x2F;狗&#x2F;车等）。</li>
</ul>
<p>预测离散类别标签。根据类别数量可分为：</p>
<ul>
<li><strong>二分类</strong>： 只有两个类别，例如垃圾邮件分类。</li>
<li><strong>多分类</strong>：包含多个类别，例如手写数字识别（0-9）。</li>
<li><strong>多标签分类</strong>：一个样本可能属于多个类别</li>
</ul>
<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>给定输入特征向量 $\mathbf{x} \in \mathbb{R}^d$，分类问题的目标是预测其所属的<strong>离散类别标签</strong> $y \in \{1, 2, ..., K\}$。</p>
<p>从概率视角看，我们需要计算<strong>后验概率</strong>（posterior probability）：<br>$$
P(y = k \mid \mathbf{x}), \quad k = 1,2,...,K
$$</p>
<p>所有分类模型最终都采用<strong>最大后验决策规则</strong>（MAP decision rule）进行预测：<br>$$
\hat{y} = \arg\max_{k} P(y = k \mid \mathbf{x})
$$<br>即选择使后验概率最大的类别作为预测结果。</p>
<h3 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h3><h4 id="判别式学习建模（Discriminative）"><a href="#判别式学习建模（Discriminative）" class="headerlink" title="判别式学习建模（Discriminative）"></a>判别式学习建模（Discriminative）</h4><ul>
<li><strong>直接建模</strong> $P(y|\mathbf{x})$</li>
<li>关注决策边界，不关心数据本身的分布</li>
</ul>
<h4 id="生成式学习建模（Generative）"><a href="#生成式学习建模（Generative）" class="headerlink" title="生成式学习建模（Generative）"></a>生成式学习建模（Generative）</h4><ul>
<li><strong>间接建模</strong>：先学习联合分布 $ P(\mathbf{x}, y) = P(y)P(\mathbf{x}|y) $</li>
<li>通过贝叶斯定理计算后验：$ P(y|\mathbf{x}) = \frac{P(y)P(\mathbf{x}|y)}{\sum_{k=1}^K P(y=k)P(\mathbf{x}|y=k)} $</li>
</ul>
<h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><p>无论采用判别式还是生成式方法，模型都包含需要从数据中学习的参数 $\theta$。参数估计主要有两种方法：</p>
<h4 id="频率派的最大似然估计（MLE）"><a href="#频率派的最大似然估计（MLE）" class="headerlink" title="频率派的最大似然估计（MLE）"></a>频率派的<strong>最大似然估计（MLE）</strong></h4><p>​		将参数视为确定的未知常量，寻找使训练数据似然函数最大化的参数值：<br>$$
\theta_{MLE} = \arg\max_\theta \prod_{i=1}^N P(y_i|\mathbf{x}_i, \theta)
$$<br>​		等价于最小化经验风险。</p>
<h4 id="贝叶斯派的最大后验估计（MAP）"><a href="#贝叶斯派的最大后验估计（MAP）" class="headerlink" title="贝叶斯派的最大后验估计（MAP）"></a>贝叶斯派的<strong>最大后验估计（MAP）</strong></h4><p>​		将参数视为随机变量，在MLE的基础上引入了参数的先验分布$p(\theta)$，通过贝叶斯定理求使<strong>后验概率</strong>最大的参数值：<br>$$
\theta_{MAP} = \arg\max_\theta \prod_{i=1}^N P(y_i|\mathbf{x}_i, \theta) \cdot p(\theta)
$$</p>
<ul>
<li><strong>正则化效应</strong>：先验分布 $p(\theta)$ 等价于对参数施加约束，防止过拟合<ul>
<li>高斯先验 $p(\theta) \sim \mathcal{N}(0, \sigma^2)$ ↔ L2 正则化</li>
<li>拉普拉斯先验 $p(\theta) \sim \text{Laplace}(0, b)$ ↔ L1 正则化</li>
</ul>
</li>
<li>在数据稀缺时，先验知识可提供更稳健的估计</li>
<li>当先验为均匀分布时，MAP 退化为 MLE</li>
</ul>
<h3 id="完整流程"><a href="#完整流程" class="headerlink" title="完整流程"></a>完整流程</h3><ol>
<li><strong>训练阶段</strong>：使用MLE或MAP估计模型参数 $\hat{\theta}$</li>
<li><strong>预测阶段</strong>：对于新样本 $\mathbf{x}_{new}$，计算后验概率：$P(y=k|\mathbf{x}_{new}, \hat{\theta})$</li>
<li><strong>决策阶段</strong>：应用最大后验决策规则：$\hat{y} = \arg\max_k P(y=k|\mathbf{x}_{new}, \hat{\theta})$</li>
</ol>
<h2 id="聚类（Clustering）"><a href="#聚类（Clustering）" class="headerlink" title="聚类（Clustering）"></a>聚类（Clustering）</h2><p>聚类是一种<strong>无监督学习</strong>任务，目标是将未标注的样本划分为若干个<strong>簇</strong>（cluster），使得同一簇内的样本尽可能相似，不同簇间的样本尽可能不同。</p>
<p>给定数据集 $\{\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_N\}$，聚类算法旨在找到一种划分 $C = \{C_1, C_2, ..., C_K\}$，使得：</p>
<ul>
<li>簇内相似度最大化：$\max \sum_{k=1}^K \sum_{\mathbf{x}_i \in C_k} \text{sim}(\mathbf{x}_i, \mu_k)$</li>
<li>簇间相似度最小化：$\min \sum_{k \lt j} \text{sim}(\mu_k, \mu_j)$，其中 $\mu_k$ 是第 $k$ 个簇的中心（或代表点），$\text{sim}(\cdot)$ 是相似度度量。</li>
</ul>
<h2 id="降维（Dimensionality-Reduction）"><a href="#降维（Dimensionality-Reduction）" class="headerlink" title="降维（Dimensionality Reduction）"></a>降维（Dimensionality Reduction）</h2><p>降维是将高维数据映射到低维空间的过程，同时尽可能保留原始数据的重要结构信息。它是<strong>无监督学习</strong>的重要分支，也是数据预处理的关键步骤。</p>
<p>给定高维数据 $\{\mathbf{x}_i \in \mathbb{R}^D\}_{i=1}^N$，降维算法寻找映射 $f: \mathbb{R}^D \rightarrow \mathbb{R}^d$，其中 $d \ll D$，使得：<br>$$\mathbf{z}_i = f(\mathbf{x}_i) \in \mathbb{R}^d$$<br>且 $\{\mathbf{z}_i\}$ 尽可能保留 $\{\mathbf{x}_i\}$ 的重要结构信息。</p>
<p>需要降维的场景:</p>
<ul>
<li><strong>维度灾难（Curse of Dimensionality）</strong>：随着维度增加，数据变得稀疏，距离度量失效，模型复杂度指数级增长</li>
<li><strong>可视化需求</strong>：人类只能理解2D或3D空间，降维使高维数据可视化成为可能</li>
<li><strong>计算效率</strong>：减少特征数量，降低模型训练和预测的时间复杂度</li>
<li><strong>去噪</strong>：丢弃不重要的维度，保留主要信号</li>
<li><strong>特征提取</strong>：从原始特征中构造更有代表性的新特征</li>
</ul>
<h2 id="排序（Ranking）"><a href="#排序（Ranking）" class="headerlink" title="排序（Ranking）"></a>排序（Ranking）</h2><p>排序任务的目标是学习一个<strong>排序函数</strong>，能够根据输入特征对项目集合进行排序，使得<strong>相关性高</strong>的项目排在前面。排序问题是信息检索和推荐系统的核心。</p>
<p>排序的应用场景:</p>
<ul>
<li><strong>搜索引擎</strong>：根据用户查询对网页进行排序</li>
<li><strong>推荐系统</strong>：为用户推荐最感兴趣的商品、视频、音乐</li>
<li><strong>问答系统</strong>：对候选答案按相关性排序</li>
<li><strong>广告点击率预测</strong>：排序广告以提高点击率</li>
<li><strong>社交媒体</strong>：排序信息流中的帖子</li>
</ul>
<h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>机器学习的发展，本质上是在<strong>表示、评估与优化</strong>这三个核心要素上的持续演进：选择何种方式表示数据与模型，如何定义优劣，以及通过何种策略寻找最优解。</p>
<p><strong>深度学习的崛起</strong>摒弃了人工特征工程，通过分层抽象自动从原始数据中提取层次化特征，使得模型能够感知高维空间中的复杂结构。然而，强大的表示能力也伴随着挑战——海量参数需要海量数据，黑箱特性损害了可解释性。</p>
<p><strong>强化学习的复兴</strong>则拓展了学习的范式。它不再满足于拟合静态数据，而是在与环境的交互中通过试错学习最优策略，将机器学习从模式识别推向序贯决策。这种范式更接近生物学习的本质，在机器人、博弈、控制等领域展现出独特价值。</p>
<p>然而，技术的飞速发展机器学习仍面临多方面的挑战：</p>
<ul>
<li><strong>数据效率</strong>：人类可以从少量样本中学习，而当前模型仍然依赖海量数据。如何让机器学习像人类一样“举一反三”，仍是未解难题。</li>
<li><strong>鲁棒性与泛化</strong>：分布外泛化、对抗样本、虚假相关——模型在实验室环境外的表现往往不尽如人意。</li>
<li><strong>可解释性与可信赖</strong>：随着模型进入医疗、金融、司法等高风险领域，黑箱决策的风险日益凸显。</li>
<li><strong>价值对齐</strong>：当模型越来越强大，如何确保其目标与人类价值观一致，成为关乎未来的重要课题。</li>
<li><strong>计算效率与可持续性</strong>：大模型的训练消耗惊人，如何在性能与能耗之间取得平衡，既是技术问题，也是环境问题。</li>
</ul>
<h1 id="常见模型划分一览表"><a href="#常见模型划分一览表" class="headerlink" title="常见模型划分一览表"></a>常见模型划分一览表</h1><table>
<thead>
<tr>
<th><strong>算法模型</strong></th>
<th><strong>频率派</strong></th>
<th><strong>贝叶斯派</strong></th>
<th><strong>判别式</strong></th>
<th><strong>生成式</strong></th>
<th><strong>参数估计&#x2F;核心准则</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>线性回归</strong></td>
<td>是</td>
<td>是 (贝叶斯线性回归)</td>
<td>是</td>
<td>否</td>
<td>最小二乘 (OLS) &#x2F; MLE</td>
</tr>
<tr>
<td><strong>逻辑回归</strong></td>
<td>是</td>
<td>是 (拉普拉斯近似等)</td>
<td>是</td>
<td>否</td>
<td>极大似然估计 (MLE)</td>
</tr>
<tr>
<td><strong>决策树&#x2F;随机森林&#x2F;提升树</strong></td>
<td>是</td>
<td>否</td>
<td>是</td>
<td>否</td>
<td>信息增益 &#x2F; 基尼系数 &#x2F; MSE</td>
</tr>
<tr>
<td><strong>SVM</strong></td>
<td>是</td>
<td>否</td>
<td>是</td>
<td>否</td>
<td>合页损失 + 正则化 (对偶优化)</td>
</tr>
<tr>
<td><strong>PCA (主成分分析)</strong></td>
<td>是</td>
<td>是 (Probabilistic PCA)</td>
<td>不适用</td>
<td>是 (概率视角下)</td>
<td>方差最大化 &#x2F; 投影误差最小化</td>
</tr>
<tr>
<td><strong>LDA (线性判别分析)</strong></td>
<td>是</td>
<td>否</td>
<td>否</td>
<td><strong>是</strong></td>
<td>Fisher 准则 &#x2F; 类内类间散度</td>
</tr>
<tr>
<td><strong>高斯判别分析 (GDA)</strong></td>
<td>是</td>
<td>是</td>
<td>否</td>
<td><strong>是</strong></td>
<td>联合概率 $P(x,y)$ 建模</td>
</tr>
<tr>
<td><strong>高斯过程 (GP)</strong></td>
<td>否</td>
<td><strong>是</strong></td>
<td>是</td>
<td>否</td>
<td>核函数 + 边际似然最大化</td>
</tr>
<tr>
<td><strong>高斯混合模型 (GMM)</strong></td>
<td>是</td>
<td>是 (贝叶斯 GMM)</td>
<td>否</td>
<td><strong>是</strong></td>
<td><strong>EM 算法</strong></td>
</tr>
<tr>
<td><strong>朴素贝叶斯</strong></td>
<td>是</td>
<td><strong>是</strong></td>
<td>否</td>
<td><strong>是</strong></td>
<td>MAP (带平滑) &#x2F; MLE</td>
</tr>
<tr>
<td><strong>贝叶斯网络</strong></td>
<td>否</td>
<td><strong>是</strong></td>
<td>否</td>
<td><strong>是</strong></td>
<td>结构学习 + 条件概率表</td>
</tr>
<tr>
<td><strong>隐马尔可夫 (HMM)</strong></td>
<td>是</td>
<td>是 (变分推断)</td>
<td>否</td>
<td><strong>是</strong></td>
<td>EM 算法</td>
</tr>
<tr>
<td><strong>条件随机场 (CRF)</strong></td>
<td>是</td>
<td>否</td>
<td><strong>是</strong></td>
<td>否</td>
<td>极大似然 (梯度上升)</td>
</tr>
<tr>
<td><strong>感知机</strong></td>
<td>是</td>
<td>否</td>
<td>是</td>
<td>否</td>
<td>随机梯度下降 (SGD)</td>
</tr>
<tr>
<td><strong>深度神经网络 (DNN)</strong></td>
<td>是</td>
<td>是 (BNN)</td>
<td>是</td>
<td>视模型而定</td>
<td>反向传播 (BP) + 优化器</td>
</tr>
</tbody></table>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="https://www.hicode365.com/cuidh5iGsdDS3Iq6RawHOWB5J" title="机器学习导论" target="_blank" rel="external">https://www.hicode365.com/cuidh5iGsdDS3Iq6RawHOWB5J</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://www.hicode365.com/" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.png" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://www.hicode365.com/" target="_blank"><span class="text-dark">hicode</span><small class="ml-1x">Algorithms Engineer</small></a></h3>
        <div>Open shared and free</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/cuid9ineUcg_C5I5d5AUzIiOJ" title="SVM支持向量机"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/cuidDZEK6Bgd97PUqFs0w-F_G" title="python中的数据类型"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
    <li class="toggle-toc">
      <a class="toggle-btn " data-toggle="collapse" href="#collapseToc" aria-expanded="false" title="文章目录" role="button">    <span>[&nbsp;</span><span>文章目录</span>
        <i class="text-collapsed icon icon-anchor"></i>
        <i class="text-in icon icon-close"></i>
        <span>]</span>
      </a>
    </li>
    
  </ul>
  
  
  <!-- Button trigger modal -->
  <button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button>
  <!-- <div class="wave-icon wave-icon-danger btn-donate" data-toggle="modal" data-target="#donateModal">
    <div class="wave-circle"><span class="icon"><i class="icon icon-bill"></i></span></div>
  </div> -->
  
  
  <div class="bar-right">
    
  </div>
  </div>
</nav>
  
<!-- Modal -->
<div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog">
  <div class="modal-dialog" role="document">
    <div class="modal-content donate">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
      <div class="modal-body">
        <div class="donate-box">
          <div class="donate-head">
            <p>感谢您的支持，我会继续努力的!</p>
          </div>
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade active in" id="alipay">
              <div class="donate-payimg">
                <img src="/images/donate/alipay.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p>
            </div>
            <div role="tabpanel" class="tab-pane fade" id="wechatpay">
              <div class="donate-payimg">
                <img src="/images/donate/wechatpay.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p>
            </div>
          </div>
          <div class="donate-footer">
            <ul class="nav nav-tabs nav-justified" role="tablist">
              <li role="presentation" class="active">
                <a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a>
              </li>
              <li role="presentation" class="">
                <a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>



</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
    <div class="copyright">
    	
        <!-- 隐藏主题版权信息 -->
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>



    <script>
(function ($) {
    $('.search-form').on('submit', function (e) {
        var keyword = $('.search-form-input[name="wd"]').val();
        window.location = 'https://www.baidu.com/s?wd=site:www.hicode365.com ' + keyword;
        return false;
    });
})(jQuery);
</script>




   




   


   <script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$']],
    displayMath: [['$$', '$$']],
    processEscapes: true,
    tags: 'ams'
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
};
</script>
<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



  <script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.js"></script>
  <script>
  //利用 FancyBox 实现点击图片放大
  $(document).ready(function() {
    $('article img').not('[hidden]').not('.panel-body img').each(function() {
      var $image = $(this);
      var imageCaption = $image.attr('alt');
      var $imageWrapLink = $image.parent('a');
      if ($imageWrapLink.length < 1) {
        var src = this.getAttribute('src');
        var idx = src.lastIndexOf('?');
        if (idx != -1) {
          src = src.substring(0, idx);
        }
        $imageWrapLink = $image.wrap('<a href="' + src + '"></a>').parent('a');
      }
      $imageWrapLink.attr('data-fancybox', 'images');
      if (imageCaption) {
        $imageWrapLink.attr('data-caption', imageCaption);
      }
    });
    $().fancybox({
      selector: '[data-fancybox="images"]',
      hash: false,
      loop: false,
    });
  });
  </script>





</body>
</html>