<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>hicode</title>
  
  <subtitle>愿你出走半生 归来仍是少年</subtitle>
  <link href="https://www.hicode365.com/atom.xml" rel="self"/>
  
  <link href="https://www.hicode365.com/"/>
  <updated>2026-02-22T12:07:05.830Z</updated>
  <id>https://www.hicode365.com/</id>
  
  <author>
    <name>hicode365</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SVM支持向量机</title>
    <link href="https://www.hicode365.com/cuid9ineUcg_C5I5d5AUzIiOJ"/>
    <id>https://www.hicode365.com/cuid9ineUcg_C5I5d5AUzIiOJ</id>
    <published>2026-02-22T09:48:39.749Z</published>
    <updated>2026-02-22T12:07:05.830Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>支持向量机（Support Vector Machine, SVM）是一种经典的二分类模型，它的核心思想是：<strong>在特征空间中寻找一个决策边界（超平面），使得两类样本被正确分开，并且离决策边界最近的样本点（即支持向量）到该边界的距离尽可能大</strong>。这个距离被称为<strong>间隔（Margin）</strong>，SVM因此也被称为<strong>最大间隔分类器</strong>。</p><h1 id="决策函数"><a href="#决策函数" class="headerlink" title="决策函数"></a>决策函数</h1><p>$$\left\{(x_1^{(1)}, x_2^{(1)}, \ldots, x_n^{(1)}),(x_1^{(2)}, x_2^{(2)}, \ldots, x_n^{(2)}),\ldots,(x_1^{(m)}, x_2^{(m)}, \ldots, x_n^{(m)})\right\}$$</p><p>这是一个样本集，包含 $ m $ 个样本，每个样本有 $ n $ 个特征。</p><ul><li>第 $ i $ 个样本为：$(x_1^{(i)}, x_2^{(i)}, \ldots, x_n^{(i)})$</li><li>上标 $(i)$ 表示第 $ i $ 个样本</li><li>下标 $ j $ 表示该样本的第 $ j $ 个特征</li><li>所以 $ x_j^{(i)} $ 表示 <strong>第 $ i $ 个样本的第 $ j $ 个特征值</strong></li></ul><p>矩阵表示法(每一行是一个样本，每一列是一个特征)：<br>$$\mathbf{X} =\begin{bmatrix}x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\\vdots   & \vdots   & \ddots & \vdots   \\x_1^{(m)} & x_2^{(m)} & \cdots & x_n^{(m)}\end{bmatrix}$$</p><p>SVM的决策函数为线性函数：</p><p>$$f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x} + b$$</p><p>其中 $\mathbf{w} \in \mathbb{R}^n$ 是权重向量（法向量），$b \in \mathbb{R}$ 是偏置项。决策边界（超平面）由方程 $\mathbf{w}^\top \mathbf{x} + b = 0$ 定义。</p><p>对于二维情况，决策边界是一条直线；对于三维情况，它是一个平面；更高维则称为超平面，如：</p><p>当样本有两个特征时（即2维），决策边界为直线：</p><img src="/images/v2-319cb9929efe5025d7ee029ac7e60098_r.png" alt="v2-319cb9929efe5025d7ee029ac7e60098_r" style="zoom:40%;" />$$w_1x_1 + w_2x_2 + b = 0$$<p>当三个（或多个）特征时，决策边界为平面：<br>$$w_1x_1 + w_2x_2 + w_3x_3 + b = 0$$</p><p>分类规则：把 $x^{(i)}$ 代入决策方程：</p><ul><li><p>得到的值$\mathbf{w}^\top \mathbf{x} + b > 0$：  $y^{(i)}$ 为正例，$y^{(i)} = +1$； </p></li><li><p>得到的值$\mathbf{w}^\top \mathbf{x} + b < 0$ ：  $y^{(i)}$ 为负例，$y^{(i)} = -1$；</p></li></ul><p>因此，正确分类的样本满足 $y^{(i)}(\mathbf{w}^\top \mathbf{x}^{(i)} + b) > 0$。</p><h1 id="间隔（Margin）与优化目标"><a href="#间隔（Margin）与优化目标" class="headerlink" title="间隔（Margin）与优化目标"></a>间隔（Margin）与优化目标</h1><h2 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h2><h3 id="点到直线的距离"><a href="#点到直线的距离" class="headerlink" title="点到直线的距离"></a>点到直线的距离</h3><p>在二维空间中，假设有一条直线的方程为 $ ax + by + c = 0 $，以及一个点 $P(x_0, y_0) $。该点到直线的距离 $d $ 可以通过以下公式计算：</p><p>$$d = \frac{|ax_0 + by_0 + c|}{\sqrt{a^2 + b^2}}$$<br>这里，$a, b, c $ 是直线方程的系数，而 $x_0, y_0 $ 是点的坐标。</p><h3 id="点到平面的距离"><a href="#点到平面的距离" class="headerlink" title="点到平面的距离"></a>点到平面的距离</h3><p>在三维空间中，假设有一个平面的方程为 $Ax + By + Cz + D = 0 $，以及一个点 $P(x_0, y_0, z_0) $。该点到平面的距离 $d $ 可以通过以下公式计算：</p><p>$$d = \frac{|Ax_0 + By_0 + Cz_0 + D|}{\sqrt{A^2 + B^2 + C^2}}$$<br>这里，$A, B, C, D $ 是平面方程的系数，而 $x_0, y_0, z_0 $ 是点的坐标。</p><p>注：</p><ul><li>在这两个公式中，分子部分表示的是点代入直线或平面方程后的值的绝对值，它反映了点相对于直线或平面的位置。</li><li>分母部分是对直线或平面法向量长度的计算（即向量 $(a, b) $ 或 $(A, B, C) $ 的模长），用于标准化距离。</li></ul><h2 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h2><p>函数间隔（Functional Margin）的定义</p><p>对于一个样本 $(\mathbf{x}^{(i)}, y^{(i)})$，其中 $y^{(i)} \in \{+1, -1\}$，我们定义其<strong>函数间隔</strong>为：</p><p>$$\hat{\gamma}_i = y^{(i)} (\mathbf{w}^\top \mathbf{x}^{(i)} + b)$$</p><ul><li>$\mathbf{w}^\top \mathbf{x}^{(i)} + b$ 是模型对样本 $\mathbf{x}^{(i)}$ 的<strong>原始打分（score）</strong>。</li><li>如果分类正确$ y^{(i)}$ 与 $ \mathbf{w}^\top \mathbf{x}^{(i)} + b$ 同号，它的绝对值越大，说明分类的置信度越高。</li></ul><h2 id="几何间隔（优化目标）"><a href="#几何间隔（优化目标）" class="headerlink" title="几何间隔（优化目标）"></a>几何间隔（优化目标）</h2><p>我们以二维决策边界为例：<br>$$w_1x_1 + w_2x_2 + b = 0，其中 \mathbf{w}=[w_1,w_2]$$<br>点到决策边界直线的距离为：<br>$$\gamma_i = d_i= \frac{|w_1x_1^{(i)} + w_2x_1^{(i)} + b|}{\sqrt{w_1^2 + w_2^2}} = y^{(i)} \frac{w_1x_1^{(i)} + w_2x_1^{(i)} + b}{\sqrt{w_1^2 + w_2^2}} = y^{(i)} \frac{\mathbf{wx} + b}{\sqrt{w_1^2 + w_2^2}} =  = y^{(i)} \frac{\mathbf{wx} + b}{\|\mathbf{w}\|}$$</p><p>$\|\mathbf{w}\|$ 为<strong>L2范数</strong>（也称为欧几里得范数）定义为:  $\|\mathbf{w}\| = \sqrt{w_1^2 + w_2^2}$ ， 表示的是<strong>权重向量 $ \mathbf{w} $ 的长度。</strong></p><p>所以几何间隔为：<br>$$\gamma_i = y^{(i)} \frac{\mathbf{wx} + b}{\|\mathbf{w}\|} = \frac{1}{\|\mathbf{w}\|}  y^{(i)}{(\mathbf{wx} + b)} = \frac{1}{\|\mathbf{w}\|} \hat{\gamma}_i$$<br>我们的优化目标是最大化距离决策边界最小的样本的间隔值。</p><h2 id="函数间隔VS几何间隔"><a href="#函数间隔VS几何间隔" class="headerlink" title="函数间隔VS几何间隔"></a>函数间隔VS几何间隔</h2><p>$$\gamma_i = \frac{\hat{\gamma}_i}{\|\mathbf{w}\|}$$</p><table><thead><tr><th>特性</th><th>函数间隔 $\hat{\gamma}_i$</th><th>几何间隔 $\gamma_i$</th></tr></thead><tbody><tr><td>是否依赖 $\mathbf{w}$ 的尺度</td><td>是（可任意缩放）</td><td>否（尺度不变）</td></tr><tr><td>是否表示真实距离</td><td>否</td><td>是</td></tr><tr><td>优化目标</td><td>不直接用于最大化（因可缩放）</td><td>用于 SVM 的核心目标：<strong>最大化最小几何间隔</strong></td></tr></tbody></table><p><strong>示例</strong>：</p><p>假设 $\mathbf{w} = (2, 2), b = -2$，对某个样本 $\mathbf{x} = (1,1), y=+1$：</p><ul><li>函数间隔：$\hat{\gamma} = 1 \cdot (2*1 + 2*1 - 2) = 2$</li><li>几何间隔：$\gamma = 2 / \sqrt{2^2 + 2^2} = 2 / \sqrt{8} = \frac{1}{\sqrt{2}}$</li></ul><p>现在把 $\mathbf{w}, b$ 都乘以 10：</p><ul><li>$\mathbf{w}' = (20,20), b' = -20$</li><li>函数间隔变为：$20$（变大了！）</li><li>但几何间隔仍是：$20 / \sqrt{800} = 20 / (10\sqrt{8}) = 2 / \sqrt{8} = \frac{1}{\sqrt{2}}$</li></ul><p><strong>几何间隔不变，函数间隔随参数缩放而变</strong>。</p><h2 id="最大间隔思想"><a href="#最大间隔思想" class="headerlink" title="最大间隔思想"></a>最大间隔思想</h2><p>SVM的目标是：<strong>找到一个超平面 $(\mathbf{w},b)$，使得所有样本中离它最近的那个样本的几何间隔尽可能大</strong>。这个最近的样本点就是<strong>支持向量（Support Vector）</strong>。</p><p>整个数据集的几何间隔定义为：</p><p>$$\gamma = \min_{i=1,\dots,m} \gamma_i$$</p><p>优化问题即为：</p><p>$$\max_{\mathbf{w},b} \gamma = \max_{\mathbf{w},b} \min_{i} \frac{y^{(i)}(\mathbf{w}^\top \mathbf{x}^{(i)} + b)}{\|\mathbf{w}\|}$$</p><h2 id="目标函数与化简"><a href="#目标函数与化简" class="headerlink" title="目标函数与化简"></a>目标函数与化简</h2><p>SVM 的核心思想是：<strong>找到一个决策边界，使得所有样本中离它最近的那个样本（即支持向量）的距离尽可能大</strong>。</p><ul><li><p>单个样本的几何间隔：<br>$$  \gamma_i = \frac{1}{\|\mathbf{w}\|} y^{(i)}(\mathbf{w}^\top \mathbf{x}_i + b)  $$</p></li><li><p>整个数据集的几何间隔是所有样本中<strong>最小的那个</strong>：<br>$$   \gamma = \min_{i=1,\dots,m} \gamma_i   $$</p></li><li><p>我们的目标是找到能最大化这个最小几何间隔的参数：<br>$$  \max_{\mathbf{w}, b} \gamma = \max_{\mathbf{w}, b} \min_{i=1,\dots,m} \frac{1}{\|\mathbf{w}\|} y^{(i)}(\mathbf{w}^\top \mathbf{x}_i + b)  $$</p></li><li><p>对间隔进行<strong>缩放</strong>：</p><p>性质：对同一超平面，对参数（$\mathbf{w}$ 和 $b$ ）同时进行缩放，几何间隔保持不变，函数间隔发生变化。</p></li></ul><p>$$\gamma_i = \frac{1}{\|\mathbf{w}\|} y^{(i)}(\mathbf{w}^\top \mathbf{x}_i + b)$$</p><ul><li><p>设对任意正实数 $c>0$，把参数变为 $(\mathbf w',b')=(c\mathbf w,c b)$。</p><ul><li><p>超平面方程由 $\mathbf w^\top \mathbf x + b =0$ 变为 $(c\mathbf w)^\top \mathbf x + c b = c(\mathbf w^\top \mathbf x + b)$。零点集不变，因此<strong>超平面本身不变</strong>（同一条直线&#x2F;平面）。</p></li><li><p>函数边距变为<br>$$    \hat\gamma_i' = y_i((c\mathbf w)^\top \mathbf x_i + c b) = c, y_i(\mathbf w^\top \mathbf x_i + b) = c,\hat\gamma_i.    $$<br>所以函数边距按同样因子 (c) 缩放。</p></li><li><p>$\mathbf w 的范数变成 |\mathbf w'|=|c\mathbf w| = c|\mathbf w|$。</p></li><li><p>几何边距变为<br>$$    \gamma_i' = \frac{\hat\gamma_i'}{|\mathbf w'|} = \frac{c\hat\gamma_i}{c|\mathbf w|} = \frac{\hat\gamma_i}{|\mathbf w|} = \gamma_i.    $$</p></li><li><p>也就是说，<strong>几何边距不变，函数间隔变为原来的c倍</strong>。</p></li><li><p>我们对 $\max_{\mathbf{w}, b} \gamma = \max_{\mathbf{w}, b} \min_{i=1,\dots,m} \frac{1}{\|\mathbf{w}\|} y^{(i)}(\mathbf{w}^\top \mathbf{x}_i + b)$ 中的$\frac{1}{\|\mathbf{w}\|} y^{(i)}(\mathbf{w}^\top \mathbf{x}_i + b)$  进行缩放（不会改变原来的结果值）,缩放到 $y^{(i)}(\mathbf{w}^\top \mathbf{x}_i + b) \ge 1$，此时：<br>$$    \begin{aligned}    &\max_{\mathbf{w}, b} \gamma = \max_{\mathbf{w}, b} \min_{i=1,\dots,m} \frac{1}{\|\mathbf{w}\|} y^{(i)}(\mathbf{w}^\top \mathbf{x}_i + b) =  \max_{\mathbf{w}, b} \min_{i=1,\dots,m} \frac{1}{\|\mathbf{w}\|} \cdot 1 = \max_{\mathbf{w}, b}\frac{1}{\|\mathbf{w}\|} = \min_{\mathbf{w}, b}{\|\mathbf{w}\|} =  \min_{\mathbf{w}, b}\frac{1}{2}{\|\mathbf{w}\|}^2 \\    &\text{s.t.}\quad y_i(\mathbf w^\top \mathbf x_i + b) \ge 1,\quad i=1,\dots,m.    \end{aligned}    $$<br>所以，<strong>SVM优化目标的的原始形式为：</strong><br>$$    \begin{aligned}    &\min_{\mathbf{w}, b}\frac{1}{2}{\|\mathbf{w}\|}^2 \\    &\text{s.t.}\quad y_i(\mathbf w^\top \mathbf x_i + b) \ge 1,\quad i=1,\dots,m.    \end{aligned}    $$<br>加入软间隔，在目标里加上松弛项并<strong>引入松弛变量 $\xi_i$ 的优化目标形式为</strong>：<br>$$    \begin{aligned}    &\min \tfrac{1}{2}|\mathbf w|^2 + C\sum_{i=1}^n\xi_i \\    &\text{s.t.}\quad y_i(\mathbf w^\top \mathbf x_i + b) \ge 1-\xi_i,\ \xi_i\ge 0, \quad i=1,\dots,m.    \end{aligned}    $$</p></li></ul></li></ul><h2 id="硬间隔与软间隔"><a href="#硬间隔与软间隔" class="headerlink" title="硬间隔与软间隔"></a>硬间隔与软间隔</h2><img src="/images/ScreenShot_2025-10-25_161439_348.png" alt="ScreenShot_2025-10-25_161439_348" style="zoom: 25%;" /><h3 id="硬间隔"><a href="#硬间隔" class="headerlink" title="硬间隔"></a>硬间隔</h3><ul><li><strong>前提</strong>：数据完全线性可分。</li><li><strong>目标</strong>：找到一个超平面，使得所有样本被正确分类，且间隔最大。</li><li><strong>数学形式</strong>：</li></ul><p>$$\begin{aligned}  &\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \\  &\text{s.t. } y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1,\quad i=1,\dots,m  \end{aligned}$$</p><p>  其中 $y_i \in \{-1, +1\}$</p><h3 id="软间隔"><a href="#软间隔" class="headerlink" title="软间隔"></a>软间隔</h3><ul><li><p><strong>现实情况</strong>：数据可能含噪声或轻微不可分。</p></li><li><p>引入<strong>松弛变量 $\xi_i \geq 0$</strong>，允许部分样本违反约束。</p></li><li><p><strong>目标函数加入正则项</strong>（控制间隔与误分类的权衡）：<br>$$  \begin{aligned}  &\min_{\mathbf{w}, b, \xi} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^m \xi_i \\  &\text{s.t. } y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i,\quad \xi_i \geq 0  \end{aligned}  $$</p><ul><li>$C$ 是<strong>惩罚系数</strong>：$C$ 越大，对误分类越敏感（趋向硬间隔）；$C$ 小则容忍更多误分类。</li><li>如果 $ ξ_i = 0 $，点被正确分类且在间隔之外。</li><li>如果 $ 0 < ξ_i <= 1 $，点被正确分类但在间隔之内。</li><li>如果 $ ξ_i > 1 $，点被错误分类。</li></ul></li></ul><h1 id="拉格朗日对偶求解（硬间隔）"><a href="#拉格朗日对偶求解（硬间隔）" class="headerlink" title="拉格朗日对偶求解（硬间隔）"></a>拉格朗日对偶求解（硬间隔）</h1><h2 id="拉格朗日函数"><a href="#拉格朗日函数" class="headerlink" title="拉格朗日函数"></a>拉格朗日函数</h2><p>上述我们已经得到SVM的优化目标函数为:<br>$$\begin{aligned}  &\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \\  &\text{s.t. } y_i(\mathbf{w}^\top \mathbf{x}_i + b) - 1 \geq 0,\quad i=1,\dots,m  \end{aligned}$$<br>目标函数是一个带不等式约束的凸二次规划（QP）问题。我们可以用拉格朗日乘子法将其转化为一个更易于求解的对偶问题。</p><p><strong>步骤 1： 引入拉格朗日函数</strong></p><p>为每个不等式约束引入一个拉格朗日乘子 $\alpha_i \ge 0$，构建拉格朗日函数：<br>$$\mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha}) = \frac{1}{2} \|\mathbf{w}\|^2 - \sum_{i=1}^m \alpha_i \left[ y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 \right]$$</p><ul><li>$\boldsymbol{\alpha} = (\alpha_1, \dots, \alpha_m)^T$</li><li>从上述式子不难看出：$\mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha}) \leq \frac{1}{2} \|\mathbf{w}\|^2$</li></ul><p>根据拉格朗日乘子法的性质，原始问题等价于以下<strong>无约束的极小极大问题</strong>，所以目标函数转换为：<br>$$\min_{\mathbf{w}, b} \max_{\boldsymbol{\alpha} \ge 0} \mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha})$$<br><strong>步骤 2： 转化为对偶问题（Dual Problem）</strong></p><p>原始问题是 $\min_{\mathbf{w}, b} \max_{\boldsymbol{\alpha} \ge 0} \mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha})$。在满足KKT条件的情况下，我们可以等价地求解它的对偶问题：<br>$$\min_{\mathbf{w}, b} \max_{\boldsymbol{\alpha} \ge 0} \mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha})\ → \\max_{\boldsymbol{\alpha} \ge 0} \min_{\mathbf{w}, b} \mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha})$$</p><p><strong>首先，求解内层的最小化问题：</strong> $\min_{\mathbf{w}, b} \mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha})$</p><p>分别对 $\mathbf{w}$ 和 $b$ 求偏导并令其为零：<br>$$\nabla_{\mathbf{w}} \mathcal{L} = \mathbf{w} - \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i = 0 \quad \Rightarrow \quad \mathbf{w} = \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i$$</p><p>$$\nabla_{\mathbf{b}} \mathcal{L} = \frac{\partial \mathcal{L}}{\partial b} = - \sum_{i=1}^m \alpha_i y_i = 0 \quad \Rightarrow \quad \sum_{i=1}^m \alpha_i y_i = 0$$</p><p>将这两个结果代回拉格朗日函数 $\mathcal{L}$：</p><p>$$\begin{aligned}\mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha}) &= \frac{1}{2} \left( \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i \right)^T \left( \sum_{j=1}^m \alpha_j y_j \mathbf{x}_j \right) - \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i^T \left( \sum_{j=1}^m \alpha_j y_j \mathbf{x}_j \right) - b \sum_{i=1}^m \alpha_i y_i + \sum_{i=1}^m \alpha_i \\&= \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j - \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j + \sum_{i=1}^m \alpha_i \\&= -\frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j + \sum_{i=1}^m \alpha_i\end{aligned}$$<br>（注意：中间项 $- b \sum_{i=1}^m \alpha_i y_i$ 因为 $\sum \alpha_i y_i = 0$ 而消失了。）</p><p><strong>步骤 3： 得到对偶问题</strong></p><p>现在，我们的目标变成了最大化上面这个结果，同时要满足约束：<br>$$\begin{aligned}& \max_{\boldsymbol{\alpha}} \quad -\frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j + \sum_{i=1}^m \alpha_i \\& \text{s.t.} \quad \sum_{i=1}^m \alpha_i y_i = 0, \\& \quad \quad \alpha_i \ge 0, \quad i = 1, \dots, m\end{aligned}$$<br>通常我们将最大化转换为最小化，得到SVM标准对偶问题的最终形式：</p><p>$$\begin{aligned}& \min_{\boldsymbol{\alpha}} \quad \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j - \sum_{i=1}^m \alpha_i \\& \text{s.t.} \quad \sum_{i=1}^m \alpha_i y_i = 0, \\& \quad \quad \alpha_i \ge 0, \quad i = 1, \dots, m\end{aligned}$$</p><p>然后就是根据所有样本的值带入上式（还有约束条件），求得 $\alpha$，再根据 $w$ 与$\alpha$的关系求得 $w$值。</p><p>在根据 $y = \mathbf{w}^\top \mathbf{x}_i + b$  随边找一个样本点带入求得 $b$ 值。</p><p><strong>$\alpha$的意义：</strong></p><p>所有$\alpha_i$ 不为零的点就是支持向量，也就是位于边界上的点，非边界上的点$\alpha = 0$ </p><h2 id="KKT条件与支持向量"><a href="#KKT条件与支持向量" class="headerlink" title="KKT条件与支持向量"></a>KKT条件与支持向量</h2><p>求解上述对偶问题后，我们得到最优的拉格朗日乘子 $\boldsymbol{\alpha}^*$。此时，KKT条件（最优解必须满足的条件）起着关键作用，特别是其中的<strong>互补松弛条件</strong>：<br>$$\alpha_i^* \left[ y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 \right] = 0, \quad \forall i$$<br>这个条件意味着：</p><ol><li>如果 $\alpha_i^* = 0$，那么对应的样本点不会对 $\mathbf{w}$ 的计算产生影响（见 $\mathbf{w} = \sum \alpha_i y_i \mathbf{x}_i$）。</li><li>如果 $\alpha_i^* > 0$，那么必然有 $y_i (\mathbf{w}^T \mathbf{x}_i + b) = 1$。这意味着这个样本点<strong>正好在最大间隔边界上</strong>！</li></ol><p>这些 $\alpha_i^* > 0$ 所对应的样本点，就是<strong>支持向量（Support Vectors）</strong>。它们决定了最终的超平面。</p><p><strong>求解 $\mathbf{w}$ 和 $b$：</strong></p><ul><li><p>$\mathbf{w}$： 我们已经从导数为零的条件得到了：<br>$$  \mathbf{w}^* = \sum_{i=1}^m \alpha_i^* y_i \mathbf{x}_i  $$<br>实际上，我们只需要支持向量（即 $\alpha_i^* > 0$ 的点）来计算：<br>$$  \mathbf{w}^* = \sum_{i \in SV} \alpha_i^* y_i \mathbf{x}_i  $$</p></li><li><p>$b$： 利用任何一个支持向量 $(\mathbf{x}_s, y_s)$（即满足 $\alpha_s^* > 0$ 的点），根据互补松弛条件：<br>$$  y_s (\mathbf{w}^{*T} \mathbf{x}_s + b^*) = 1  $$<br>可以解出：<br>$$  b^* = y_s - \mathbf{w}^{*T} \mathbf{x}_s  $$<br>为了数值稳定性，通常使用所有支持向量计算出的 $b$ 的平均值。</p></li></ul><h1 id="拉格朗日对偶与求解（软间隔）"><a href="#拉格朗日对偶与求解（软间隔）" class="headerlink" title="拉格朗日对偶与求解（软间隔）"></a>拉格朗日对偶与求解（软间隔）</h1><h2 id="松弛变量与惩罚参数"><a href="#松弛变量与惩罚参数" class="headerlink" title="松弛变量与惩罚参数"></a>松弛变量与惩罚参数</h2><p><strong>优化问题（原始问题，Primal Problem with Soft Margin）：</strong></p><p>由于数据可能不是线性可分的，我们引入<strong>松弛变量（Slack Variables）</strong> $\xi_i \ge 0$ 来允许一些样本犯错误。同时，在目标函数中加入对这些错误的惩罚项。优化问题变为：</p><p>$$\begin{aligned}& \min_{\mathbf{w}, b, \boldsymbol{\xi}} \quad \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^m \xi_i \\& \text{s.t.} \quad y_i (\mathbf{w}^T \mathbf{x}_i + b) \ge 1 - \xi_i, \quad i = 1, \dots, m \\& \quad \quad \xi_i \ge 0, \quad i = 1, \dots, m\end{aligned}$$</p><p>这里：</p><ul><li>$\xi_i$ 是松弛变量。$\xi_i = 0$ 表示样本 $x_i$ 被正确分类且位于间隔边界之外；$0 < \xi_i \le 1$ 表示样本位于间隔内部，但在正确的一侧；$\xi_i > 1$ 表示样本被错误分类。</li><li>$C > 0$ 是<strong>惩罚参数</strong>，由用户指定。它控制了我们对分类错误的容忍度：<ul><li>$C$ 越大，对错误的惩罚越重，间隔带越“硬”，可能过拟合。</li><li>$C$ 越小，对错误的惩罚越轻，间隔带越“宽软”，可能欠拟合。</li></ul></li></ul><p><strong>构建拉格朗日函数：</strong></p><p>我们现在有两个不等式约束，因此引入两组拉格朗日乘子：$\alpha_i \ge 0$ 和 $\mu_i \ge 0$。</p><p>$$\mathcal{L}(\mathbf{w}, b, \boldsymbol{\xi}, \boldsymbol{\alpha}, \boldsymbol{\mu}) = \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^m \xi_i - \sum_{i=1}^m \alpha_i \left[ y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 + \xi_i \right] - \sum_{i=1}^m \mu_i \xi_i$$</p><p><strong>转化为对偶问题：</strong></p><p>同样，我们求解 $\min_{\mathbf{w}, b, \boldsymbol{\xi}} \max_{\boldsymbol{\alpha}, \boldsymbol{\mu} \ge 0} \mathcal{L}$ 的对偶问题 $\max_{\boldsymbol{\alpha}, \boldsymbol{\mu} \ge 0} \min_{\mathbf{w}, b, \boldsymbol{\xi}} \mathcal{L}$。</p><ol><li><p><strong>对 $\mathbf{w}, b, \xi_i$ 求偏导并令为零：</strong><br>$$   \nabla_{\mathbf{w}} \mathcal{L} = \mathbf{w} - \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i = 0 \quad \Rightarrow \quad \mathbf{w} = \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i   $$</p><p>$$   \frac{\partial \mathcal{L}}{\partial b} = - \sum_{i=1}^m \alpha_i y_i = 0 \quad \Rightarrow \quad \sum_{i=1}^m \alpha_i y_i = 0   $$</p><p>$$   \frac{\partial \mathcal{L}}{\partial \xi_i} = C - \alpha_i - \mu_i = 0 \quad \Rightarrow \quad \alpha_i + \mu_i = C   $$</p></li><li><p><strong>将结果代回拉格朗日函数：</strong><br>将 $\mathbf{w} = \sum \alpha_i y_i \mathbf{x}_i$， $\mu_i = C - \alpha_i$ 代入 $\mathcal{L}$。经过与硬间隔类似的化简过程（注意包含 $\xi_i$ 的项会相互抵消），我们得到：</p></li></ol><p><strong>对偶问题（Dual Problem with Soft Margin）：</strong><br>$$\boxed{\begin{aligned}& \min_{\boldsymbol{\alpha}} \quad \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j - \sum_{i=1}^m \alpha_i \\& \text{s.t.} \quad \sum_{i=1}^m \alpha_i y_i = 0, \\& \quad \quad 0 \le \alpha_i \le C, \quad i = 1, \dots, m\end{aligned}}$$</p><p><strong>关键变化：</strong> 与硬间隔相比，软间隔的对偶问题唯一的变化是约束条件从 $\alpha_i \ge 0$ 变成了 $0 \le \alpha_i \le C$。这个上界 $C$ 来自于关系式 $\alpha_i = C - \mu_i$ 和 $\mu_i \ge 0$。</p><hr><h2 id="KKT条件与支持向量-1"><a href="#KKT条件与支持向量-1" class="headerlink" title="KKT条件与支持向量"></a>KKT条件与支持向量</h2><p>对于软间隔SVM，KKT条件（最优解必须满足的条件）变得更加丰富，它们完整地描述了支持向量的不同类型。</p><p><strong>KKT条件：</strong></p><ol><li><p><strong>平稳性：</strong> $\mathbf{w} = \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i$, $\sum_{i=1}^m \alpha_i y_i = 0$, $\alpha_i + \mu_i = C$.</p></li><li><p><strong>原始可行性：</strong> $y_i (\mathbf{w}^T \mathbf{x}_i + b) \ge 1 - \xi_i$, $\xi_i \ge 0$.</p></li><li><p><strong>对偶可行性：</strong> $\alpha_i \ge 0$, $\mu_i \ge 0$.</p></li><li><p><strong>互补松弛性：</strong><br>$$   \alpha_i \left[ y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 + \xi_i \right] = 0   $$</p><p>$$   \mu_i \xi_i = 0   $$</p></li></ol><p>根据这些KKT条件，我们可以将样本点分为三类：</p><table><thead><tr><th align="left">$\alpha_i$ 的值</th><th align="left">$\xi_i$ 的值</th><th align="left">几何位置</th><th align="left">类型</th></tr></thead><tbody><tr><td align="left">$\alpha_i = 0$</td><td align="left">$\xi_i = 0$</td><td align="left">被正确分类，<strong>在间隔边界之外</strong></td><td align="left"><strong>非支持向量</strong></td></tr><tr><td align="left">$0 < \alpha_i < C$</td><td align="left">$\xi_i = 0$</td><td align="left">恰好落在<strong>间隔边界</strong>上 ($y_i(\mathbf{w}^T\mathbf{x}_i + b) = 1$)</td><td align="left"><strong>标准支持向量</strong></td></tr><tr><td align="left">$\alpha_i = C$</td><td align="left">$0 < \xi_i \le 1$</td><td align="left">位于<strong>间隔内部</strong>，但在正确的一侧</td><td align="left"><strong>边界支持向量</strong></td></tr><tr><td align="left">$\alpha_i = C$</td><td align="left">$\xi_i > 1$</td><td align="left">在<strong>错误的一侧</strong>，被误分类</td><td align="left"><strong>误分类支持向量</strong></td></tr></tbody></table><p><strong>求解 $\mathbf{w}$ 和 $b$：</strong></p><ul><li><p>$\mathbf{w}^*$ 的求解与硬间隔相同，只依赖于支持向量 ($\alpha_i > 0$)：<br>$$  \mathbf{w}^* = \sum_{i=1}^m \alpha_i^* y_i \mathbf{x}_i = \sum_{i \in SV} \alpha_i^* y_i \mathbf{x}_i  $$</p></li><li><p>$b^*$ 的求解：在硬间隔中，我们使用任意一个支持向量。在软间隔中，为了数值稳定性，我们通常使用<strong>所有标准支持向量</strong> ($0 < \alpha_i < C$) 来计算 $b$ 的平均值。<br>对于任何一个标准支持向量，由于 $\xi_i = 0$ 且 $0 < \alpha_i < C$，根据互补松弛条件有：<br>$$  y_i (\mathbf{w}^{*T} \mathbf{x}_i + b^*) = 1  $$<br>因此，<br>$$  b^* = \frac{1}{|S|} \sum_{i \in S} (y_i - \mathbf{w}^{*T} \mathbf{x}_i)  $$<br>其中 $S = \{ i | 0 < \alpha_i < C \}$ 是所有标准支持向量的集合。</p></li></ul><hr><h1 id="最终的决策函数（软间隔版本）"><a href="#最终的决策函数（软间隔版本）" class="headerlink" title="最终的决策函数（软间隔版本）"></a>最终的决策函数（软间隔版本）</h1><p>软间隔SVM的最终决策函数在形式上与硬间隔完全相同：</p><p>$$f(\mathbf{x}) = \text{sign}(\mathbf{w}^{*T} \mathbf{x} + b^*)$$</p><p>将 $\mathbf{w}^* = \sum_{i \in SV} \alpha_i^* y_i \mathbf{x}_i$ 代入：</p><p>$$\boxed{f(\mathbf{x}) = \text{sign}\left( \sum_{i \in SV} \alpha_i^* y_i \mathbf{x}_i^T \mathbf{x} + b^* \right)}$$</p><p><strong>关键点：</strong></p><ol><li><strong>形式不变</strong>：决策函数仍然是支持向量的线性组合。</li><li><strong>支持向量更多样</strong>：支持向量集合 $SV$ 现在包含了所有 $\alpha_i > 0$ 的样本，即标准支持向量和那些被允许“犯错”的边界&#x2F;误分类支持向量。正是这些“犯错”的样本使得模型获得了鲁棒性。</li><li><strong>核函数应用</strong>：同样，我们可以将内积 $\mathbf{x}_i^T \mathbf{x}$ 替换为核函数 $K(\mathbf{x}_i, \mathbf{x})$，从而将软间隔SVM应用于非线性分类问题，这被称为<strong>非线性软间隔SVM</strong>。</li></ol><h2 id="软间隔-vs-硬间隔"><a href="#软间隔-vs-硬间隔" class="headerlink" title="软间隔 vs.硬间隔"></a>软间隔 vs.硬间隔</h2><table><thead><tr><th align="left">特性</th><th align="left">硬间隔SVM</th><th align="left">软间隔SVM</th></tr></thead><tbody><tr><td align="left"><strong>适用场景</strong></td><td align="left">数据严格线性可分</td><td align="left">数据近似线性可分或存在噪声</td></tr><tr><td align="left"><strong>优化目标</strong></td><td align="left">$\min \frac{1}{2}\|\mathbf{w}\|^2$</td><td align="left">$\min \frac{1}{2}\|\mathbf{w}\|^2 + C \sum \xi_i$</td></tr><tr><td align="left"><strong>对偶约束</strong></td><td align="left">$0 \le \alpha_i$</td><td align="left">$0 \le \alpha_i \le C$</td></tr><tr><td align="left"><strong>支持向量</strong></td><td align="left">都在间隔边界上</td><td align="left">在间隔边界上、内部或误分点</td></tr><tr><td align="left"><strong>参数</strong></td><td align="left">无</td><td align="left">惩罚参数 $C$</td></tr></tbody></table><h1 id="核函数（Kernel-Function）"><a href="#核函数（Kernel-Function）" class="headerlink" title="核函数（Kernel Function）"></a>核函数（Kernel Function）</h1><h2 id="低维不可分与高维映射"><a href="#低维不可分与高维映射" class="headerlink" title="低维不可分与高维映射"></a>低维不可分与高维映射</h2><img src="/images/ScreenShot_2026-02-22_185554_909.png" alt="ScreenShot_2026-02-22_185554_909" style="zoom:40%;" /><p>当数据在原始特征空间中线性不可分时，一个自然的想法是将数据映射到更高维的空间，使其变得线性可分。设映射函数为 $\phi(\mathbf{x})$，则在高维空间中的决策函数为：<br>$$f(\mathbf{x}) = \mathbf{w}^\top \phi(\mathbf{x}) + b$$</p><h2 id="核技巧（Kernel-Trick）"><a href="#核技巧（Kernel-Trick）" class="headerlink" title="核技巧（Kernel Trick）"></a>核技巧（Kernel Trick）</h2><p>直接计算 $\phi(\mathbf{x})$ 可能非常复杂，甚至无限维。但注意到在对偶问题中，所有涉及样本的地方都是以内积形式出现：$(\mathbf{x}^{(i)})^\top \mathbf{x}^{(j)}$。如果存在一个函数 $K(\cdot,\cdot)$，使得：</p><p>$$K(\mathbf{x}^{(i)}, \mathbf{x}^{(j)}) = \phi(\mathbf{x}^{(i)})^\top \phi(\mathbf{x}^{(j)})$$</p><p>那么我们就可以<strong>在低维空间中计算核函数 $K$</strong>，而无需显式地计算高维映射 $\phi$，从而大大降低计算量。这就是<strong>核技巧</strong>。</p><h2 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h2><ul><li><strong>线性核</strong>：$K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^\top \mathbf{x}_j$（即无映射，用于线性可分情况）</li><li><strong>多项式核</strong>：$K(\mathbf{x}_i, \mathbf{x}_j) = (\gamma \mathbf{x}_i^\top \mathbf{x}_j + r)^d$</li><li><strong>高斯核（RBF核）</strong>：$K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2)$（最常用，可将特征映射到无穷维）</li><li><strong>Sigmoid核</strong>：$K(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\gamma \mathbf{x}_i^\top \mathbf{x}_j + r)$</li></ul><p>核函数的选择通常依赖领域知识或交叉验证。</p><h2 id="核函数的选取原则"><a href="#核函数的选取原则" class="headerlink" title="核函数的选取原则"></a>核函数的选取原则</h2><ul><li>若特征数 $n$ 很大，而样本数 $m$ 较小，通常使用线性核（避免过拟合）；</li><li>若 $n$ 适中，$m$ 也适中，可尝试高斯核；</li><li>若 $n$ 很小，$m$ 很大，需要先手工增加特征，再考虑线性核。</li></ul><hr><h1 id="硬间隔、软间隔、核函数对比"><a href="#硬间隔、软间隔、核函数对比" class="headerlink" title="硬间隔、软间隔、核函数对比"></a>硬间隔、软间隔、核函数对比</h1><table><thead><tr><th>特性</th><th>硬间隔SVM</th><th>软间隔SVM</th><th>核SVM</th></tr></thead><tbody><tr><td>适用场景</td><td>数据严格线性可分</td><td>数据近似线性可分或含噪声</td><td>非线性可分数据</td></tr><tr><td>优化目标</td><td>$\min \frac12\|\mathbf{w}\|^2$</td><td>$\min \frac12\|\mathbf{w}\|^2 + C\sum\xi_i$</td><td>在核空间中间接优化</td></tr><tr><td>对偶约束</td><td>$\alpha_i \ge 0$</td><td>$0 \le \alpha_i \le C$</td><td>同上，但内积换为核函数</td></tr><tr><td>支持向量</td><td>仅在间隔边界上</td><td>包括边界、内部、误分类点</td><td>核空间中的支持向量</td></tr><tr><td>关键参数</td><td>无</td><td>惩罚参数 $C$</td><td>核参数（如 $\gamma$）及 $C$</td></tr></tbody></table><p>SVM通过最大化间隔获得良好的泛化能力，引入软间隔和核函数后能处理各种复杂数据，是机器学习中非常经典的算法。理解其背后的对偶推导和KKT条件，有助于深入掌握支持向量机的精髓。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;支持向量机（Support Vector Machine, SVM）是一种经典的二分类模型，它的核心思想是：&lt;strong&gt;在特征空间中寻找</summary>
      
    
    
    
    <category term="机器学习" scheme="https://www.hicode365.com/categories/ml/"/>
    
    
    <category term="SVM" scheme="https://www.hicode365.com/tags/SVM/"/>
    
    <category term="支持向量积" scheme="https://www.hicode365.com/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E7%A7%AF/"/>
    
    <category term="核函数" scheme="https://www.hicode365.com/tags/%E6%A0%B8%E5%87%BD%E6%95%B0/"/>
    
    <category term="软间隔" scheme="https://www.hicode365.com/tags/%E8%BD%AF%E9%97%B4%E9%9A%94/"/>
    
  </entry>
  
  <entry>
    <title>机器学习导论</title>
    <link href="https://www.hicode365.com/cuidh5iGsdDS3Iq6RawHOWB5J"/>
    <id>https://www.hicode365.com/cuidh5iGsdDS3Iq6RawHOWB5J</id>
    <published>2026-02-22T01:35:12.017Z</published>
    <updated>2026-02-22T07:05:54.959Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>在传统的编程范式中，我们输入数据和规则，通过计算机得到答案。而机器学习（Machine Learning）则不同，我们向计算机输入数据和答案（即最终的结果），让计算机自己去发现其中的学习模型、发现规律，从而构建能够进行预测的数学模型。简而言之，<strong>机器学习是一门研究如何让计算机从数据中自动获取规律，并利用这些规律来预测未知、辅助决策的科学。</strong></p><p>机器学习是一门交叉学科，根据不同的视角可以被划分为多种范式，如： </p><p>从<strong>统计推断的哲学思想</strong>划分，机器学习模型可以划分为：<strong>频率学派与贝叶斯学派</strong>。前者认为模型的参数是固定但未知的“常数”，学习的过程就是通过优化方法寻找这个最优值；而后者则认为参数本身服从某种分布，学习的过程是在观察数据后，对先验信念进行更新。</p><p>若从<strong>建模的最终目标</strong>划分，算法模型又可分为<strong>判别式模型与生成式模型</strong>。判别式模型关注的是寻找不同类别之间的决策边界，直接学习如何划分数据；而生成式模型则试图理解数据本身的产生过程，通过学习联合分布来间接进行预测。</p><h1 id="按统计推断划分"><a href="#按统计推断划分" class="headerlink" title="按统计推断划分"></a>按统计推断划分</h1><h2 id="频率学派"><a href="#频率学派" class="headerlink" title="频率学派"></a>频率学派</h2><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>频率派（Frequentist）将概率解释为事件在长期重复试验中发生的频率。在这一框架下，模型参数 $ \theta $ 被视为一个确定的未知常量，虽然我们不知道它的具体值，但它本身并不具有随机性。我们只能通过观测数据来估计这个常量的值。</p><h3 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h3><p>给定观测数据集 $ X $，所有样本的联合概率为：</p><p>$$p(X|\theta) = \prod_{i=1}^N p(x_i|\theta)$$</p><p>这个联合概率被称为<strong>似然函数</strong>（Likelihood Function），它衡量了在特定参数 $ \theta $ 下观测到当前数据的可能性。频率派的估计方法就是寻找能使似然函数最大化的参数值，即<strong>最大似然估计</strong>（Maximum Likelihood Estimation, MLE）：</p><p>$$\theta_{MLE} = \arg\max_{\theta} p(X|\theta) = \arg\max_{\theta} \sum_{i=1}^N \log p(x_i|\theta)$$</p><p>由于对数函数是单调递增的，且能将连乘转化为连加便于计算，实际中通常使用对数似然。</p><h3 id="频率派与机器学习"><a href="#频率派与机器学习" class="headerlink" title="频率派与机器学习"></a>频率派与机器学习</h3><p>频率派的思想深刻影响了<strong>统计机器学习</strong>的发展。许多经典算法都可以从MLE的角度理解：</p><ul><li><strong>线性回归</strong>：假设误差服从高斯分布时，MLE等价于最小二乘法</li><li><strong>逻辑回归</strong>：直接对条件概率 $ p(y|x) $ 进行MLE估计</li><li><strong>支持向量机</strong>：可以看作是在特定损失函数下的频率派方法</li></ul><p>频率派方法的优势在于其理论简洁性、计算效率高，且在大样本条件下具有良好的渐近性质。然而，它也存在明显的局限性：无法融入先验知识，在小样本情况下容易过拟合，且只能给出点估计而非分布估计。</p><hr><h3 id="频率学派的参数估计"><a href="#频率学派的参数估计" class="headerlink" title="频率学派的参数估计"></a>频率学派的参数估计</h3><table><thead><tr><th><strong>方法</strong></th><th><strong>全称</strong></th><th><strong>核心思想</strong></th><th><strong>适用场景</strong></th></tr></thead><tbody><tr><td><strong>MLE</strong></td><td>最大似然估计</td><td>$\hat{\theta}_{MLE} = \arg\max P(D \mid \theta)$   寻找使观测数据出现概率最大的参数</td><td><strong>最通用</strong>。大样本下具有渐近正态性、一致性。对模型假设敏感。</td></tr><tr><td><strong>MM</strong></td><td>矩估计法</td><td>令样本矩等于总体矩：$E[X^k] = \frac{1}{n}\sum X_i^k$</td><td>简单快速，不需要知道分布的具体形式。但在小样本下往往不是有效估计量。</td></tr><tr><td><strong>LSE</strong></td><td>最小二乘估计</td><td>$\min \sum (y_i - \hat{y}_i)^2$   最小化残差平方和</td><td>线性回归。<strong>不需要概率分布假设</strong>（若误差服从正态分布，则等价于 MLE）。</td></tr><tr><td><strong>GMM</strong></td><td>广义矩估计</td><td>利用过定矩条件（矩条件多于参数），最小化二次型距离</td><td><strong>经济计量学常用</strong>。解决了内生性问题，且不需要对误差分布做强假设。</td></tr><tr><td><strong>M-估计</strong></td><td>稳健估计</td><td>替换平方损失函数为更抗噪的 $\rho$ 函数（如 Huber 损失）</td><td><strong>数据有异常值（Outliers）时</strong>。比 MLE 更鲁棒。</td></tr><tr><td><strong>EM</strong></td><td>期望极大算法</td><td>交替进行 E-step（求期望）和 M-step（极大化似然）</td><td><strong>含有隐变量</strong>（Latent variables）的概率模型，如 GMM 聚类、HMM。</td></tr></tbody></table><h2 id="贝叶斯学派"><a href="#贝叶斯学派" class="headerlink" title="贝叶斯学派"></a>贝叶斯学派</h2><h3 id="核心思想-1"><a href="#核心思想-1" class="headerlink" title="核心思想"></a>核心思想</h3><p>贝叶斯派（Bayesian）将概率解释为对事件发生的不确定性的度量，这种不确定性可以是主观的信念。在这一框架下，参数 $ \theta $ 被视为一个随机变量，服从某个先验分布 $ p(\theta) $，这个先验分布反映了我们在看到数据之前对参数的认知。</p><h3 id="贝叶斯定理与后验分布"><a href="#贝叶斯定理与后验分布" class="headerlink" title="贝叶斯定理与后验分布"></a>贝叶斯定理与后验分布</h3><p>当我们观测到数据 $ X $ 后，根据贝叶斯定理，我们可以更新对参数的认知，得到<strong>后验分布</strong>（Posterior Distribution）：</p><p>$$p(\theta|X) = \frac{p(X|\theta) \cdot p(\theta)}{p(X)} = \frac{p(X|\theta) \cdot p(\theta)}{\int p(X|\theta) \cdot p(\theta) \, d\theta}$$</p><p>其中：</p><ul><li>$ p(X|\theta) $ 是似然函数，与频率派中的定义相同</li><li>$ p(\theta) $ 是先验分布，体现了我们对参数的主观先验知识</li><li>$ p(X) = \int p(X|\theta)p(\theta)d\theta $ 是边缘似然，也称为证据（Evidence），作为归一化常数确保后验概率之和为1</li></ul><h3 id="最大后验估计"><a href="#最大后验估计" class="headerlink" title="最大后验估计"></a>最大后验估计</h3><p>虽然贝叶斯派的完整结果是整个后验分布，但在实际应用中有时也需要一个点估计。这时可以采用<strong>最大后验估计</strong>（Maximum A Posteriori, MAP）：</p><p>$$\theta_{MAP} = \arg\max_{\theta} p(\theta|X) = \arg\max_{\theta} p(X|\theta) \cdot p(\theta) \tag{4}$$</p><p>MAP估计巧妙地结合了似然函数和先验信息。从优化的角度看，先验分布起到了<strong>正则化</strong>的作用：例如，高斯先验对应L2正则化，拉普拉斯先验对应L1正则化。</p><h3 id="贝叶斯预测"><a href="#贝叶斯预测" class="headerlink" title="贝叶斯预测"></a>贝叶斯预测</h3><p>贝叶斯方法真正的优势在于其能够进行概率预测。当我们得到后验分布后，对于新样本 $ x_{new} $ 的预测分布可以通过对参数空间进行积分得到：</p><p>$$p(x_{new}|X) = \int p(x_{new}|\theta) \cdot p(\theta|X) \, d\theta \tag{5}$$</p><p>这个过程被称为<strong>贝叶斯模型平均</strong>（Bayesian Model Averaging）。与频率派只使用单一最优参数不同，贝叶斯方法考虑了所有可能的参数值，并依据后验概率进行加权平均，从而自然地体现了模型的不确定性。</p><h3 id="贝叶斯派与机器学习"><a href="#贝叶斯派与机器学习" class="headerlink" title="贝叶斯派与机器学习"></a>贝叶斯派与机器学习</h3><p>贝叶斯派思想催生了**概率图模型（Probabilistic Graphical Models）**这一重要领域，包括：</p><ul><li><strong>朴素贝叶斯分类器</strong>：最简单的贝叶斯模型，假设特征条件独立</li><li><strong>高斯过程</strong>：非参数贝叶斯方法，用于回归和分类</li><li><strong>贝叶斯神经网络</strong>：为神经网络权重赋予先验分布</li></ul><p>贝叶斯方法的优势在于：能够自然地融合先验知识、防止过拟合、提供不确定性估计、适用于小样本学习。但同时也面临挑战：后验分布的计算通常涉及高维积分，难以解析求解，需要借助近似方法如马尔可夫链蒙特卡洛（MCMC）、变分推断等。</p><h3 id="贝叶斯学派的参数估计"><a href="#贝叶斯学派的参数估计" class="headerlink" title="贝叶斯学派的参数估计"></a>贝叶斯学派的参数估计</h3><table><thead><tr><th><strong>方法</strong></th><th><strong>核心思想</strong></th><th><strong>输出形式</strong></th><th><strong>特点</strong></th></tr></thead><tbody><tr><td><strong>MAP</strong></td><td>最大后验估计   $\arg\max P(D \mid \theta) P(\theta)$</td><td><strong>点估计</strong>   (单个数值)</td><td>相当于 <strong>MLE + 先验</strong>。在线性回归中增加高斯先验即等价于 <strong>L2 正则化 (Ridge)</strong>。</td></tr><tr><td><strong>贝叶斯推断</strong></td><td>计算完整的后验分布   $P(\theta \mid D) = \frac{P(D \mid \theta) P(\theta)}{\int P(D \mid \theta) P(\theta) d\theta}$</td><td><strong>后验分布</strong></td><td>提供不确定性度量（均值、方差）。当先验是<strong>共轭先验</strong>时有解析解。</td></tr><tr><td><strong>MCMC</strong></td><td>马尔可夫链蒙特卡洛采样 (如 Gibbs 采样)</td><td><strong>采样样本点集</strong></td><td>通过随机采样模拟复杂的后验分布。<strong>万能但慢</strong>，适用于高维复杂模型。</td></tr><tr><td><strong>变分推断 (VI)</strong></td><td>将推断问题转化为<strong>优化问题</strong>   寻找一个简单分布 $q(\theta)$ 最小化 $KL(q \| p$</td><td><strong>近似解析分布</strong></td><td>牺牲了一定精度换取速度。是大规模深度贝叶斯学习（如 VAE）的核心。</td></tr></tbody></table><h2 id="频率派与贝叶斯派对比"><a href="#频率派与贝叶斯派对比" class="headerlink" title="频率派与贝叶斯派对比"></a>频率派与贝叶斯派对比</h2><p>尽管存在哲学分歧，两大学派在方法论上有着深刻的联系：</p><ul><li><strong>MLE与MAP的关系</strong>：当先验分布为均匀分布时，MAP估计退化为MLE估计</li><li><strong>正则化视角</strong>：MLE + 正则化项 等价于 MAP（特定先验）</li><li><strong>大样本一致性</strong>：当样本量趋于无穷时，后验分布会集中在MLE估计附近，贝叶斯估计渐近等价于频率派估计</li></ul><p>现代机器学习中，两大学派的界限正变得越来越模糊。研究者们开始汲取两者的优势：</p><ul><li><strong>贝叶斯深度学习</strong>：将贝叶斯思想引入深度神经网络，解决过拟合和不确定性估计问题</li><li><strong>集成学习</strong>：结合多个模型的思想与贝叶斯模型平均有异曲同工之妙</li><li><strong>概率编程</strong>：提供灵活的框架来表达和求解概率模型</li><li><strong>变分自编码器（VAE）</strong>：巧妙结合了深度学习与变分推断</li></ul><h2 id="应用场景选择"><a href="#应用场景选择" class="headerlink" title="应用场景选择"></a>应用场景选择</h2><p>在实际问题中，选择哪种方法取决于具体需求：</p><ul><li><strong>数据量巨大</strong>：频率派方法通常计算更高效</li><li><strong>小样本或零样本学习</strong>：贝叶斯方法可以利用先验知识</li><li><strong>需要不确定性估计</strong>：贝叶斯方法自然提供</li><li><strong>模型解释性要求高</strong>：简单频率派模型更易解释</li><li><strong>计算资源有限</strong>：频率派方法通常更友好</li></ul><h1 id="按建模目标划分"><a href="#按建模目标划分" class="headerlink" title="按建模目标划分"></a>按建模目标划分</h1><h2 id="判别式"><a href="#判别式" class="headerlink" title="判别式"></a>判别式</h2><ul><li><strong>直接</strong>对后验概率 <code>P(y | x)</code> 进行建模。</li><li><strong>关注点</strong>：直接学习决策边界，即不同类别之间的界限。</li><li><strong>例子</strong>：逻辑回归、神经网络、支持向量机。</li><li><strong>比喻</strong>：学会直接区分狗和猫的图片（只看区别）。</li></ul><h2 id="生成式"><a href="#生成式" class="headerlink" title="生成式"></a>生成式</h2><ul><li><strong>间接</strong>地对<strong>似然 (Likelihood)</strong> <code>P(x | y)</code> 和<strong>先验 (Prior)</strong> <code>P(y)</code> 进行建模。</li><li><strong>关注点</strong>：为每个类别单独建模其特征的分布。“生成式”是因为一旦学到了 <code>P(x | y)</code>，就可以为任何类别 <code>y</code> 生成新的样本 <code>x</code>。</li><li><strong>步骤</strong>：<ol><li>为每个类别 <code>y</code> 假设一个特征分布模型（例如高斯分布）。</li><li>从训练数据中估计每个类别分布的参数（如均值、方差）。</li><li>利用贝叶斯定理，将学到的 <code>P(x | y)</code> 和 <code>P(y)</code> 转换为最终用于分类的 <code>P(y | x)</code>。</li></ol></li><li><strong>比喻</strong>：分别学习“狗看起来是什么样”和“猫看起来是什么样”的完整模型，然后对于一个新动物，看它更符合哪个模型。</li></ul><h2 id="判别式与生成式的模型对比"><a href="#判别式与生成式的模型对比" class="headerlink" title="判别式与生成式的模型对比"></a>判别式与生成式的模型对比</h2><p><strong>判别模型</strong> 和 <strong>生成模型</strong> 的根本区别在于它们<strong>解决问题的思路和关注点</strong>不同。</p><ul><li><p><strong>判别模型</strong> 致力于 <strong>“找到区别”</strong>。</p><ul><li><strong>思路</strong>：直接学习不同类别数据之间的<strong>决策边界</strong>，而不关心单个类别本身的具体样貌。</li><li><strong>目标</strong>：回答“<strong>它更像是猫还是狗？</strong>”</li><li><strong>好比</strong>：一个裁判，他不需要会画画，只需要掌握一个关键标准（比如身长）来快速区分两者。</li></ul></li><li><p><strong>生成模型</strong> 致力于 <strong>“理解本质”</strong>。</p><ul><li><strong>思路</strong>：分别学习每一类数据（如猫、狗）的<strong>整体特征和内部结构</strong>，为每个类别建立一个完整的“概念模型”。</li><li><strong>目标</strong>：回答“<strong>猫&#x2F;狗长什么样子？</strong>”</li><li><strong>好比</strong>：一个艺术家，他需要透彻地了解猫和狗的骨骼、肌肉、毛发等所有细节，才能把它们画出来。</li></ul></li></ul><table><thead><tr><th align="left">特征</th><th align="left"><strong>判别模型（Discriminative）</strong></th><th align="left"><strong>生成模型（Generative）</strong></th></tr></thead><tbody><tr><td align="left"><strong>核心思想</strong></td><td align="left">学习类别之间的<strong>边界</strong>，找到“差异”。</td><td align="left">学习数据本身的<strong>分布</strong>，理解每一类的“本质”。</td></tr><tr><td align="left"><strong>解决的问题</strong></td><td align="left">这是X还是Y</td><td align="left">什么是X</td></tr><tr><td align="left"><strong>学习内容</strong></td><td align="left">条件概率p(y|x)   （x是数据特征，y是数据标签）</td><td align="left">联合概率p(x,y)、分布p(x)（无监督的情况）</td></tr><tr><td align="left"><strong>能力</strong></td><td align="left">主要用于<strong>分类和回归</strong>，无法生成新数据。</td><td align="left">既可以进行分类，也可以<strong>生成新的数据</strong>（如画一只猫）。</td></tr><tr><td align="left"><strong>类比</strong></td><td align="left">只学会一个<strong>投机取巧的判别技巧</strong>（如比身长）。</td><td align="left">学完后对猫狗有<strong>直观认知</strong>，能画出它们。</td></tr><tr><td align="left"><strong>常见算法</strong></td><td align="left">逻辑回归、支持向量机、决策树、CRF、神经网络</td><td align="left">朴素贝叶斯、高斯混合、隐马尔可夫模型、VAF、生成对抗网络</td></tr></tbody></table><p><strong>判别模型因为只专注于区分，因此不具备生成能力；而生成模型因为学会了数据的“本质”，所以具备生成新数据的能力。</strong></p><p>ChatGPT、Midjourney这类能创作内容（生成文本、图像）的模型，其核心都是<strong>生成模型</strong>。而许多用于图像分类、垃圾邮件过滤等任务的模型，则更多是<strong>判别模型</strong>。</p><h2 id="生成与判别式模型的选择"><a href="#生成与判别式模型的选择" class="headerlink" title="生成与判别式模型的选择"></a>生成与判别式模型的选择</h2><ul><li>当数据量较少或对数据分布有较强先验知识时，<strong>生成学习方法</strong>可能更有效。<ul><li>适用于数据生成、缺失值处理；</li><li>在小样本下可能比判别模型（如逻辑回归）更鲁棒。</li></ul></li><li>当数据量充足且计算资源有限时，判别学习方法可能更合适。</li></ul><h1 id="按学习目标划分"><a href="#按学习目标划分" class="headerlink" title="按学习目标划分"></a>按学习目标划分</h1><h2 id="回归（Regression）"><a href="#回归（Regression）" class="headerlink" title="回归（Regression）"></a>回归（Regression）</h2><ul><li>回归旨在建立输入特征与连续型输出变量之间的映射关系，回归任务的目标是预测一个<strong>连续的数值输出</strong> $y \in \mathbb{R}$，</li><li><strong>核心思想</strong>：找到一条曲线&#x2F;超平面，使预测值 $\hat{y}$ 尽可能接近真实值 $y$，最小化预测误差: $ \min_\theta \sum_{i=1}^N (y_i - \hat{y}_i)^2 $</li><li>应用场景：预测房价、售额预测、股票价格等。</li></ul><p>常见的回归算法:</p><ul><li><strong>线性回归</strong>：最简单的回归模型，假设输入与输出呈线性关系</li><li><strong>岭回归&#x2F;Lasso</strong>：在线性回归基础上加入L2&#x2F;L1正则化，防止过拟合</li><li><strong>决策树回归</strong>：通过树结构分段拟合数据</li><li><strong>随机森林回归</strong>：集成多棵决策树，降低方差</li><li><strong>梯度提升回归</strong>（GBDT、XGBoost）：通过迭代优化残差</li><li><strong>支持向量回归（SVR）</strong>：利用核方法处理非线性关系</li><li><strong>神经网络</strong>：能够拟合任意复杂的非线性函数</li></ul><h2 id="分类（Classification）"><a href="#分类（Classification）" class="headerlink" title="分类（Classification）"></a>分类（Classification）</h2><ul><li>目标是预测离散的类别标签（Y）。</li><li>例如：判断邮件是否为垃圾邮件（类别：垃圾邮件&#x2F;非垃圾邮件）、识别图像中的物体类别（类别：猫&#x2F;狗&#x2F;车等）。</li></ul><p>预测离散类别标签。根据类别数量可分为：</p><ul><li><strong>二分类</strong>： 只有两个类别，例如垃圾邮件分类。</li><li><strong>多分类</strong>：包含多个类别，例如手写数字识别（0-9）。</li><li><strong>多标签分类</strong>：一个样本可能属于多个类别</li></ul><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>给定输入特征向量 $\mathbf{x} \in \mathbb{R}^d$，分类问题的目标是预测其所属的<strong>离散类别标签</strong> $y \in \{1, 2, ..., K\}$。</p><p>从概率视角看，我们需要计算<strong>后验概率</strong>（posterior probability）：<br>$$P(y = k \mid \mathbf{x}), \quad k = 1,2,...,K$$</p><p>所有分类模型最终都采用<strong>最大后验决策规则</strong>（MAP decision rule）进行预测：<br>$$\hat{y} = \arg\max_{k} P(y = k \mid \mathbf{x})$$<br>即选择使后验概率最大的类别作为预测结果。</p><h3 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h3><h4 id="判别式学习建模（Discriminative）"><a href="#判别式学习建模（Discriminative）" class="headerlink" title="判别式学习建模（Discriminative）"></a>判别式学习建模（Discriminative）</h4><ul><li><strong>直接建模</strong> $P(y|\mathbf{x})$</li><li>关注决策边界，不关心数据本身的分布</li></ul><h4 id="生成式学习建模（Generative）"><a href="#生成式学习建模（Generative）" class="headerlink" title="生成式学习建模（Generative）"></a>生成式学习建模（Generative）</h4><ul><li><strong>间接建模</strong>：先学习联合分布 $ P(\mathbf{x}, y) = P(y)P(\mathbf{x}|y) $</li><li>通过贝叶斯定理计算后验：$ P(y|\mathbf{x}) = \frac{P(y)P(\mathbf{x}|y)}{\sum_{k=1}^K P(y=k)P(\mathbf{x}|y=k)} $</li></ul><h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><p>无论采用判别式还是生成式方法，模型都包含需要从数据中学习的参数 $\theta$。参数估计主要有两种方法：</p><h4 id="频率派的最大似然估计（MLE）"><a href="#频率派的最大似然估计（MLE）" class="headerlink" title="频率派的最大似然估计（MLE）"></a>频率派的<strong>最大似然估计（MLE）</strong></h4><p>​将参数视为确定的未知常量，寻找使训练数据似然函数最大化的参数值：<br>$$\theta_{MLE} = \arg\max_\theta \prod_{i=1}^N P(y_i|\mathbf{x}_i, \theta)$$<br>​等价于最小化经验风险。</p><h4 id="贝叶斯派的最大后验估计（MAP）"><a href="#贝叶斯派的最大后验估计（MAP）" class="headerlink" title="贝叶斯派的最大后验估计（MAP）"></a>贝叶斯派的<strong>最大后验估计（MAP）</strong></h4><p>​将参数视为随机变量，在MLE的基础上引入了参数的先验分布$p(\theta)$，通过贝叶斯定理求使<strong>后验概率</strong>最大的参数值：<br>$$\theta_{MAP} = \arg\max_\theta \prod_{i=1}^N P(y_i|\mathbf{x}_i, \theta) \cdot p(\theta)$$</p><ul><li><strong>正则化效应</strong>：先验分布 $p(\theta)$ 等价于对参数施加约束，防止过拟合<ul><li>高斯先验 $p(\theta) \sim \mathcal{N}(0, \sigma^2)$ ↔ L2 正则化</li><li>拉普拉斯先验 $p(\theta) \sim \text{Laplace}(0, b)$ ↔ L1 正则化</li></ul></li><li>在数据稀缺时，先验知识可提供更稳健的估计</li><li>当先验为均匀分布时，MAP 退化为 MLE</li></ul><h3 id="完整流程"><a href="#完整流程" class="headerlink" title="完整流程"></a>完整流程</h3><ol><li><strong>训练阶段</strong>：使用MLE或MAP估计模型参数 $\hat{\theta}$</li><li><strong>预测阶段</strong>：对于新样本 $\mathbf{x}_{new}$，计算后验概率：$P(y=k|\mathbf{x}_{new}, \hat{\theta})$</li><li><strong>决策阶段</strong>：应用最大后验决策规则：$\hat{y} = \arg\max_k P(y=k|\mathbf{x}_{new}, \hat{\theta})$</li></ol><h2 id="聚类（Clustering）"><a href="#聚类（Clustering）" class="headerlink" title="聚类（Clustering）"></a>聚类（Clustering）</h2><p>聚类是一种<strong>无监督学习</strong>任务，目标是将未标注的样本划分为若干个<strong>簇</strong>（cluster），使得同一簇内的样本尽可能相似，不同簇间的样本尽可能不同。</p><p>给定数据集 $\{\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_N\}$，聚类算法旨在找到一种划分 $C = \{C_1, C_2, ..., C_K\}$，使得：</p><ul><li>簇内相似度最大化：$\max \sum_{k=1}^K \sum_{\mathbf{x}_i \in C_k} \text{sim}(\mathbf{x}_i, \mu_k)$</li><li>簇间相似度最小化：$\min \sum_{k \lt j} \text{sim}(\mu_k, \mu_j)$，其中 $\mu_k$ 是第 $k$ 个簇的中心（或代表点），$\text{sim}(\cdot)$ 是相似度度量。</li></ul><h2 id="降维（Dimensionality-Reduction）"><a href="#降维（Dimensionality-Reduction）" class="headerlink" title="降维（Dimensionality Reduction）"></a>降维（Dimensionality Reduction）</h2><p>降维是将高维数据映射到低维空间的过程，同时尽可能保留原始数据的重要结构信息。它是<strong>无监督学习</strong>的重要分支，也是数据预处理的关键步骤。</p><p>给定高维数据 $\{\mathbf{x}_i \in \mathbb{R}^D\}_{i=1}^N$，降维算法寻找映射 $f: \mathbb{R}^D \rightarrow \mathbb{R}^d$，其中 $d \ll D$，使得：<br>$$\mathbf{z}_i = f(\mathbf{x}_i) \in \mathbb{R}^d$$<br>且 $\{\mathbf{z}_i\}$ 尽可能保留 $\{\mathbf{x}_i\}$ 的重要结构信息。</p><p>需要降维的场景:</p><ul><li><strong>维度灾难（Curse of Dimensionality）</strong>：随着维度增加，数据变得稀疏，距离度量失效，模型复杂度指数级增长</li><li><strong>可视化需求</strong>：人类只能理解2D或3D空间，降维使高维数据可视化成为可能</li><li><strong>计算效率</strong>：减少特征数量，降低模型训练和预测的时间复杂度</li><li><strong>去噪</strong>：丢弃不重要的维度，保留主要信号</li><li><strong>特征提取</strong>：从原始特征中构造更有代表性的新特征</li></ul><h2 id="排序（Ranking）"><a href="#排序（Ranking）" class="headerlink" title="排序（Ranking）"></a>排序（Ranking）</h2><p>排序任务的目标是学习一个<strong>排序函数</strong>，能够根据输入特征对项目集合进行排序，使得<strong>相关性高</strong>的项目排在前面。排序问题是信息检索和推荐系统的核心。</p><p>排序的应用场景:</p><ul><li><strong>搜索引擎</strong>：根据用户查询对网页进行排序</li><li><strong>推荐系统</strong>：为用户推荐最感兴趣的商品、视频、音乐</li><li><strong>问答系统</strong>：对候选答案按相关性排序</li><li><strong>广告点击率预测</strong>：排序广告以提高点击率</li><li><strong>社交媒体</strong>：排序信息流中的帖子</li></ul><h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>机器学习的发展，本质上是在<strong>表示、评估与优化</strong>这三个核心要素上的持续演进：选择何种方式表示数据与模型，如何定义优劣，以及通过何种策略寻找最优解。</p><p><strong>深度学习的崛起</strong>摒弃了人工特征工程，通过分层抽象自动从原始数据中提取层次化特征，使得模型能够感知高维空间中的复杂结构。然而，强大的表示能力也伴随着挑战——海量参数需要海量数据，黑箱特性损害了可解释性。</p><p><strong>强化学习的复兴</strong>则拓展了学习的范式。它不再满足于拟合静态数据，而是在与环境的交互中通过试错学习最优策略，将机器学习从模式识别推向序贯决策。这种范式更接近生物学习的本质，在机器人、博弈、控制等领域展现出独特价值。</p><p>然而，技术的飞速发展机器学习仍面临多方面的挑战：</p><ul><li><strong>数据效率</strong>：人类可以从少量样本中学习，而当前模型仍然依赖海量数据。如何让机器学习像人类一样“举一反三”，仍是未解难题。</li><li><strong>鲁棒性与泛化</strong>：分布外泛化、对抗样本、虚假相关——模型在实验室环境外的表现往往不尽如人意。</li><li><strong>可解释性与可信赖</strong>：随着模型进入医疗、金融、司法等高风险领域，黑箱决策的风险日益凸显。</li><li><strong>价值对齐</strong>：当模型越来越强大，如何确保其目标与人类价值观一致，成为关乎未来的重要课题。</li><li><strong>计算效率与可持续性</strong>：大模型的训练消耗惊人，如何在性能与能耗之间取得平衡，既是技术问题，也是环境问题。</li></ul><h1 id="常见模型划分一览表"><a href="#常见模型划分一览表" class="headerlink" title="常见模型划分一览表"></a>常见模型划分一览表</h1><table><thead><tr><th><strong>算法模型</strong></th><th><strong>频率派</strong></th><th><strong>贝叶斯派</strong></th><th><strong>判别式</strong></th><th><strong>生成式</strong></th><th><strong>参数估计&#x2F;核心准则</strong></th></tr></thead><tbody><tr><td><strong>线性回归</strong></td><td>是</td><td>是 (贝叶斯线性回归)</td><td>是</td><td>否</td><td>最小二乘 (OLS) &#x2F; MLE</td></tr><tr><td><strong>逻辑回归</strong></td><td>是</td><td>是 (拉普拉斯近似等)</td><td>是</td><td>否</td><td>极大似然估计 (MLE)</td></tr><tr><td><strong>决策树&#x2F;随机森林&#x2F;提升树</strong></td><td>是</td><td>否</td><td>是</td><td>否</td><td>信息增益 &#x2F; 基尼系数 &#x2F; MSE</td></tr><tr><td><strong>SVM</strong></td><td>是</td><td>否</td><td>是</td><td>否</td><td>合页损失 + 正则化 (对偶优化)</td></tr><tr><td><strong>PCA (主成分分析)</strong></td><td>是</td><td>是 (Probabilistic PCA)</td><td>不适用</td><td>是 (概率视角下)</td><td>方差最大化 &#x2F; 投影误差最小化</td></tr><tr><td><strong>LDA (线性判别分析)</strong></td><td>是</td><td>否</td><td>否</td><td><strong>是</strong></td><td>Fisher 准则 &#x2F; 类内类间散度</td></tr><tr><td><strong>高斯判别分析 (GDA)</strong></td><td>是</td><td>是</td><td>否</td><td><strong>是</strong></td><td>联合概率 $P(x,y)$ 建模</td></tr><tr><td><strong>高斯过程 (GP)</strong></td><td>否</td><td><strong>是</strong></td><td>是</td><td>否</td><td>核函数 + 边际似然最大化</td></tr><tr><td><strong>高斯混合模型 (GMM)</strong></td><td>是</td><td>是 (贝叶斯 GMM)</td><td>否</td><td><strong>是</strong></td><td><strong>EM 算法</strong></td></tr><tr><td><strong>朴素贝叶斯</strong></td><td>是</td><td><strong>是</strong></td><td>否</td><td><strong>是</strong></td><td>MAP (带平滑) &#x2F; MLE</td></tr><tr><td><strong>贝叶斯网络</strong></td><td>否</td><td><strong>是</strong></td><td>否</td><td><strong>是</strong></td><td>结构学习 + 条件概率表</td></tr><tr><td><strong>隐马尔可夫 (HMM)</strong></td><td>是</td><td>是 (变分推断)</td><td>否</td><td><strong>是</strong></td><td>EM 算法</td></tr><tr><td><strong>条件随机场 (CRF)</strong></td><td>是</td><td>否</td><td><strong>是</strong></td><td>否</td><td>极大似然 (梯度上升)</td></tr><tr><td><strong>感知机</strong></td><td>是</td><td>否</td><td>是</td><td>否</td><td>随机梯度下降 (SGD)</td></tr><tr><td><strong>深度神经网络 (DNN)</strong></td><td>是</td><td>是 (BNN)</td><td>是</td><td>视模型而定</td><td>反向传播 (BP) + 优化器</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;在传统的编程范式中，我们输入数据和规则，通过计算机得到答案。而机器学习（Machine Learning）则不同，我们向计算机输入数据和答案</summary>
      
    
    
    
    <category term="机器学习" scheme="https://www.hicode365.com/categories/ml/"/>
    
    
    <category term="频率学派" scheme="https://www.hicode365.com/tags/%E9%A2%91%E7%8E%87%E5%AD%A6%E6%B4%BE/"/>
    
    <category term="贝叶斯学派" scheme="https://www.hicode365.com/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E6%B4%BE/"/>
    
    <category term="判别式模型" scheme="https://www.hicode365.com/tags/%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="生成式模型" scheme="https://www.hicode365.com/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="回归" scheme="https://www.hicode365.com/tags/%E5%9B%9E%E5%BD%92/"/>
    
    <category term="分类" scheme="https://www.hicode365.com/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>python中的数据类型</title>
    <link href="https://www.hicode365.com/cuidDZEK6Bgd97PUqFs0w-F_G"/>
    <id>https://www.hicode365.com/cuidDZEK6Bgd97PUqFs0w-F_G</id>
    <published>2023-08-15T02:10:06.000Z</published>
    <updated>2026-01-01T15:46:05.612Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python中的数据类型"><a href="#Python中的数据类型" class="headerlink" title="Python中的数据类型"></a>Python中的数据类型</h1><h2 id="可变数据类型"><a href="#可变数据类型" class="headerlink" title="可变数据类型"></a>可变数据类型</h2><p>对变量的值进行修改时，变量对应的内存地址不变，对应的值发生了改变，这种数据类型就称为可变数据类型。</p><h2 id="不可变数据类型"><a href="#不可变数据类型" class="headerlink" title="不可变数据类型"></a>不可变数据类型</h2><p>对变量的进行修改时，变量对应的内存地址发生了改变(变量指向了新的内存)，从而修改了变量的值，而变量对应的原内存的值并没有被改变，这种数据类型就称为可变数据类型。</p><p>也就是：不可变数据类型更改后地址发生改变，可变数据类型更改地址不发生改变</p><h2 id="常用数据类型"><a href="#常用数据类型" class="headerlink" title="常用数据类型"></a>常用数据类型</h2><table><thead><tr><th>数据类型</th><th>是否是可变数据类型</th><th>是否有序</th></tr></thead><tbody><tr><td>None (空)</td><td>不可变</td><td>-</td></tr><tr><td>int (整数)</td><td>不可变</td><td>-</td></tr><tr><td>float (浮点)</td><td>不可变</td><td>-</td></tr><tr><td>bool (布尔)</td><td>不可变</td><td>-</td></tr><tr><td>str (字符串)</td><td>不可变</td><td>-</td></tr><tr><td>tuple (元组)</td><td>不可变</td><td>序列类型，有序</td></tr><tr><td>list (列表)</td><td>可变</td><td>序列类型，有序</td></tr><tr><td>set (集合)</td><td>可变</td><td>序列类型，无序，不可重复</td></tr><tr><td>dict (字典)</td><td>可变</td><td>映射类型，v3.6及以后无有序, 前面版本无序</td></tr></tbody></table><h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><table><thead><tr><th>数据类型</th><th>是否是可变数据类型</th><th>是否有序</th><th>说明</th></tr></thead><tbody><tr><td>bytes</td><td>不可变</td><td>-</td><td>定义字节：b’hello’,bytes(5)</td></tr><tr><td>bytearray</td><td>可变</td><td>-</td><td>定义字节数组：bytearray(b’hello’), bytearray(10)</td></tr><tr><td>complex (复数)</td><td>不可变</td><td>-</td><td>由一个实数和一个虚数组合构成，如：4+3j</td></tr><tr><td>frozenset (冻结的set)</td><td>不可变</td><td>无序</td><td>冻结的set初始化后不能再添加或删除元素</td></tr><tr><td>array (数组)</td><td>可变</td><td>有序</td><td>数组中的元素必须是同一类型</td></tr><tr><td>OrderedDict</td><td>可变</td><td>有序</td><td>key有序，setdefault取值key不存在也不报错</td></tr><tr><td>defaultdict</td><td>可变</td><td>有序</td><td>取值时Key不存在也不会抛出KeyError异常</td></tr><tr><td>deque</td><td>可变</td><td>有序</td><td>高效插入和删除的<strong>双向队列列表</strong></td></tr></tbody></table><h2 id="常见数据类型的操作和转换"><a href="#常见数据类型的操作和转换" class="headerlink" title="常见数据类型的操作和转换"></a>常见数据类型的操作和转换</h2><h3 id="list列表"><a href="#list列表" class="headerlink" title="list列表[ ]"></a>list列表[ ]</h3><p>list是**&#x3D;&#x3D;可变&#x3D;&#x3D;<strong>、</strong>&#x3D;&#x3D;可重复&#x3D;&#x3D;<strong>的</strong>&#x3D;&#x3D;有序&#x3D;&#x3D;**列表，里面的元素的数据类型也可以不同(也可以是另一个list)。list可根据索引号取其中的数据。</p><h4 id="list的生成"><a href="#list的生成" class="headerlink" title="list的生成"></a>list的生成</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">list1 = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;list1: &quot;</span>, <span class="built_in">list</span>(list1)) <span class="comment"># 输出： list1:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span></span><br><span class="line">list2 = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, <span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;list2: &quot;</span>, <span class="built_in">list</span>(list2))</span><br><span class="line">list3 = [i*i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>) <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;list3: &quot;</span>, <span class="built_in">list</span>(list3))</span><br><span class="line">list4 = (<span class="built_in">str</span>(i) + j <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">10</span>, <span class="number">2</span>) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="string">&quot;xyz&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;list4: &quot;</span>, <span class="built_in">list</span>(list4))</span><br></pre></td></tr></table></figure><h4 id="list元素反转、排序和次数统计"><a href="#list元素反转、排序和次数统计" class="headerlink" title="list元素反转、排序和次数统计"></a>list元素反转、排序和次数统计</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">list1 = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">list1.reverse()  <span class="comment"># 元素顺序反转</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;list reverse: &quot;</span>, list1)</span><br><span class="line">list1.sort(reverse=<span class="literal">False</span>)  <span class="comment"># 排序</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;list sort: &quot;</span>, list1)</span><br><span class="line">list1 = <span class="built_in">sorted</span>(list1, reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;list sort: &quot;</span>, list1)</span><br><span class="line">times = list1.count(<span class="number">5</span>)  <span class="comment"># 查看list中的元素出现的次数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;times: &quot;</span>, times)</span><br></pre></td></tr></table></figure><h4 id="list元素的添加、删除和取值"><a href="#list元素的添加、删除和取值" class="headerlink" title="list元素的添加、删除和取值"></a>list元素的添加、删除和取值</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">list1.append(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;append value: &quot;</span>, list1)  <span class="comment"># 添加元素</span></span><br><span class="line">list1.insert(<span class="number">1</span>, <span class="number">10</span>)  <span class="comment"># 在指定位置添加元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;insert value: &quot;</span>, list1)</span><br><span class="line">list1.remove(<span class="number">10</span>)  <span class="comment"># 删除指定value元素(第一个匹配的元素)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;remove value: &quot;</span>, list1)</span><br><span class="line">value = list1.pop(<span class="number">12</span>)  <span class="comment"># 删除指定index的元素并返回删除的值</span></span><br><span class="line">list1.pop()  <span class="comment"># 不指定index时默认删除最后一个元素</span></span><br><span class="line">list1.pop(-<span class="number">2</span>)  <span class="comment"># 删除倒数第二个元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;remove index: &quot;</span>, list1)</span><br><span class="line"></span><br><span class="line">index_value = list1.index(<span class="number">3</span>)  <span class="comment"># 查找第一个value为100的index值，如果不存在报TypeError异常</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;index_value: &quot;</span>, index_value)</span><br><span class="line"><span class="built_in">print</span>(list1)</span><br><span class="line">index_value = list1.index(<span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>)  <span class="comment"># 指定范围，从第7(包括)个到第9(不包括)个元素之间查找value为5的index</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;index_value: &quot;</span>, index_value)</span><br></pre></td></tr></table></figure><h4 id="list添加多个元素、list的合并"><a href="#list添加多个元素、list的合并" class="headerlink" title="list添加多个元素、list的合并"></a>list添加多个元素、list的合并</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">list2 = [<span class="number">100</span>, <span class="number">101</span>, <span class="number">102</span>]</span><br><span class="line"><span class="comment"># list1 = list1 + list2</span></span><br><span class="line">list1.extend(list2)</span><br><span class="line"><span class="built_in">print</span>(list1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">10</span>)</span><br></pre></td></tr></table></figure><h4 id="list的遍历"><a href="#list的遍历" class="headerlink" title="list的遍历"></a>list的遍历</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> list1:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;value: %i&quot;</span> % value)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">50</span>)</span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(list1)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;index: %i, value: %i&quot;</span> % (index, list1[index]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">50</span>)</span><br><span class="line"><span class="keyword">for</span> index, value <span class="keyword">in</span> <span class="built_in">enumerate</span>(list1):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;index: %i, value: %i&quot;</span> % (index, value))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">50</span>)</span><br><span class="line"><span class="keyword">for</span> index, value <span class="keyword">in</span> <span class="built_in">enumerate</span>(list1, <span class="number">100</span>):  <span class="comment"># index从100开始</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;index: %i, value: %i&quot;</span> % (index, value))</span><br></pre></td></tr></table></figure><h4 id="list中使用切片-slice-取值"><a href="#list中使用切片-slice-取值" class="headerlink" title="list中使用切片(slice)取值"></a>list中使用切片(slice)取值</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">elements = list1[<span class="number">0</span>:<span class="number">3</span>]  <span class="comment"># 取第0到3条元素(包括头不包括尾)</span></span><br><span class="line"><span class="comment"># elements = list1[:3]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;slice elements: &quot;</span>, elements)</span><br><span class="line">elements = list1[<span class="number">1</span>:]  <span class="comment"># 取第1到最后一个元素(包括头也包括尾)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;slice elements: &quot;</span>, elements)</span><br><span class="line">elements = list1[-<span class="number">2</span>]  <span class="comment"># 取倒数第二条</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;slice elements: &quot;</span>, elements)</span><br><span class="line">elements = list1[<span class="number">4</span>:-<span class="number">2</span>]  <span class="comment"># 取第四条到倒数第二条(包括头不包括尾)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;slice elements: &quot;</span>, elements)</span><br><span class="line">elements = list1[<span class="number">0</span>:<span class="number">6</span>:<span class="number">2</span>]  <span class="comment"># 取第0条到第6条中每2个取一个</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;slice elements: &quot;</span>, elements)</span><br><span class="line">elements = list1[:]  <span class="comment"># 取所有元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;slice elements: &quot;</span>, elements)</span><br></pre></td></tr></table></figure><p><strong>&#x3D;&#x3D;列表、元组和字符串&#x3D;&#x3D;都可以使用切片进行操作</strong></p><h4 id="list的深copy和浅copy"><a href="#list的深copy和浅copy" class="headerlink" title="list的深copy和浅copy"></a>list的深copy和浅copy</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 浅拷贝只拷贝了引用，没有拷贝内容</span></span><br><span class="line">list2 = list1</span><br><span class="line">list2[<span class="number">1</span>] = <span class="number">1000</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;list1: &quot;</span>, list1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;list2: &quot;</span>, list1)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(list1), <span class="built_in">id</span>(list2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 深拷贝是对于一个对象所有层次的拷贝(递归拷贝)</span></span><br><span class="line">list3 = list1.copy()</span><br><span class="line"><span class="comment"># import copy</span></span><br><span class="line"><span class="comment"># list3 = copy.copy(list1)</span></span><br><span class="line"><span class="comment"># list3 = copy.deepcopy(list1)</span></span><br><span class="line">list1[<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;list1: &quot;</span>, list1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;list3: &quot;</span>, list3)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(list1), <span class="built_in">id</span>(list3))</span><br></pre></td></tr></table></figure><h3 id="set集合"><a href="#set集合" class="headerlink" title="set集合{ }"></a>set集合{ }</h3><p>set是**&#x3D;&#x3D;可变&#x3D;&#x3D;**、&#x3D;&#x3D;<strong>不可重复</strong>&#x3D;&#x3D;的&#x3D;&#x3D;<strong>无序</strong>&#x3D;&#x3D;列表。 &#x3D;&#x3D;<strong>set中不可以放入可变对象</strong>&#x3D;&#x3D;，因为无法判断两个可变对象是否相等而去重。</p><h4 id="set的定义"><a href="#set的定义" class="headerlink" title="set的定义"></a>set的定义</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">set0 = &#123;<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>&#125;  <span class="comment"># 直接定义set集合</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;set0: &quot;</span>, set0)  <span class="comment"># 输出 set0:  &#123;0,1, 2, 3, 4, 5, 6&#125;</span></span><br><span class="line"></span><br><span class="line">set1 = <span class="built_in">set</span>([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])  <span class="comment"># 通过list定义set</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;set1: &quot;</span>, set1)</span><br><span class="line"></span><br><span class="line">set2 = <span class="built_in">set</span>((<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>))  <span class="comment"># 通过tuple定义set</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;set2: &quot;</span>, set2)</span><br><span class="line"></span><br><span class="line">set3 = <span class="built_in">set</span>(&#123;<span class="string">&quot;x&quot;</span>: <span class="number">2</span>, <span class="number">10</span>: <span class="string">&quot;b&quot;</span>&#125;)  <span class="comment"># 通过dict定义set</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;set3: &quot;</span>, set3)  <span class="comment"># 输出 set3:  &#123;10, &#x27;x&#x27;&#125;</span></span><br><span class="line"></span><br><span class="line">my_list = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">set4 = <span class="built_in">set</span>(my_list)  <span class="comment"># set中不可以放入可变对象,然而为何放入list却不报错?</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;set4: &quot;</span>, set4)  <span class="comment"># 输出 set4:  &#123;0, 1, 2, 3, 4, 5, 6&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 由下面操作可以得出结论,set是先把list做遍历得到不可变的int对象类型后再放入set中</span></span><br><span class="line">my_list[<span class="number">0</span>] = <span class="number">10</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;set4 with my_list changed: &quot;</span>, set4)  <span class="comment"># 输出 &#123;0, 1, 2, 3, 4, 5, 6&#125;</span></span><br><span class="line"></span><br><span class="line">my_list.append([<span class="number">10</span>, <span class="number">20</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;my_list: &quot;</span>, my_list)</span><br><span class="line">set5 = <span class="built_in">set</span>(my_list)  <span class="comment"># 在list再放入list,此时将报错</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;set5: &quot;</span>, set5)</span><br></pre></td></tr></table></figure><h4 id="set元素的添加、删除和取值"><a href="#set元素的添加、删除和取值" class="headerlink" title="set元素的添加、删除和取值"></a>set元素的添加、删除和取值</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">set0 = &#123;<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;set0: &quot;</span>, set0)</span><br><span class="line">set0.add(<span class="string">&quot;cn&quot;</span>)  <span class="comment"># 添加单个元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;set0: &quot;</span>, set0)</span><br><span class="line"></span><br><span class="line">set0.update([<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>])  <span class="comment"># 添加多个元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;set0: &quot;</span>, set0)</span><br><span class="line"></span><br><span class="line">set0.add((<span class="string">&quot;com&quot;</span>, <span class="string">&quot;cn&quot;</span>))  <span class="comment"># 添加元组(元组是不可变数据类型)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;set0: &quot;</span>, set0)</span><br><span class="line"><span class="comment"># set0.add([10, 20])  # 添加list报错,不能添加可变的数据类型(不能添加,但可以使用list创建set)</span></span><br><span class="line"><span class="comment"># set0.add(&#123;10, 20&#125;)  # 添加set报错,(可是使用不可变的frozenset添加:set0.add(frozenset(&#123;10, 20&#125;)))</span></span><br><span class="line"><span class="comment"># set0.add(&#123;&quot;x&quot;: 2, 10: &quot;b&quot;&#125;)  # 添加dict报错,不能添加可变的数据类型(不能添加,但可以使用dict创建set)</span></span><br><span class="line"></span><br><span class="line">set0.remove(<span class="string">&quot;cn&quot;</span>)  <span class="comment"># 根据值删除元素(set不能根据索引删除)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;set0: &quot;</span>, set0)</span><br></pre></td></tr></table></figure><h4 id="set取并集和交集"><a href="#set取并集和交集" class="headerlink" title="set取并集和交集"></a>set取并集和交集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">set1 = &#123;<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">100</span>&#125;</span><br><span class="line">my_set = set0 | set1  <span class="comment"># 取并集</span></span><br><span class="line"><span class="built_in">print</span>(my_set)</span><br><span class="line">my_set = set0 &amp; set1  <span class="comment"># 取交集</span></span><br><span class="line"><span class="built_in">print</span>(my_set)</span><br></pre></td></tr></table></figure><h4 id="set遍历"><a href="#set遍历" class="headerlink" title="set遍历"></a>set遍历</h4><p>注：set的遍历同list</p><h3 id="dict字典"><a href="#dict字典" class="headerlink" title="dict字典{ }"></a>dict字典{ }</h3><p>dict是**&#x3D;&#x3D;无序&#x3D;&#x3D;<strong>，key&#x3D;&#x3D;<strong>不可重复</strong>&#x3D;&#x3D;、</strong>&#x3D;&#x3D;不可变&#x3D;&#x3D;**内容以key-value键值对形式存在的映射</p><p><strong>dict中的key只能是不可变对象且唯一</strong>, 一个key对应一个value，多次对一个key设置value，后面的值会把前面的冲掉。</p><blockquote><p>dict一般用在需要高速查找的很多地方。dict的key必须是不可变对象，这是因为dict根据key来计算value的存储位置，如果每次计算相同的key得出的结果不同，那dict内部就完全混乱了。这种通过key计算位置的算法称为哈希算法（Hash）。要保证hash的正确性，作为key的对象就不能变。在Python中，字符串、整数等都是不可变的，因此，可以放心地作为key。而list、set是可变的，所以就不能作为key。</p></blockquote><h4 id="dict的创建和增删改查"><a href="#dict的创建和增删改查" class="headerlink" title="dict的创建和增删改查"></a>dict的创建和增删改查</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">dict1 = &#123;<span class="string">&quot;addr&quot;</span>: <span class="string">&quot;北京&quot;</span>, <span class="string">&quot;age&quot;</span>: <span class="number">18</span>, <span class="string">&quot;gender&quot;</span>: <span class="string">&quot;女&quot;</span>&#125;</span><br><span class="line">dict1[<span class="string">&quot;height&quot;</span>] = <span class="number">1.77</span>  <span class="comment"># 添加元素</span></span><br><span class="line">dict1.pop(<span class="string">&quot;age&quot;</span>)  <span class="comment"># 删除元素输出</span></span><br><span class="line">item_del = dict1.popitem()  <span class="comment"># 产出dict中的最后一个item并返回</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;item_del: &quot;</span>, item_del)</span><br><span class="line">dict1[<span class="string">&quot;addr&quot;</span>] = <span class="string">&quot;深圳&quot;</span>  <span class="comment"># 修改元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;dict1: &quot;</span>, dict1)</span><br><span class="line"></span><br><span class="line">keys = dict1.keys()  <span class="comment"># 获取dict的所有key</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;keys: &quot;</span>, keys)  <span class="comment"># dict_keys([&#x27;add&#x27;, &#x27;height&#x27;])</span></span><br><span class="line">addr = dict1.get(<span class="string">&quot;addr&quot;</span>)  <span class="comment"># 根据key获取value,若key不存在报异常(defaultdict字典不报异常)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;addr: &quot;</span>, addr)</span><br><span class="line">addr = dict1.setdefault(<span class="string">&quot;addr&quot;</span>)  <span class="comment"># 根据key获取value,若key不存返回None,也可设置默认返回值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;addr: &quot;</span>, addr)</span><br><span class="line">name = dict1.get(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;unknow&quot;</span>)  <span class="comment"># 根据key获取value,若key不存返回默认值&#x27;unknow&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;name: &quot;</span>, name)</span><br></pre></td></tr></table></figure><h4 id="dict的遍历"><a href="#dict的遍历" class="headerlink" title="dict的遍历"></a>dict的遍历</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dict的遍历</span></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> dict1:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;key: %s, value: %s&quot;</span> % (key, dict1[key]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">50</span>)</span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> dict1.values():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;value: &quot;</span>, value)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> dict1.items():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;key: %s, value: %s&quot;</span> % (key, value))</span><br></pre></td></tr></table></figure><h4 id="dict的合并"><a href="#dict的合并" class="headerlink" title="dict的合并"></a>dict的合并</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dict2 = &#123;<span class="string">&quot;mobel&quot;</span>: <span class="number">15888888888</span>, <span class="string">&quot;postal_code&quot;</span>: <span class="number">10000</span>&#125;  <span class="comment"># 合并两个dict</span></span><br><span class="line">dict1.update(dict2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;dict1: &quot;</span>, dict1)</span><br></pre></td></tr></table></figure><h4 id="dict和list的异同"><a href="#dict和list的异同" class="headerlink" title="dict和list的异同"></a>dict和list的异同</h4><blockquote><p><strong>list</strong>查找和插入的时间随着元素的增加而增加；<br>占用空间小，浪费内存很少</p></blockquote><blockquote><p><strong>dict</strong>查找和插入的速度极快，不会随着key的增加而变慢；<br>需要占用大量的内存，内存浪费多。<br>所以，dict是用空间来换取时间的一种方法。</p></blockquote><h4 id="dict的排序"><a href="#dict的排序" class="headerlink" title="dict的排序"></a>dict的排序</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dict排序</span></span><br><span class="line">dict3 = &#123;<span class="string">&#x27;sh&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;hz&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;tj&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;bj&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;gz&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;sz&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;wh&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认排序，并仅返回key</span></span><br><span class="line">key_rank1 = <span class="built_in">sorted</span>(dict3.keys(), reverse=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;key_rank1: &quot;</span>, key_rank1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认排序(以key来排序)，并返回key和value</span></span><br><span class="line">dict_key_rank1 = <span class="built_in">sorted</span>(dict3.items(), reverse=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;dict_key_rank1: &quot;</span>, <span class="built_in">dict</span>(dict_key_rank1))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以key排序</span></span><br><span class="line">dict_key_rank2 = <span class="built_in">sorted</span>(dict3.items(), key=<span class="keyword">lambda</span> item: item[<span class="number">0</span>], reverse=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;dict_key_rank2: &quot;</span>, <span class="built_in">dict</span>(dict_key_rank2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以value排序</span></span><br><span class="line">dict_value_rank1 = <span class="built_in">sorted</span>(dict3.items(), key=<span class="keyword">lambda</span> item: item[<span class="number">1</span>], reverse=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;dict_value_rank1: &quot;</span>, <span class="built_in">dict</span>(dict_value_rank1))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以value排序</span></span><br><span class="line">dict4 = &#123;<span class="string">&#x27;上海&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;杭州&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;天津&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;北京&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;广州&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;深圳&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;武汉&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">dict_value_rank2 = <span class="built_in">sorted</span>(dict4.items(), key=<span class="keyword">lambda</span> item: item[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;dict_value_rank2: &quot;</span>, <span class="built_in">dict</span>(dict_value_rank2))</span><br></pre></td></tr></table></figure><h3 id="tuple元组"><a href="#tuple元组" class="headerlink" title="tuple元组( )"></a>tuple元组( )</h3><p>tuple是**&#x3D;&#x3D;不可变&#x3D;&#x3D;<strong>、</strong>&#x3D;&#x3D;有序&#x3D;&#x3D;**的列表，所以一般在定义tuple时就进行初始化赋值。</p><p>注意：</p><ul><li><p>在定义只有一个元素的tuple时其元素后面要加逗号</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tuple0 = ()  <span class="comment"># 创建空元祖</span></span><br><span class="line">tuple0 = (<span class="number">1</span>)  <span class="comment"># 不是tuple，会当成括号处理</span></span><br><span class="line">tuple0 = (<span class="number">1</span>,)  <span class="comment"># 正确的tuple</span></span><br></pre></td></tr></table></figure></li><li><p>tuple虽然不可变但tuple中的元素对象却是可变的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">my_list = [<span class="string">&quot;x&quot;</span>, <span class="string">&quot;y&quot;</span>]</span><br><span class="line">tuple1 = (<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, my_list)  <span class="comment"># tuple包含list,list变化时,tuple1也就跟着变化</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tuple1: &quot;</span>, tuple1)  <span class="comment"># tuple1:  (&#x27;a&#x27;, &#x27;b&#x27;, [&#x27;x&#x27;, &#x27;y&#x27;])</span></span><br><span class="line">my_list.append(<span class="string">&quot;z&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tuple1 with my_list changed: &quot;</span>, tuple1)  <span class="comment"># tuple1变为(&#x27;a&#x27;, &#x27;b&#x27;, [&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;])</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="tuple的创建"><a href="#tuple的创建" class="headerlink" title="tuple的创建"></a>tuple的创建</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tuple2 = (<span class="number">1</span>, <span class="string">&quot;good&quot;</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="string">&quot;good&quot;</span>, <span class="literal">True</span>)  <span class="comment"># 创建元组,里面的元素类型可以不同</span></span><br><span class="line">tuple3 = (<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, *tuple2, <span class="number">4</span>, <span class="number">5</span>)  <span class="comment"># 元组引用另一个数组中的所有元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tuple3: &quot;</span>, tuple3)</span><br></pre></td></tr></table></figure><h4 id="tuple中元素的增删改查"><a href="#tuple中元素的增删改查" class="headerlink" title="tuple中元素的增删改查"></a>tuple中元素的增删改查</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">element = tuple2[<span class="number">4</span>]  <span class="comment"># 根据索引获取元组中的元素</span></span><br><span class="line">element = tuple2[-<span class="number">2</span>]  <span class="comment"># 使用索引获取元组中的元素</span></span><br><span class="line">index = tuple2.index(<span class="string">&quot;good&quot;</span>)  <span class="comment"># 获取第一个匹配给定值的index值</span></span><br><span class="line"><span class="keyword">del</span> tuple2  <span class="comment"># 删除元组</span></span><br><span class="line"><span class="comment"># tuple2[4] = &quot;well&quot; # 修改元组的元素,报错</span></span><br><span class="line"></span><br><span class="line">tuple4 = (<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="number">4</span>, <span class="number">5</span>, [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tuple4: &quot;</span>, tuple4)</span><br><span class="line"><span class="comment"># tuple4[-1] = [10, 20, 30] #报错</span></span><br><span class="line">tuple4[-<span class="number">1</span>][<span class="number">0</span>] = <span class="number">100</span>  <span class="comment"># 可以通过修改元组中的list,从而改变元组</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tuple4: &quot;</span>, tuple4) <span class="comment"># tuple4:  (&#x27;a&#x27;, &#x27;b&#x27;, 4, 5, [100, 7, 8])</span></span><br></pre></td></tr></table></figure><h4 id="tuple的遍历"><a href="#tuple的遍历" class="headerlink" title="tuple的遍历"></a>tuple的遍历</h4><p>注：tuple的遍历同list</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Python中的数据类型&quot;&gt;&lt;a href=&quot;#Python中的数据类型&quot; class=&quot;headerlink&quot; title=&quot;Python中的数据类型&quot;&gt;&lt;/a&gt;Python中的数据类型&lt;/h1&gt;&lt;h2 id=&quot;可变数据类型&quot;&gt;&lt;a href=&quot;#可变数据类型&quot;</summary>
      
    
    
    
    <category term="工程与实战" scheme="https://www.hicode365.com/categories/%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="python" scheme="https://www.hicode365.com/tags/python/"/>
    
    <category term="数据类型" scheme="https://www.hicode365.com/tags/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>docker常用命令</title>
    <link href="https://www.hicode365.com/1c41d072cac63725dbd48a1273b2fd97"/>
    <id>https://www.hicode365.com/1c41d072cac63725dbd48a1273b2fd97</id>
    <published>2022-11-27T14:28:39.000Z</published>
    <updated>2026-02-22T07:05:09.517Z</updated>
    
    <content type="html"><![CDATA[<h1 id="docker常用命令"><a href="#docker常用命令" class="headerlink" title="docker常用命令"></a>docker常用命令</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">docker pull tensorflow/serving #从仓库拉取镜像</span><br><span class="line">docker pull tensorflow/serving:latest-gpu #从仓库拉取GPU镜像</span><br><span class="line">docker pull tensorflow/serving:2.8.3-gpu #从仓库拉取GPU镜像</span><br><span class="line">docker run -it tensorflow/serving #进入到镜像中</span><br><span class="line">exit #退出镜像</span><br><span class="line">docker run tensorflow/serving #运行某个容器</span><br><span class="line">docker ps // 查看所有正在运行容器</span><br><span class="line">docker stop containerId // containerId 是容器的ID</span><br><span class="line"></span><br><span class="line">docker ps -a // 查看所有容器</span><br><span class="line">docker ps -a -q // 查看所有容器ID</span><br><span class="line"></span><br><span class="line">docker stop $(docker ps -a -q) //  stop停止所有容器</span><br><span class="line">docker rm $(docker ps -a -q) //   remove删除所有容器</span><br><span class="line">docker rm/rmi #删除容器/镜像</span><br><span class="line">docker cp local_files containerId:docker_files #本地文件复制到docker</span><br></pre></td></tr></table></figure><h1 id="tf-serving部署"><a href="#tf-serving部署" class="headerlink" title="tf-serving部署"></a>tf-serving部署</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">-p: 指定主机到docker容器的端口映射</span><br><span class="line">--mount: 表示要进行挂载,其中</span><br><span class="line">type=bind: 是选择挂载模式，</span><br><span class="line">source: 要部署模型的存储路径，也就是挂载的源（必须是绝对路径），</span><br><span class="line">target: 要挂载的目标位置，模型挂载到docker容器中的位置，也就是docker容器中的目录（放在集装箱的哪里）</span><br><span class="line">-t: 指定的是挂载到哪个容器</span><br><span class="line">-e: 环境变量 </span><br><span class="line">MODEL_NAME: 必须与target指定路径的最后一个文件夹名称相同</span><br><span class="line">--per_process_gpu_memory_fraction: 运行时所需的GPU显存资源最大比率的值设定</span><br><span class="line"></span><br><span class="line">-v:</span><br><span class="line">path1:path2 分别指模型在机器种储存的路径（必须是绝对路径），模型在容器中储存的路径（放在集装箱的哪里）</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 8500:8500 \</span><br><span class="line">    --mount type=bind,source=/Users/coreyzhong/workspace/tensorflow/saved_model/,target=/models/test-model \</span><br><span class="line">    -t tensorflow/serving:1.15.0 \</span><br><span class="line">    -e MODEL_NAME=test-model --model_base_path=/models/test-model/ &amp;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model_path=&quot;/Users/havorld/jupyter/model_save/&quot;</span><br><span class="line">docker run -t --rm -p 8500:8500 -p 8501:8501 \</span><br><span class="line">  -v &quot;$model_path/din:/models/tf_saved_models&quot; \</span><br><span class="line">  -e MODEL_NAME=tf_saved_models \</span><br><span class="line">  tensorflow/serving &amp;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">查看TensorFlow-Serving状态： curl http://localhost:8501/v1/models/$&#123;model_name&#125;</span><br><span class="line">查看TensorFlow-Serving模型信息： curl http://localhost:8501/v1/models/$&#123;model_name&#125;/metadata</span><br><span class="line">查看模型信息: saved_model_cli show --dir=&#x27;./$&#123;model_path&#125;/20220422104620&#x27; --all</span><br><span class="line">使用Http请求进行模型预测： </span><br><span class="line">curl -d &#x27;&#123;&quot;instances&quot;: [1,2,3,4,5]&#125;&#x27; -X POST http://localhost:8501/v1/models/$&#123;model_name&#125;:predict</span><br><span class="line">其中instances的value为模型输入Tensor的字符串形式，矩阵维度需要和Tensor对应。</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">docker run -t --rm -p 8501:8501 \</span><br><span class="line">    -v &quot;/home/Personas/havorld/tfserving/tf_saved_models:/models/tf_saved_models&quot; \</span><br><span class="line">    -e MODEL_NAME=tf_saved_models \</span><br><span class="line">    tensorflow/serving</span><br><span class="line"></span><br><span class="line">docker run -t --rm -p 8501:8501 \</span><br><span class="line">-v &quot;/Users/haopeng.meng/jupyter/tf_saved_models:/models/tf_saved_models&quot; \</span><br><span class="line">-e MODEL_NAME=tf_saved_models \</span><br><span class="line">tensorflow/serving</span><br><span class="line"></span><br><span class="line">cat /opt/logs/rec-feed-api/access.log | grep &quot;feed recommend-&gt; uid:55&quot; | grep &quot;id=34174686&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sudo docker run -t --rm -p 8501:8501 -p 8500:8500 \</span><br><span class="line">    -v &quot;/home/meng.haopeng/tfserving/tf_saved_models:/models/tf_saved_models&quot; \</span><br><span class="line">    -e MODEL_NAME=tf_saved_models \</span><br><span class="line">    tensorflow/serving</span><br><span class="line"></span><br><span class="line">docker run -p 8501:8501 -p 8500:8500 \</span><br><span class="line">--mount type=bind,source=/Users/haopeng.meng/jupyter/tf_saved_models,target=/models/tf_saved_models \</span><br><span class="line">-e MODEL_NAME=tf_saved_models \</span><br><span class="line">-t tensorflow/serving</span><br><span class="line"></span><br><span class="line">docker run -p 8500:8500 \</span><br><span class="line">      --mount type=bind,source=./intent/,target=/models/intent_score \</span><br><span class="line">      -e MODEL_NAME=intent_score -t tensorflow/serving:1.10.0</span><br><span class="line"></span><br><span class="line">docker run -p 8501:8501 -p 8500:8500 --mount type=bind,source=/my/model/path/m,target=/models/m -e MODEL_NAME=m -t tensorflow/serving:2.1.0</span><br><span class="line"></span><br><span class="line">sudo docker run -t --rm -p 8501:8501 -p 8500:8500 \</span><br><span class="line">    -v &quot;/home/meng.haopeng/tfserving/tf_saved_models:/models/tf_saved_models&quot; \</span><br><span class="line">    -e MODEL_NAME=tf_saved_models \</span><br><span class="line">    tensorflow/serving</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">docker run -t -p 443:8500 -p 8500:8501 -v <span class="string">&quot;/data/lsj/dmp/SavedModel/:/models/&quot;</span> tensorflow/serving --model_config_file=/models/models.config --model_config_file_poll_wait_seconds=300</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">run use container</span></span><br><span class="line">docker run -t -p 8501:8500 --name=tf_serving_multi_version_01 -v &quot;/data/tf-model/models/:/models/&quot; tensorflow/serving --model_config_file=/models/models.config --model_config_file_poll_wait_seconds=300 --allow_version_labels_for_unavailable_models=true --enable_batching=true --batching_parameters_file=/models/batch.config</span><br><span class="line"></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">tf-serving部署</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker run -t --rm -p 8501:8501 \</span><br><span class="line">-v &quot;/Users/haopeng.meng/jupyter/tf_saved_models:/models/tf_saved_models&quot; \</span><br><span class="line">-e MODEL_NAME=tf_saved_models \</span><br><span class="line">tensorflow/serving</span><br><span class="line"></span><br><span class="line">docker run -p 8501:8501 -p 8500:8500 \</span><br><span class="line">--mount type=bind,source=/Users/haopeng.meng/jupyter/tf_saved_models,target=/models/tf_saved_models \</span><br><span class="line">-e MODEL_NAME=tf_saved_models \</span><br><span class="line">-t tensorflow/serving</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">sudo docker run -t --rm -p 8501:8501 -p 8500:8500 \</span><br><span class="line">    -v &quot;/home/meng.haopeng/tfserving/tf_saved_models:/models/tf_saved_models&quot; \</span><br><span class="line">    -e MODEL_NAME=tf_saved_models \</span><br><span class="line">    tensorflow/serving</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sudo docker run --name feed -t --rm -p 8700:8500 -p 8701:8501 \</span><br><span class="line">    -v &quot;/home/meng.haopeng/tfserving/tf_saved_models:/models/tf_saved_models&quot; \</span><br><span class="line">    -e MODEL_NAME=tf_saved_models \</span><br><span class="line">    tensorflow/serving</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker run --name feed -t --rm -p 8701:8501 -p 8700:8500 \</span><br><span class="line">--mount type=bind,source=/home/Personas/havorld/tfserving/model_save,target=/models/model_save \</span><br><span class="line">-e MODEL_NAME=model_save \</span><br><span class="line">-t tensorflow/serving:latest-gpu </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker run --name feed -t --rm -p 8700:8500 -p 8701:8501 \</span><br><span class="line">--mount type=bind,source=/Users/haopeng.meng/Desktop/recommend/rec-alg-feed/model_save/din/serving/,target=/models/serving \</span><br><span class="line">-e MODEL_NAME=serving \</span><br><span class="line">-t tensorflow/serving</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">docker run --name feed -t --rm -p 8700:8500 -p 8701:8501 \</span><br><span class="line">        --mount type=bind,source=/Users/haopeng.meng/Desktop/recommend/serving/din/,target=/models/serving \</span><br><span class="line">        -e MODEL_NAME=serving \</span><br><span class="line">        -t tensorflow/serving</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;docker常用命令&quot;&gt;&lt;a href=&quot;#docker常用命令&quot; class=&quot;headerlink&quot; title=&quot;docker常用命令&quot;&gt;&lt;/a&gt;docker常用命令&lt;/h1&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;t</summary>
      
    
    
    
    <category term="docker" scheme="https://www.hicode365.com/categories/docker/"/>
    
    
    <category term="docker" scheme="https://www.hicode365.com/tags/docker/"/>
    
    <category term="tfserving" scheme="https://www.hicode365.com/tags/tfserving/"/>
    
  </entry>
  
  <entry>
    <title>git常用命令</title>
    <link href="https://www.hicode365.com/cuidrRz6-aJ020Ll6zs2_iuIT"/>
    <id>https://www.hicode365.com/cuidrRz6-aJ020Ll6zs2_iuIT</id>
    <published>2021-09-28T03:05:35.000Z</published>
    <updated>2023-08-15T02:10:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="git常用命令"><a href="#git常用命令" class="headerlink" title="git常用命令"></a>git常用命令</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/username/project.github.io.git #拉取代码(master/main)</span><br><span class="line">git clone -b _dev https://github.com/username/project.github.io.git  #拉取分支(非master或main分支)</span><br><span class="line">git checkout --track origin/_remote  #获取指定的远程分支到本地</span><br><span class="line"></span><br><span class="line">git branch #查看本地分支</span><br><span class="line">git branch -a #查看远程分支</span><br><span class="line">git branch -vv#查看分支的绑定信息</span><br><span class="line"></span><br><span class="line">git branch _local #创建本地分支</span><br><span class="line">git branch -d _local #删除本地分支(当前分支不能停留在要删除的分支上)</span><br><span class="line">git checkout _local #切换到本地分支</span><br><span class="line">git checkout -b _local # 创建并切换到本地创建的分支</span><br><span class="line"></span><br><span class="line">git push --set-upstream origin _remote #创建远程分支</span><br><span class="line">git branch -r -d origin/_remote  #删除远程分支(记得push一下 git push origin _remote)</span><br><span class="line">git checkout -b _local origin/_remote #创建本地分支绑定远程分支</span><br><span class="line"></span><br><span class="line">git status #查看修改过代码的类</span><br><span class="line">git diff #查看修改的代码</span><br><span class="line"></span><br><span class="line">git add src/main/java/com/so/alg/RecommendServiceImpl.java #添加修改的代码</span><br><span class="line">git commit -m &quot;recommend feed modified&quot; #给修改的代码添加注释</span><br><span class="line">git pull origin _remote #从远程更新变动的代码</span><br><span class="line">git push origin _local:_remote #提交</span><br><span class="line"></span><br><span class="line">git merge _remote #合并分支到master上(需要先切换到master分支上,在执行合并)</span><br><span class="line"></span><br><span class="line">git tag -a 2020071801 -m &#x27;v2.0部署&#x27; #添加tag</span><br><span class="line">git show 2020071801 #展示tag</span><br><span class="line">git push --tags  #提交tag</span><br></pre></td></tr></table></figure><h2 id="git配置SSH公钥和私钥"><a href="#git配置SSH公钥和私钥" class="headerlink" title="git配置SSH公钥和私钥"></a>git配置SSH公钥和私钥</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1.生成公钥和私钥(邮箱为github注册邮箱)</span></span><br><span class="line">ssh-keygen -t rsa -C &quot;xxxxxx@gmail.com&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2.设置公钥私钥key的保存位置(可以直接确认则保存在默认位置)</span></span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/Users/username/.ssh/id_rsa):</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">3.输入、重复输入密钥盐值</span></span><br><span class="line">Enter passphrase (empty for no passphrase):</span><br><span class="line">Enter same passphrase again:</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">4.复制打印的公钥内容，并在github-&gt;Settings-&gt;SSH and GPG keys-&gt;New SSH Key中设置(title随意起)</span></span><br><span class="line">cat .ssh\id_rsa.pub</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">5.查看密钥是否配置成功(会提示输入盐值)</span></span><br><span class="line">ssh -T git@github.com</span><br><span class="line">Enter passphrase for key &#x27;/Users/username/.ssh/id_rsa&#x27;:</span><br><span class="line">Hi hicode360! You&#x27;ve successfully authenticated, but GitHub does not provide shell access.</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">6.配置全局git信息</span></span><br><span class="line">git config --global user.name &quot;username&quot; # github用户名</span><br><span class="line">git config --global user.email  &quot;xxxxxx@gmail.com&quot; #github注册邮箱</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;git常用命令&quot;&gt;&lt;a href=&quot;#git常用命令&quot; class=&quot;headerlink&quot; title=&quot;git常用命令&quot;&gt;&lt;/a&gt;git常用命令&lt;/h2&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=</summary>
      
    
    
    
    <category term="git" scheme="https://www.hicode365.com/categories/git/"/>
    
    
    <category term="git" scheme="https://www.hicode365.com/tags/git/"/>
    
    <category term="命令" scheme="https://www.hicode365.com/tags/%E5%91%BD%E4%BB%A4/"/>
    
  </entry>
  
</feed>
