{"meta":{"title":"hicode","subtitle":"愿你出走半生 归来仍是少年","description":"写下理解留下轨迹，用工程沉淀算法思考对抗复杂，在算法与工程交汇处构建可落地的智能。持续学习，长期主义，做可复利的技术人。","author":"hicode365","url":"https://www.hicode365.com","root":"/"},"pages":[{"title":"about","date":"2021-09-27T09:02:54.000Z","updated":"2026-02-21T10:23:27.271Z","comments":true,"path":"about/index.html","permalink":"https://www.hicode365.com/about/index.html","excerpt":"","text":"写下理解，留下轨迹。 用工程沉淀算法，思考对抗复杂。 在算法与工程交汇处，构建可落地的智能。 持续学习，长期主义，做可复利的技术人。"},{"title":"tags","date":"2021-09-27T08:55:32.000Z","updated":"2023-08-15T02:10:06.000Z","comments":true,"path":"tags/index.html","permalink":"https://www.hicode365.com/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2021-09-27T08:54:47.000Z","updated":"2023-08-15T02:10:06.000Z","comments":true,"path":"categories/index.html","permalink":"https://www.hicode365.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"SVM支持向量机","slug":"机器学习/SVM支持向量机","date":"2026-02-22T09:48:39.749Z","updated":"2026-02-22T12:07:05.830Z","comments":true,"path":"cuid9ineUcg_C5I5d5AUzIiOJ.html","permalink":"https://www.hicode365.com/cuid9ineUcg_C5I5d5AUzIiOJ","excerpt":"","text":"引言支持向量机（Support Vector Machine, SVM）是一种经典的二分类模型，它的核心思想是：在特征空间中寻找一个决策边界（超平面），使得两类样本被正确分开，并且离决策边界最近的样本点（即支持向量）到该边界的距离尽可能大。这个距离被称为间隔（Margin），SVM因此也被称为最大间隔分类器。 决策函数$$ \\left\\{ (x_1^{(1)}, x_2^{(1)}, \\ldots, x_n^{(1)}), (x_1^{(2)}, x_2^{(2)}, \\ldots, x_n^{(2)}), \\ldots, (x_1^{(m)}, x_2^{(m)}, \\ldots, x_n^{(m)}) \\right\\} $$ 这是一个样本集，包含 $ m $ 个样本，每个样本有 $ n $ 个特征。 第 $ i $ 个样本为：$(x_1^{(i)}, x_2^{(i)}, \\ldots, x_n^{(i)})$ 上标 $(i)$ 表示第 $ i $ 个样本 下标 $ j $ 表示该样本的第 $ j $ 个特征 所以 $ x_j^{(i)} $ 表示 第 $ i $ 个样本的第 $ j $ 个特征值 矩阵表示法(每一行是一个样本，每一列是一个特征)：$$ \\mathbf{X} = \\begin{bmatrix} x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\ x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\end{bmatrix} $$ SVM的决策函数为线性函数： $$ f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b $$ 其中 $\\mathbf{w} \\in \\mathbb{R}^n$ 是权重向量（法向量），$b \\in \\mathbb{R}$ 是偏置项。决策边界（超平面）由方程 $\\mathbf{w}^\\top \\mathbf{x} + b = 0$ 定义。 对于二维情况，决策边界是一条直线；对于三维情况，它是一个平面；更高维则称为超平面，如： 当样本有两个特征时（即2维），决策边界为直线： $$ w_1x_1 + w_2x_2 + b = 0 $$ 当三个（或多个）特征时，决策边界为平面：$$ w_1x_1 + w_2x_2 + w_3x_3 + b = 0 $$ 分类规则：把 $x^{(i)}$ 代入决策方程： 得到的值$\\mathbf{w}^\\top \\mathbf{x} + b > 0$： $y^{(i)}$ 为正例，$y^{(i)} = +1$； 得到的值$\\mathbf{w}^\\top \\mathbf{x} + b < 0$ ： $y^{(i)}$ 为负例，$y^{(i)} = -1$； 因此，正确分类的样本满足 $y^{(i)}(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b) > 0$。 间隔（Margin）与优化目标距离计算点到直线的距离在二维空间中，假设有一条直线的方程为 $ ax + by + c = 0 $，以及一个点 $P(x_0, y_0) $。该点到直线的距离 $d $ 可以通过以下公式计算： $$ d = \\frac{|ax_0 + by_0 + c|}{\\sqrt{a^2 + b^2}} $$这里，$a, b, c $ 是直线方程的系数，而 $x_0, y_0 $ 是点的坐标。 点到平面的距离在三维空间中，假设有一个平面的方程为 $Ax + By + Cz + D = 0 $，以及一个点 $P(x_0, y_0, z_0) $。该点到平面的距离 $d $ 可以通过以下公式计算： $$ d = \\frac{|Ax_0 + By_0 + Cz_0 + D|}{\\sqrt{A^2 + B^2 + C^2}} $$这里，$A, B, C, D $ 是平面方程的系数，而 $x_0, y_0, z_0 $ 是点的坐标。 注： 在这两个公式中，分子部分表示的是点代入直线或平面方程后的值的绝对值，它反映了点相对于直线或平面的位置。 分母部分是对直线或平面法向量长度的计算（即向量 $(a, b) $ 或 $(A, B, C) $ 的模长），用于标准化距离。 函数间隔函数间隔（Functional Margin）的定义 对于一个样本 $(\\mathbf{x}^{(i)}, y^{(i)})$，其中 $y^{(i)} \\in \\{+1, -1\\}$，我们定义其函数间隔为： $$ \\hat{\\gamma}_i = y^{(i)} (\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b) $$ $\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b$ 是模型对样本 $\\mathbf{x}^{(i)}$ 的原始打分（score）。 如果分类正确$ y^{(i)}$ 与 $ \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b$ 同号，它的绝对值越大，说明分类的置信度越高。 几何间隔（优化目标）我们以二维决策边界为例：$$ w_1x_1 + w_2x_2 + b = 0，其中 \\mathbf{w}=[w_1,w_2] $$点到决策边界直线的距离为：$$ \\gamma_i = d_i= \\frac{|w_1x_1^{(i)} + w_2x_1^{(i)} + b|}{\\sqrt{w_1^2 + w_2^2}} = y^{(i)} \\frac{w_1x_1^{(i)} + w_2x_1^{(i)} + b}{\\sqrt{w_1^2 + w_2^2}} = y^{(i)} \\frac{\\mathbf{wx} + b}{\\sqrt{w_1^2 + w_2^2}} = = y^{(i)} \\frac{\\mathbf{wx} + b}{\\|\\mathbf{w}\\|} $$ $\\|\\mathbf{w}\\|$ 为L2范数（也称为欧几里得范数）定义为: $\\|\\mathbf{w}\\| = \\sqrt{w_1^2 + w_2^2}$ ， 表示的是权重向量 $ \\mathbf{w} $ 的长度。 所以几何间隔为：$$ \\gamma_i = y^{(i)} \\frac{\\mathbf{wx} + b}{\\|\\mathbf{w}\\|} = \\frac{1}{\\|\\mathbf{w}\\|} y^{(i)}{(\\mathbf{wx} + b)} = \\frac{1}{\\|\\mathbf{w}\\|} \\hat{\\gamma}_i $$我们的优化目标是最大化距离决策边界最小的样本的间隔值。 函数间隔VS几何间隔$$ \\gamma_i = \\frac{\\hat{\\gamma}_i}{\\|\\mathbf{w}\\|} $$ 特性 函数间隔 $\\hat{\\gamma}_i$ 几何间隔 $\\gamma_i$ 是否依赖 $\\mathbf{w}$ 的尺度 是（可任意缩放） 否（尺度不变） 是否表示真实距离 否 是 优化目标 不直接用于最大化（因可缩放） 用于 SVM 的核心目标：最大化最小几何间隔 示例： 假设 $\\mathbf{w} = (2, 2), b = -2$，对某个样本 $\\mathbf{x} = (1,1), y=+1$： 函数间隔：$\\hat{\\gamma} = 1 \\cdot (2*1 + 2*1 - 2) = 2$ 几何间隔：$\\gamma = 2 / \\sqrt{2^2 + 2^2} = 2 / \\sqrt{8} = \\frac{1}{\\sqrt{2}}$ 现在把 $\\mathbf{w}, b$ 都乘以 10： $\\mathbf{w}' = (20,20), b' = -20$ 函数间隔变为：$20$（变大了！） 但几何间隔仍是：$20 / \\sqrt{800} = 20 / (10\\sqrt{8}) = 2 / \\sqrt{8} = \\frac{1}{\\sqrt{2}}$ 几何间隔不变，函数间隔随参数缩放而变。 最大间隔思想SVM的目标是：找到一个超平面 $(\\mathbf{w},b)$，使得所有样本中离它最近的那个样本的几何间隔尽可能大。这个最近的样本点就是支持向量（Support Vector）。 整个数据集的几何间隔定义为： $$ \\gamma = \\min_{i=1,\\dots,m} \\gamma_i $$ 优化问题即为： $$ \\max_{\\mathbf{w},b} \\gamma = \\max_{\\mathbf{w},b} \\min_{i} \\frac{y^{(i)}(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b)}{\\|\\mathbf{w}\\|} $$ 目标函数与化简SVM 的核心思想是：找到一个决策边界，使得所有样本中离它最近的那个样本（即支持向量）的距离尽可能大。 单个样本的几何间隔：$$ \\gamma_i = \\frac{1}{\\|\\mathbf{w}\\|} y^{(i)}(\\mathbf{w}^\\top \\mathbf{x}_i + b) $$ 整个数据集的几何间隔是所有样本中最小的那个：$$ \\gamma = \\min_{i=1,\\dots,m} \\gamma_i $$ 我们的目标是找到能最大化这个最小几何间隔的参数：$$ \\max_{\\mathbf{w}, b} \\gamma = \\max_{\\mathbf{w}, b} \\min_{i=1,\\dots,m} \\frac{1}{\\|\\mathbf{w}\\|} y^{(i)}(\\mathbf{w}^\\top \\mathbf{x}_i + b) $$ 对间隔进行缩放： 性质：对同一超平面，对参数（$\\mathbf{w}$ 和 $b$ ）同时进行缩放，几何间隔保持不变，函数间隔发生变化。 $$ \\gamma_i = \\frac{1}{\\|\\mathbf{w}\\|} y^{(i)}(\\mathbf{w}^\\top \\mathbf{x}_i + b) $$ 设对任意正实数 $c>0$，把参数变为 $(\\mathbf w',b')=(c\\mathbf w,c b)$。 超平面方程由 $\\mathbf w^\\top \\mathbf x + b =0$ 变为 $(c\\mathbf w)^\\top \\mathbf x + c b = c(\\mathbf w^\\top \\mathbf x + b)$。零点集不变，因此超平面本身不变（同一条直线&#x2F;平面）。 函数边距变为$$ \\hat\\gamma_i' = y_i((c\\mathbf w)^\\top \\mathbf x_i + c b) = c, y_i(\\mathbf w^\\top \\mathbf x_i + b) = c,\\hat\\gamma_i. $$所以函数边距按同样因子 (c) 缩放。 $\\mathbf w 的范数变成 |\\mathbf w'|=|c\\mathbf w| = c|\\mathbf w|$。 几何边距变为$$ \\gamma_i' = \\frac{\\hat\\gamma_i'}{|\\mathbf w'|} = \\frac{c\\hat\\gamma_i}{c|\\mathbf w|} = \\frac{\\hat\\gamma_i}{|\\mathbf w|} = \\gamma_i. $$ 也就是说，几何边距不变，函数间隔变为原来的c倍。 我们对 $\\max_{\\mathbf{w}, b} \\gamma = \\max_{\\mathbf{w}, b} \\min_{i=1,\\dots,m} \\frac{1}{\\|\\mathbf{w}\\|} y^{(i)}(\\mathbf{w}^\\top \\mathbf{x}_i + b)$ 中的$\\frac{1}{\\|\\mathbf{w}\\|} y^{(i)}(\\mathbf{w}^\\top \\mathbf{x}_i + b)$ 进行缩放（不会改变原来的结果值）,缩放到 $y^{(i)}(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1$，此时：$$ \\begin{aligned} &\\max_{\\mathbf{w}, b} \\gamma = \\max_{\\mathbf{w}, b} \\min_{i=1,\\dots,m} \\frac{1}{\\|\\mathbf{w}\\|} y^{(i)}(\\mathbf{w}^\\top \\mathbf{x}_i + b) = \\max_{\\mathbf{w}, b} \\min_{i=1,\\dots,m} \\frac{1}{\\|\\mathbf{w}\\|} \\cdot 1 = \\max_{\\mathbf{w}, b}\\frac{1}{\\|\\mathbf{w}\\|} = \\min_{\\mathbf{w}, b}{\\|\\mathbf{w}\\|} = \\min_{\\mathbf{w}, b}\\frac{1}{2}{\\|\\mathbf{w}\\|}^2 \\\\ &\\text{s.t.}\\quad y_i(\\mathbf w^\\top \\mathbf x_i + b) \\ge 1,\\quad i=1,\\dots,m. \\end{aligned} $$所以，SVM优化目标的的原始形式为：$$ \\begin{aligned} &\\min_{\\mathbf{w}, b}\\frac{1}{2}{\\|\\mathbf{w}\\|}^2 \\\\ &\\text{s.t.}\\quad y_i(\\mathbf w^\\top \\mathbf x_i + b) \\ge 1,\\quad i=1,\\dots,m. \\end{aligned} $$加入软间隔，在目标里加上松弛项并引入松弛变量 $\\xi_i$ 的优化目标形式为：$$ \\begin{aligned} &\\min \\tfrac{1}{2}|\\mathbf w|^2 + C\\sum_{i=1}^n\\xi_i \\\\ &\\text{s.t.}\\quad y_i(\\mathbf w^\\top \\mathbf x_i + b) \\ge 1-\\xi_i,\\ \\xi_i\\ge 0, \\quad i=1,\\dots,m. \\end{aligned} $$ 硬间隔与软间隔 硬间隔 前提：数据完全线性可分。 目标：找到一个超平面，使得所有样本被正确分类，且间隔最大。 数学形式： $$ \\begin{aligned} &\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\\\ &\\text{s.t. } y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1,\\quad i=1,\\dots,m \\end{aligned} $$ 其中 $y_i \\in \\{-1, +1\\}$ 软间隔 现实情况：数据可能含噪声或轻微不可分。 引入松弛变量 $\\xi_i \\geq 0$，允许部分样本违反约束。 目标函数加入正则项（控制间隔与误分类的权衡）：$$ \\begin{aligned} &\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^m \\xi_i \\\\ &\\text{s.t. } y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1 - \\xi_i,\\quad \\xi_i \\geq 0 \\end{aligned} $$ $C$ 是惩罚系数：$C$ 越大，对误分类越敏感（趋向硬间隔）；$C$ 小则容忍更多误分类。 如果 $ ξ_i = 0 $，点被正确分类且在间隔之外。 如果 $ 0 < ξ_i 1 $，点被错误分类。 拉格朗日对偶求解（硬间隔）拉格朗日函数上述我们已经得到SVM的优化目标函数为:$$ \\begin{aligned} &\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\\\ &\\text{s.t. } y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) - 1 \\geq 0,\\quad i=1,\\dots,m \\end{aligned} $$目标函数是一个带不等式约束的凸二次规划（QP）问题。我们可以用拉格朗日乘子法将其转化为一个更易于求解的对偶问题。 步骤 1： 引入拉格朗日函数 为每个不等式约束引入一个拉格朗日乘子 $\\alpha_i \\ge 0$，构建拉格朗日函数：$$ \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha}) = \\frac{1}{2} \\|\\mathbf{w}\\|^2 - \\sum_{i=1}^m \\alpha_i \\left[ y_i (\\mathbf{w}^T \\mathbf{x}_i + b) - 1 \\right] $$ $\\boldsymbol{\\alpha} = (\\alpha_1, \\dots, \\alpha_m)^T$ 从上述式子不难看出：$\\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha}) \\leq \\frac{1}{2} \\|\\mathbf{w}\\|^2$ 根据拉格朗日乘子法的性质，原始问题等价于以下无约束的极小极大问题，所以目标函数转换为：$$ \\min_{\\mathbf{w}, b} \\max_{\\boldsymbol{\\alpha} \\ge 0} \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha}) $$步骤 2： 转化为对偶问题（Dual Problem） 原始问题是 $\\min_{\\mathbf{w}, b} \\max_{\\boldsymbol{\\alpha} \\ge 0} \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha})$。在满足KKT条件的情况下，我们可以等价地求解它的对偶问题：$$ \\min_{\\mathbf{w}, b} \\max_{\\boldsymbol{\\alpha} \\ge 0} \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha}) \\ → \\ \\max_{\\boldsymbol{\\alpha} \\ge 0} \\min_{\\mathbf{w}, b} \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha}) $$ 首先，求解内层的最小化问题： $\\min_{\\mathbf{w}, b} \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha})$ 分别对 $\\mathbf{w}$ 和 $b$ 求偏导并令其为零：$$ \\nabla_{\\mathbf{w}} \\mathcal{L} = \\mathbf{w} - \\sum_{i=1}^m \\alpha_i y_i \\mathbf{x}_i = 0 \\quad \\Rightarrow \\quad \\mathbf{w} = \\sum_{i=1}^m \\alpha_i y_i \\mathbf{x}_i $$ $$ \\nabla_{\\mathbf{b}} \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial b} = - \\sum_{i=1}^m \\alpha_i y_i = 0 \\quad \\Rightarrow \\quad \\sum_{i=1}^m \\alpha_i y_i = 0 $$ 将这两个结果代回拉格朗日函数 $\\mathcal{L}$： $$ \\begin{aligned} \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha}) &= \\frac{1}{2} \\left( \\sum_{i=1}^m \\alpha_i y_i \\mathbf{x}_i \\right)^T \\left( \\sum_{j=1}^m \\alpha_j y_j \\mathbf{x}_j \\right) - \\sum_{i=1}^m \\alpha_i y_i \\mathbf{x}_i^T \\left( \\sum_{j=1}^m \\alpha_j y_j \\mathbf{x}_j \\right) - b \\sum_{i=1}^m \\alpha_i y_i + \\sum_{i=1}^m \\alpha_i \\\\ &= \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j - \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j + \\sum_{i=1}^m \\alpha_i \\\\ &= -\\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j + \\sum_{i=1}^m \\alpha_i \\end{aligned} $$（注意：中间项 $- b \\sum_{i=1}^m \\alpha_i y_i$ 因为 $\\sum \\alpha_i y_i = 0$ 而消失了。） 步骤 3： 得到对偶问题 现在，我们的目标变成了最大化上面这个结果，同时要满足约束：$$ \\begin{aligned} & \\max_{\\boldsymbol{\\alpha}} \\quad -\\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j + \\sum_{i=1}^m \\alpha_i \\\\ & \\text{s.t.} \\quad \\sum_{i=1}^m \\alpha_i y_i = 0, \\\\ & \\quad \\quad \\alpha_i \\ge 0, \\quad i = 1, \\dots, m \\end{aligned} $$通常我们将最大化转换为最小化，得到SVM标准对偶问题的最终形式： $$ \\begin{aligned} & \\min_{\\boldsymbol{\\alpha}} \\quad \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j - \\sum_{i=1}^m \\alpha_i \\\\ & \\text{s.t.} \\quad \\sum_{i=1}^m \\alpha_i y_i = 0, \\\\ & \\quad \\quad \\alpha_i \\ge 0, \\quad i = 1, \\dots, m \\end{aligned} $$ 然后就是根据所有样本的值带入上式（还有约束条件），求得 $\\alpha$，再根据 $w$ 与$\\alpha$的关系求得 $w$值。 在根据 $y = \\mathbf{w}^\\top \\mathbf{x}_i + b$ 随边找一个样本点带入求得 $b$ 值。 $\\alpha$的意义： 所有$\\alpha_i$ 不为零的点就是支持向量，也就是位于边界上的点，非边界上的点$\\alpha = 0$ KKT条件与支持向量求解上述对偶问题后，我们得到最优的拉格朗日乘子 $\\boldsymbol{\\alpha}^*$。此时，KKT条件（最优解必须满足的条件）起着关键作用，特别是其中的互补松弛条件：$$ \\alpha_i^* \\left[ y_i (\\mathbf{w}^T \\mathbf{x}_i + b) - 1 \\right] = 0, \\quad \\forall i $$这个条件意味着： 如果 $\\alpha_i^* = 0$，那么对应的样本点不会对 $\\mathbf{w}$ 的计算产生影响（见 $\\mathbf{w} = \\sum \\alpha_i y_i \\mathbf{x}_i$）。 如果 $\\alpha_i^* > 0$，那么必然有 $y_i (\\mathbf{w}^T \\mathbf{x}_i + b) = 1$。这意味着这个样本点正好在最大间隔边界上！ 这些 $\\alpha_i^* > 0$ 所对应的样本点，就是支持向量（Support Vectors）。它们决定了最终的超平面。 求解 $\\mathbf{w}$ 和 $b$： $\\mathbf{w}$： 我们已经从导数为零的条件得到了：$$ \\mathbf{w}^* = \\sum_{i=1}^m \\alpha_i^* y_i \\mathbf{x}_i $$实际上，我们只需要支持向量（即 $\\alpha_i^* > 0$ 的点）来计算：$$ \\mathbf{w}^* = \\sum_{i \\in SV} \\alpha_i^* y_i \\mathbf{x}_i $$ $b$： 利用任何一个支持向量 $(\\mathbf{x}_s, y_s)$（即满足 $\\alpha_s^* > 0$ 的点），根据互补松弛条件：$$ y_s (\\mathbf{w}^{*T} \\mathbf{x}_s + b^*) = 1 $$可以解出：$$ b^* = y_s - \\mathbf{w}^{*T} \\mathbf{x}_s $$为了数值稳定性，通常使用所有支持向量计算出的 $b$ 的平均值。 拉格朗日对偶与求解（软间隔）松弛变量与惩罚参数优化问题（原始问题，Primal Problem with Soft Margin）： 由于数据可能不是线性可分的，我们引入松弛变量（Slack Variables） $\\xi_i \\ge 0$ 来允许一些样本犯错误。同时，在目标函数中加入对这些错误的惩罚项。优化问题变为： $$ \\begin{aligned} & \\min_{\\mathbf{w}, b, \\boldsymbol{\\xi}} \\quad \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^m \\xi_i \\\\ & \\text{s.t.} \\quad y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\ge 1 - \\xi_i, \\quad i = 1, \\dots, m \\\\ & \\quad \\quad \\xi_i \\ge 0, \\quad i = 1, \\dots, m \\end{aligned} $$ 这里： $\\xi_i$ 是松弛变量。$\\xi_i = 0$ 表示样本 $x_i$ 被正确分类且位于间隔边界之外；$0 < \\xi_i \\le 1$ 表示样本位于间隔内部，但在正确的一侧；$\\xi_i > 1$ 表示样本被错误分类。 $C > 0$ 是惩罚参数，由用户指定。它控制了我们对分类错误的容忍度： $C$ 越大，对错误的惩罚越重，间隔带越“硬”，可能过拟合。 $C$ 越小，对错误的惩罚越轻，间隔带越“宽软”，可能欠拟合。 构建拉格朗日函数： 我们现在有两个不等式约束，因此引入两组拉格朗日乘子：$\\alpha_i \\ge 0$ 和 $\\mu_i \\ge 0$。 $$ \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\xi}, \\boldsymbol{\\alpha}, \\boldsymbol{\\mu}) = \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^m \\xi_i - \\sum_{i=1}^m \\alpha_i \\left[ y_i (\\mathbf{w}^T \\mathbf{x}_i + b) - 1 + \\xi_i \\right] - \\sum_{i=1}^m \\mu_i \\xi_i $$ 转化为对偶问题： 同样，我们求解 $\\min_{\\mathbf{w}, b, \\boldsymbol{\\xi}} \\max_{\\boldsymbol{\\alpha}, \\boldsymbol{\\mu} \\ge 0} \\mathcal{L}$ 的对偶问题 $\\max_{\\boldsymbol{\\alpha}, \\boldsymbol{\\mu} \\ge 0} \\min_{\\mathbf{w}, b, \\boldsymbol{\\xi}} \\mathcal{L}$。 对 $\\mathbf{w}, b, \\xi_i$ 求偏导并令为零：$$ \\nabla_{\\mathbf{w}} \\mathcal{L} = \\mathbf{w} - \\sum_{i=1}^m \\alpha_i y_i \\mathbf{x}_i = 0 \\quad \\Rightarrow \\quad \\mathbf{w} = \\sum_{i=1}^m \\alpha_i y_i \\mathbf{x}_i $$ $$ \\frac{\\partial \\mathcal{L}}{\\partial b} = - \\sum_{i=1}^m \\alpha_i y_i = 0 \\quad \\Rightarrow \\quad \\sum_{i=1}^m \\alpha_i y_i = 0 $$ $$ \\frac{\\partial \\mathcal{L}}{\\partial \\xi_i} = C - \\alpha_i - \\mu_i = 0 \\quad \\Rightarrow \\quad \\alpha_i + \\mu_i = C $$ 将结果代回拉格朗日函数：将 $\\mathbf{w} = \\sum \\alpha_i y_i \\mathbf{x}_i$， $\\mu_i = C - \\alpha_i$ 代入 $\\mathcal{L}$。经过与硬间隔类似的化简过程（注意包含 $\\xi_i$ 的项会相互抵消），我们得到： 对偶问题（Dual Problem with Soft Margin）：$$ \\boxed{ \\begin{aligned} & \\min_{\\boldsymbol{\\alpha}} \\quad \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j - \\sum_{i=1}^m \\alpha_i \\\\ & \\text{s.t.} \\quad \\sum_{i=1}^m \\alpha_i y_i = 0, \\\\ & \\quad \\quad 0 \\le \\alpha_i \\le C, \\quad i = 1, \\dots, m \\end{aligned} } $$ 关键变化： 与硬间隔相比，软间隔的对偶问题唯一的变化是约束条件从 $\\alpha_i \\ge 0$ 变成了 $0 \\le \\alpha_i \\le C$。这个上界 $C$ 来自于关系式 $\\alpha_i = C - \\mu_i$ 和 $\\mu_i \\ge 0$。 KKT条件与支持向量对于软间隔SVM，KKT条件（最优解必须满足的条件）变得更加丰富，它们完整地描述了支持向量的不同类型。 KKT条件： 平稳性： $\\mathbf{w} = \\sum_{i=1}^m \\alpha_i y_i \\mathbf{x}_i$, $\\sum_{i=1}^m \\alpha_i y_i = 0$, $\\alpha_i + \\mu_i = C$. 原始可行性： $y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\ge 1 - \\xi_i$, $\\xi_i \\ge 0$. 对偶可行性： $\\alpha_i \\ge 0$, $\\mu_i \\ge 0$. 互补松弛性：$$ \\alpha_i \\left[ y_i (\\mathbf{w}^T \\mathbf{x}_i + b) - 1 + \\xi_i \\right] = 0 $$ $$ \\mu_i \\xi_i = 0 $$ 根据这些KKT条件，我们可以将样本点分为三类： $\\alpha_i$ 的值 $\\xi_i$ 的值 几何位置 类型 $\\alpha_i = 0$ $\\xi_i = 0$ 被正确分类，在间隔边界之外 非支持向量 $0 < \\alpha_i < C$ $\\xi_i = 0$ 恰好落在间隔边界上 ($y_i(\\mathbf{w}^T\\mathbf{x}_i + b) = 1$) 标准支持向量 $\\alpha_i = C$ $0 < \\xi_i \\le 1$ 位于间隔内部，但在正确的一侧 边界支持向量 $\\alpha_i = C$ $\\xi_i > 1$ 在错误的一侧，被误分类 误分类支持向量 求解 $\\mathbf{w}$ 和 $b$： $\\mathbf{w}^*$ 的求解与硬间隔相同，只依赖于支持向量 ($\\alpha_i > 0$)：$$ \\mathbf{w}^* = \\sum_{i=1}^m \\alpha_i^* y_i \\mathbf{x}_i = \\sum_{i \\in SV} \\alpha_i^* y_i \\mathbf{x}_i $$ $b^*$ 的求解：在硬间隔中，我们使用任意一个支持向量。在软间隔中，为了数值稳定性，我们通常使用所有标准支持向量 ($0 < \\alpha_i < C$) 来计算 $b$ 的平均值。对于任何一个标准支持向量，由于 $\\xi_i = 0$ 且 $0 < \\alpha_i < C$，根据互补松弛条件有：$$ y_i (\\mathbf{w}^{*T} \\mathbf{x}_i + b^*) = 1 $$因此，$$ b^* = \\frac{1}{|S|} \\sum_{i \\in S} (y_i - \\mathbf{w}^{*T} \\mathbf{x}_i) $$其中 $S = \\{ i | 0 < \\alpha_i < C \\}$ 是所有标准支持向量的集合。 最终的决策函数（软间隔版本）软间隔SVM的最终决策函数在形式上与硬间隔完全相同： $$ f(\\mathbf{x}) = \\text{sign}(\\mathbf{w}^{*T} \\mathbf{x} + b^*) $$ 将 $\\mathbf{w}^* = \\sum_{i \\in SV} \\alpha_i^* y_i \\mathbf{x}_i$ 代入： $$ \\boxed{ f(\\mathbf{x}) = \\text{sign}\\left( \\sum_{i \\in SV} \\alpha_i^* y_i \\mathbf{x}_i^T \\mathbf{x} + b^* \\right) } $$ 关键点： 形式不变：决策函数仍然是支持向量的线性组合。 支持向量更多样：支持向量集合 $SV$ 现在包含了所有 $\\alpha_i > 0$ 的样本，即标准支持向量和那些被允许“犯错”的边界&#x2F;误分类支持向量。正是这些“犯错”的样本使得模型获得了鲁棒性。 核函数应用：同样，我们可以将内积 $\\mathbf{x}_i^T \\mathbf{x}$ 替换为核函数 $K(\\mathbf{x}_i, \\mathbf{x})$，从而将软间隔SVM应用于非线性分类问题，这被称为非线性软间隔SVM。 软间隔 vs.硬间隔 特性 硬间隔SVM 软间隔SVM 适用场景 数据严格线性可分 数据近似线性可分或存在噪声 优化目标 $\\min \\frac{1}{2}\\|\\mathbf{w}\\|^2$ $\\min \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum \\xi_i$ 对偶约束 $0 \\le \\alpha_i$ $0 \\le \\alpha_i \\le C$ 支持向量 都在间隔边界上 在间隔边界上、内部或误分点 参数 无 惩罚参数 $C$ 核函数（Kernel Function）低维不可分与高维映射 当数据在原始特征空间中线性不可分时，一个自然的想法是将数据映射到更高维的空间，使其变得线性可分。设映射函数为 $\\phi(\\mathbf{x})$，则在高维空间中的决策函数为：$$ f(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x}) + b $$ 核技巧（Kernel Trick）直接计算 $\\phi(\\mathbf{x})$ 可能非常复杂，甚至无限维。但注意到在对偶问题中，所有涉及样本的地方都是以内积形式出现：$(\\mathbf{x}^{(i)})^\\top \\mathbf{x}^{(j)}$。如果存在一个函数 $K(\\cdot,\\cdot)$，使得： $$ K(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)}) = \\phi(\\mathbf{x}^{(i)})^\\top \\phi(\\mathbf{x}^{(j)}) $$ 那么我们就可以在低维空间中计算核函数 $K$，而无需显式地计算高维映射 $\\phi$，从而大大降低计算量。这就是核技巧。 常用核函数 线性核：$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^\\top \\mathbf{x}_j$（即无映射，用于线性可分情况） 多项式核：$K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i^\\top \\mathbf{x}_j + r)^d$ 高斯核（RBF核）：$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2)$（最常用，可将特征映射到无穷维） Sigmoid核：$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\gamma \\mathbf{x}_i^\\top \\mathbf{x}_j + r)$ 核函数的选择通常依赖领域知识或交叉验证。 核函数的选取原则 若特征数 $n$ 很大，而样本数 $m$ 较小，通常使用线性核（避免过拟合）； 若 $n$ 适中，$m$ 也适中，可尝试高斯核； 若 $n$ 很小，$m$ 很大，需要先手工增加特征，再考虑线性核。 硬间隔、软间隔、核函数对比 特性 硬间隔SVM 软间隔SVM 核SVM 适用场景 数据严格线性可分 数据近似线性可分或含噪声 非线性可分数据 优化目标 $\\min \\frac12\\|\\mathbf{w}\\|^2$ $\\min \\frac12\\|\\mathbf{w}\\|^2 + C\\sum\\xi_i$ 在核空间中间接优化 对偶约束 $\\alpha_i \\ge 0$ $0 \\le \\alpha_i \\le C$ 同上，但内积换为核函数 支持向量 仅在间隔边界上 包括边界、内部、误分类点 核空间中的支持向量 关键参数 无 惩罚参数 $C$ 核参数（如 $\\gamma$）及 $C$ SVM通过最大化间隔获得良好的泛化能力，引入软间隔和核函数后能处理各种复杂数据，是机器学习中非常经典的算法。理解其背后的对偶推导和KKT条件，有助于深入掌握支持向量机的精髓。","categories":[{"name":"机器学习","slug":"ml","permalink":"https://www.hicode365.com/categories/ml/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"https://www.hicode365.com/tags/SVM/"},{"name":"支持向量积","slug":"支持向量积","permalink":"https://www.hicode365.com/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E7%A7%AF/"},{"name":"核函数","slug":"核函数","permalink":"https://www.hicode365.com/tags/%E6%A0%B8%E5%87%BD%E6%95%B0/"},{"name":"软间隔","slug":"软间隔","permalink":"https://www.hicode365.com/tags/%E8%BD%AF%E9%97%B4%E9%9A%94/"}]},{"title":"机器学习导论","slug":"机器学习/机器学习导论","date":"2026-02-22T01:35:12.017Z","updated":"2026-02-22T07:05:54.959Z","comments":true,"path":"cuidh5iGsdDS3Iq6RawHOWB5J.html","permalink":"https://www.hicode365.com/cuidh5iGsdDS3Iq6RawHOWB5J","excerpt":"","text":"引言在传统的编程范式中，我们输入数据和规则，通过计算机得到答案。而机器学习（Machine Learning）则不同，我们向计算机输入数据和答案（即最终的结果），让计算机自己去发现其中的学习模型、发现规律，从而构建能够进行预测的数学模型。简而言之，机器学习是一门研究如何让计算机从数据中自动获取规律，并利用这些规律来预测未知、辅助决策的科学。 机器学习是一门交叉学科，根据不同的视角可以被划分为多种范式，如： 从统计推断的哲学思想划分，机器学习模型可以划分为：频率学派与贝叶斯学派。前者认为模型的参数是固定但未知的“常数”，学习的过程就是通过优化方法寻找这个最优值；而后者则认为参数本身服从某种分布，学习的过程是在观察数据后，对先验信念进行更新。 若从建模的最终目标划分，算法模型又可分为判别式模型与生成式模型。判别式模型关注的是寻找不同类别之间的决策边界，直接学习如何划分数据；而生成式模型则试图理解数据本身的产生过程，通过学习联合分布来间接进行预测。 按统计推断划分频率学派核心思想频率派（Frequentist）将概率解释为事件在长期重复试验中发生的频率。在这一框架下，模型参数 $ \\theta $ 被视为一个确定的未知常量，虽然我们不知道它的具体值，但它本身并不具有随机性。我们只能通过观测数据来估计这个常量的值。 最大似然估计给定观测数据集 $ X $，所有样本的联合概率为： $$ p(X|\\theta) = \\prod_{i=1}^N p(x_i|\\theta) $$ 这个联合概率被称为似然函数（Likelihood Function），它衡量了在特定参数 $ \\theta $ 下观测到当前数据的可能性。频率派的估计方法就是寻找能使似然函数最大化的参数值，即最大似然估计（Maximum Likelihood Estimation, MLE）： $$ \\theta_{MLE} = \\arg\\max_{\\theta} p(X|\\theta) = \\arg\\max_{\\theta} \\sum_{i=1}^N \\log p(x_i|\\theta) $$ 由于对数函数是单调递增的，且能将连乘转化为连加便于计算，实际中通常使用对数似然。 频率派与机器学习频率派的思想深刻影响了统计机器学习的发展。许多经典算法都可以从MLE的角度理解： 线性回归：假设误差服从高斯分布时，MLE等价于最小二乘法 逻辑回归：直接对条件概率 $ p(y|x) $ 进行MLE估计 支持向量机：可以看作是在特定损失函数下的频率派方法 频率派方法的优势在于其理论简洁性、计算效率高，且在大样本条件下具有良好的渐近性质。然而，它也存在明显的局限性：无法融入先验知识，在小样本情况下容易过拟合，且只能给出点估计而非分布估计。 频率学派的参数估计 方法 全称 核心思想 适用场景 MLE 最大似然估计 $\\hat{\\theta}_{MLE} = \\arg\\max P(D \\mid \\theta)$ 寻找使观测数据出现概率最大的参数 最通用。大样本下具有渐近正态性、一致性。对模型假设敏感。 MM 矩估计法 令样本矩等于总体矩：$E[X^k] = \\frac{1}{n}\\sum X_i^k$ 简单快速，不需要知道分布的具体形式。但在小样本下往往不是有效估计量。 LSE 最小二乘估计 $\\min \\sum (y_i - \\hat{y}_i)^2$ 最小化残差平方和 线性回归。不需要概率分布假设（若误差服从正态分布，则等价于 MLE）。 GMM 广义矩估计 利用过定矩条件（矩条件多于参数），最小化二次型距离 经济计量学常用。解决了内生性问题，且不需要对误差分布做强假设。 M-估计 稳健估计 替换平方损失函数为更抗噪的 $\\rho$ 函数（如 Huber 损失） 数据有异常值（Outliers）时。比 MLE 更鲁棒。 EM 期望极大算法 交替进行 E-step（求期望）和 M-step（极大化似然） 含有隐变量（Latent variables）的概率模型，如 GMM 聚类、HMM。 贝叶斯学派核心思想贝叶斯派（Bayesian）将概率解释为对事件发生的不确定性的度量，这种不确定性可以是主观的信念。在这一框架下，参数 $ \\theta $ 被视为一个随机变量，服从某个先验分布 $ p(\\theta) $，这个先验分布反映了我们在看到数据之前对参数的认知。 贝叶斯定理与后验分布当我们观测到数据 $ X $ 后，根据贝叶斯定理，我们可以更新对参数的认知，得到后验分布（Posterior Distribution）： $$ p(\\theta|X) = \\frac{p(X|\\theta) \\cdot p(\\theta)}{p(X)} = \\frac{p(X|\\theta) \\cdot p(\\theta)}{\\int p(X|\\theta) \\cdot p(\\theta) \\, d\\theta} $$ 其中： $ p(X|\\theta) $ 是似然函数，与频率派中的定义相同 $ p(\\theta) $ 是先验分布，体现了我们对参数的主观先验知识 $ p(X) = \\int p(X|\\theta)p(\\theta)d\\theta $ 是边缘似然，也称为证据（Evidence），作为归一化常数确保后验概率之和为1 最大后验估计虽然贝叶斯派的完整结果是整个后验分布，但在实际应用中有时也需要一个点估计。这时可以采用最大后验估计（Maximum A Posteriori, MAP）： $$ \\theta_{MAP} = \\arg\\max_{\\theta} p(\\theta|X) = \\arg\\max_{\\theta} p(X|\\theta) \\cdot p(\\theta) \\tag{4} $$ MAP估计巧妙地结合了似然函数和先验信息。从优化的角度看，先验分布起到了正则化的作用：例如，高斯先验对应L2正则化，拉普拉斯先验对应L1正则化。 贝叶斯预测贝叶斯方法真正的优势在于其能够进行概率预测。当我们得到后验分布后，对于新样本 $ x_{new} $ 的预测分布可以通过对参数空间进行积分得到： $$ p(x_{new}|X) = \\int p(x_{new}|\\theta) \\cdot p(\\theta|X) \\, d\\theta \\tag{5} $$ 这个过程被称为贝叶斯模型平均（Bayesian Model Averaging）。与频率派只使用单一最优参数不同，贝叶斯方法考虑了所有可能的参数值，并依据后验概率进行加权平均，从而自然地体现了模型的不确定性。 贝叶斯派与机器学习贝叶斯派思想催生了**概率图模型（Probabilistic Graphical Models）**这一重要领域，包括： 朴素贝叶斯分类器：最简单的贝叶斯模型，假设特征条件独立 高斯过程：非参数贝叶斯方法，用于回归和分类 贝叶斯神经网络：为神经网络权重赋予先验分布 贝叶斯方法的优势在于：能够自然地融合先验知识、防止过拟合、提供不确定性估计、适用于小样本学习。但同时也面临挑战：后验分布的计算通常涉及高维积分，难以解析求解，需要借助近似方法如马尔可夫链蒙特卡洛（MCMC）、变分推断等。 贝叶斯学派的参数估计 方法 核心思想 输出形式 特点 MAP 最大后验估计 $\\arg\\max P(D \\mid \\theta) P(\\theta)$ 点估计 (单个数值) 相当于 MLE + 先验。在线性回归中增加高斯先验即等价于 L2 正则化 (Ridge)。 贝叶斯推断 计算完整的后验分布 $P(\\theta \\mid D) = \\frac{P(D \\mid \\theta) P(\\theta)}{\\int P(D \\mid \\theta) P(\\theta) d\\theta}$ 后验分布 提供不确定性度量（均值、方差）。当先验是共轭先验时有解析解。 MCMC 马尔可夫链蒙特卡洛采样 (如 Gibbs 采样) 采样样本点集 通过随机采样模拟复杂的后验分布。万能但慢，适用于高维复杂模型。 变分推断 (VI) 将推断问题转化为优化问题 寻找一个简单分布 $q(\\theta)$ 最小化 $KL(q \\| p$ 近似解析分布 牺牲了一定精度换取速度。是大规模深度贝叶斯学习（如 VAE）的核心。 频率派与贝叶斯派对比尽管存在哲学分歧，两大学派在方法论上有着深刻的联系： MLE与MAP的关系：当先验分布为均匀分布时，MAP估计退化为MLE估计 正则化视角：MLE + 正则化项 等价于 MAP（特定先验） 大样本一致性：当样本量趋于无穷时，后验分布会集中在MLE估计附近，贝叶斯估计渐近等价于频率派估计 现代机器学习中，两大学派的界限正变得越来越模糊。研究者们开始汲取两者的优势： 贝叶斯深度学习：将贝叶斯思想引入深度神经网络，解决过拟合和不确定性估计问题 集成学习：结合多个模型的思想与贝叶斯模型平均有异曲同工之妙 概率编程：提供灵活的框架来表达和求解概率模型 变分自编码器（VAE）：巧妙结合了深度学习与变分推断 应用场景选择在实际问题中，选择哪种方法取决于具体需求： 数据量巨大：频率派方法通常计算更高效 小样本或零样本学习：贝叶斯方法可以利用先验知识 需要不确定性估计：贝叶斯方法自然提供 模型解释性要求高：简单频率派模型更易解释 计算资源有限：频率派方法通常更友好 按建模目标划分判别式 直接对后验概率 P(y | x) 进行建模。 关注点：直接学习决策边界，即不同类别之间的界限。 例子：逻辑回归、神经网络、支持向量机。 比喻：学会直接区分狗和猫的图片（只看区别）。 生成式 间接地对似然 (Likelihood) P(x | y) 和先验 (Prior) P(y) 进行建模。 关注点：为每个类别单独建模其特征的分布。“生成式”是因为一旦学到了 P(x | y)，就可以为任何类别 y 生成新的样本 x。 步骤： 为每个类别 y 假设一个特征分布模型（例如高斯分布）。 从训练数据中估计每个类别分布的参数（如均值、方差）。 利用贝叶斯定理，将学到的 P(x | y) 和 P(y) 转换为最终用于分类的 P(y | x)。 比喻：分别学习“狗看起来是什么样”和“猫看起来是什么样”的完整模型，然后对于一个新动物，看它更符合哪个模型。 判别式与生成式的模型对比判别模型 和 生成模型 的根本区别在于它们解决问题的思路和关注点不同。 判别模型 致力于 “找到区别”。 思路：直接学习不同类别数据之间的决策边界，而不关心单个类别本身的具体样貌。 目标：回答“它更像是猫还是狗？” 好比：一个裁判，他不需要会画画，只需要掌握一个关键标准（比如身长）来快速区分两者。 生成模型 致力于 “理解本质”。 思路：分别学习每一类数据（如猫、狗）的整体特征和内部结构，为每个类别建立一个完整的“概念模型”。 目标：回答“猫&#x2F;狗长什么样子？” 好比：一个艺术家，他需要透彻地了解猫和狗的骨骼、肌肉、毛发等所有细节，才能把它们画出来。 特征 判别模型（Discriminative） 生成模型（Generative） 核心思想 学习类别之间的边界，找到“差异”。 学习数据本身的分布，理解每一类的“本质”。 解决的问题 这是X还是Y 什么是X 学习内容 条件概率p(y|x) （x是数据特征，y是数据标签） 联合概率p(x,y)、分布p(x)（无监督的情况） 能力 主要用于分类和回归，无法生成新数据。 既可以进行分类，也可以生成新的数据（如画一只猫）。 类比 只学会一个投机取巧的判别技巧（如比身长）。 学完后对猫狗有直观认知，能画出它们。 常见算法 逻辑回归、支持向量机、决策树、CRF、神经网络 朴素贝叶斯、高斯混合、隐马尔可夫模型、VAF、生成对抗网络 判别模型因为只专注于区分，因此不具备生成能力；而生成模型因为学会了数据的“本质”，所以具备生成新数据的能力。 ChatGPT、Midjourney这类能创作内容（生成文本、图像）的模型，其核心都是生成模型。而许多用于图像分类、垃圾邮件过滤等任务的模型，则更多是判别模型。 生成与判别式模型的选择 当数据量较少或对数据分布有较强先验知识时，生成学习方法可能更有效。 适用于数据生成、缺失值处理； 在小样本下可能比判别模型（如逻辑回归）更鲁棒。 当数据量充足且计算资源有限时，判别学习方法可能更合适。 按学习目标划分回归（Regression） 回归旨在建立输入特征与连续型输出变量之间的映射关系，回归任务的目标是预测一个连续的数值输出 $y \\in \\mathbb{R}$， 核心思想：找到一条曲线&#x2F;超平面，使预测值 $\\hat{y}$ 尽可能接近真实值 $y$，最小化预测误差: $ \\min_\\theta \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 $ 应用场景：预测房价、售额预测、股票价格等。 常见的回归算法: 线性回归：最简单的回归模型，假设输入与输出呈线性关系 岭回归&#x2F;Lasso：在线性回归基础上加入L2&#x2F;L1正则化，防止过拟合 决策树回归：通过树结构分段拟合数据 随机森林回归：集成多棵决策树，降低方差 梯度提升回归（GBDT、XGBoost）：通过迭代优化残差 支持向量回归（SVR）：利用核方法处理非线性关系 神经网络：能够拟合任意复杂的非线性函数 分类（Classification） 目标是预测离散的类别标签（Y）。 例如：判断邮件是否为垃圾邮件（类别：垃圾邮件&#x2F;非垃圾邮件）、识别图像中的物体类别（类别：猫&#x2F;狗&#x2F;车等）。 预测离散类别标签。根据类别数量可分为： 二分类： 只有两个类别，例如垃圾邮件分类。 多分类：包含多个类别，例如手写数字识别（0-9）。 多标签分类：一个样本可能属于多个类别 目标给定输入特征向量 $\\mathbf{x} \\in \\mathbb{R}^d$，分类问题的目标是预测其所属的离散类别标签 $y \\in \\{1, 2, ..., K\\}$。 从概率视角看，我们需要计算后验概率（posterior probability）：$$ P(y = k \\mid \\mathbf{x}), \\quad k = 1,2,...,K $$ 所有分类模型最终都采用最大后验决策规则（MAP decision rule）进行预测：$$ \\hat{y} = \\arg\\max_{k} P(y = k \\mid \\mathbf{x}) $$即选择使后验概率最大的类别作为预测结果。 建模判别式学习建模（Discriminative） 直接建模 $P(y|\\mathbf{x})$ 关注决策边界，不关心数据本身的分布 生成式学习建模（Generative） 间接建模：先学习联合分布 $ P(\\mathbf{x}, y) = P(y)P(\\mathbf{x}|y) $ 通过贝叶斯定理计算后验：$ P(y|\\mathbf{x}) = \\frac{P(y)P(\\mathbf{x}|y)}{\\sum_{k=1}^K P(y=k)P(\\mathbf{x}|y=k)} $ 参数估计无论采用判别式还是生成式方法，模型都包含需要从数据中学习的参数 $\\theta$。参数估计主要有两种方法： 频率派的最大似然估计（MLE）​ 将参数视为确定的未知常量，寻找使训练数据似然函数最大化的参数值：$$ \\theta_{MLE} = \\arg\\max_\\theta \\prod_{i=1}^N P(y_i|\\mathbf{x}_i, \\theta) $$​ 等价于最小化经验风险。 贝叶斯派的最大后验估计（MAP）​ 将参数视为随机变量，在MLE的基础上引入了参数的先验分布$p(\\theta)$，通过贝叶斯定理求使后验概率最大的参数值：$$ \\theta_{MAP} = \\arg\\max_\\theta \\prod_{i=1}^N P(y_i|\\mathbf{x}_i, \\theta) \\cdot p(\\theta) $$ 正则化效应：先验分布 $p(\\theta)$ 等价于对参数施加约束，防止过拟合 高斯先验 $p(\\theta) \\sim \\mathcal{N}(0, \\sigma^2)$ ↔ L2 正则化 拉普拉斯先验 $p(\\theta) \\sim \\text{Laplace}(0, b)$ ↔ L1 正则化 在数据稀缺时，先验知识可提供更稳健的估计 当先验为均匀分布时，MAP 退化为 MLE 完整流程 训练阶段：使用MLE或MAP估计模型参数 $\\hat{\\theta}$ 预测阶段：对于新样本 $\\mathbf{x}_{new}$，计算后验概率：$P(y=k|\\mathbf{x}_{new}, \\hat{\\theta})$ 决策阶段：应用最大后验决策规则：$\\hat{y} = \\arg\\max_k P(y=k|\\mathbf{x}_{new}, \\hat{\\theta})$ 聚类（Clustering）聚类是一种无监督学习任务，目标是将未标注的样本划分为若干个簇（cluster），使得同一簇内的样本尽可能相似，不同簇间的样本尽可能不同。 给定数据集 $\\{\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_N\\}$，聚类算法旨在找到一种划分 $C = \\{C_1, C_2, ..., C_K\\}$，使得： 簇内相似度最大化：$\\max \\sum_{k=1}^K \\sum_{\\mathbf{x}_i \\in C_k} \\text{sim}(\\mathbf{x}_i, \\mu_k)$ 簇间相似度最小化：$\\min \\sum_{k \\lt j} \\text{sim}(\\mu_k, \\mu_j)$，其中 $\\mu_k$ 是第 $k$ 个簇的中心（或代表点），$\\text{sim}(\\cdot)$ 是相似度度量。 降维（Dimensionality Reduction）降维是将高维数据映射到低维空间的过程，同时尽可能保留原始数据的重要结构信息。它是无监督学习的重要分支，也是数据预处理的关键步骤。 给定高维数据 $\\{\\mathbf{x}_i \\in \\mathbb{R}^D\\}_{i=1}^N$，降维算法寻找映射 $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}^d$，其中 $d \\ll D$，使得：$$\\mathbf{z}_i = f(\\mathbf{x}_i) \\in \\mathbb{R}^d$$且 $\\{\\mathbf{z}_i\\}$ 尽可能保留 $\\{\\mathbf{x}_i\\}$ 的重要结构信息。 需要降维的场景: 维度灾难（Curse of Dimensionality）：随着维度增加，数据变得稀疏，距离度量失效，模型复杂度指数级增长 可视化需求：人类只能理解2D或3D空间，降维使高维数据可视化成为可能 计算效率：减少特征数量，降低模型训练和预测的时间复杂度 去噪：丢弃不重要的维度，保留主要信号 特征提取：从原始特征中构造更有代表性的新特征 排序（Ranking）排序任务的目标是学习一个排序函数，能够根据输入特征对项目集合进行排序，使得相关性高的项目排在前面。排序问题是信息检索和推荐系统的核心。 排序的应用场景: 搜索引擎：根据用户查询对网页进行排序 推荐系统：为用户推荐最感兴趣的商品、视频、音乐 问答系统：对候选答案按相关性排序 广告点击率预测：排序广告以提高点击率 社交媒体：排序信息流中的帖子 结语机器学习的发展，本质上是在表示、评估与优化这三个核心要素上的持续演进：选择何种方式表示数据与模型，如何定义优劣，以及通过何种策略寻找最优解。 深度学习的崛起摒弃了人工特征工程，通过分层抽象自动从原始数据中提取层次化特征，使得模型能够感知高维空间中的复杂结构。然而，强大的表示能力也伴随着挑战——海量参数需要海量数据，黑箱特性损害了可解释性。 强化学习的复兴则拓展了学习的范式。它不再满足于拟合静态数据，而是在与环境的交互中通过试错学习最优策略，将机器学习从模式识别推向序贯决策。这种范式更接近生物学习的本质，在机器人、博弈、控制等领域展现出独特价值。 然而，技术的飞速发展机器学习仍面临多方面的挑战： 数据效率：人类可以从少量样本中学习，而当前模型仍然依赖海量数据。如何让机器学习像人类一样“举一反三”，仍是未解难题。 鲁棒性与泛化：分布外泛化、对抗样本、虚假相关——模型在实验室环境外的表现往往不尽如人意。 可解释性与可信赖：随着模型进入医疗、金融、司法等高风险领域，黑箱决策的风险日益凸显。 价值对齐：当模型越来越强大，如何确保其目标与人类价值观一致，成为关乎未来的重要课题。 计算效率与可持续性：大模型的训练消耗惊人，如何在性能与能耗之间取得平衡，既是技术问题，也是环境问题。 常见模型划分一览表 算法模型 频率派 贝叶斯派 判别式 生成式 参数估计&#x2F;核心准则 线性回归 是 是 (贝叶斯线性回归) 是 否 最小二乘 (OLS) &#x2F; MLE 逻辑回归 是 是 (拉普拉斯近似等) 是 否 极大似然估计 (MLE) 决策树&#x2F;随机森林&#x2F;提升树 是 否 是 否 信息增益 &#x2F; 基尼系数 &#x2F; MSE SVM 是 否 是 否 合页损失 + 正则化 (对偶优化) PCA (主成分分析) 是 是 (Probabilistic PCA) 不适用 是 (概率视角下) 方差最大化 &#x2F; 投影误差最小化 LDA (线性判别分析) 是 否 否 是 Fisher 准则 &#x2F; 类内类间散度 高斯判别分析 (GDA) 是 是 否 是 联合概率 $P(x,y)$ 建模 高斯过程 (GP) 否 是 是 否 核函数 + 边际似然最大化 高斯混合模型 (GMM) 是 是 (贝叶斯 GMM) 否 是 EM 算法 朴素贝叶斯 是 是 否 是 MAP (带平滑) &#x2F; MLE 贝叶斯网络 否 是 否 是 结构学习 + 条件概率表 隐马尔可夫 (HMM) 是 是 (变分推断) 否 是 EM 算法 条件随机场 (CRF) 是 否 是 否 极大似然 (梯度上升) 感知机 是 否 是 否 随机梯度下降 (SGD) 深度神经网络 (DNN) 是 是 (BNN) 是 视模型而定 反向传播 (BP) + 优化器","categories":[{"name":"机器学习","slug":"ml","permalink":"https://www.hicode365.com/categories/ml/"}],"tags":[{"name":"频率学派","slug":"频率学派","permalink":"https://www.hicode365.com/tags/%E9%A2%91%E7%8E%87%E5%AD%A6%E6%B4%BE/"},{"name":"贝叶斯学派","slug":"贝叶斯学派","permalink":"https://www.hicode365.com/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E6%B4%BE/"},{"name":"判别式模型","slug":"判别式模型","permalink":"https://www.hicode365.com/tags/%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B/"},{"name":"生成式模型","slug":"生成式模型","permalink":"https://www.hicode365.com/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B/"},{"name":"回归","slug":"回归","permalink":"https://www.hicode365.com/tags/%E5%9B%9E%E5%BD%92/"},{"name":"分类","slug":"分类","permalink":"https://www.hicode365.com/tags/%E5%88%86%E7%B1%BB/"}]},{"title":"python中的数据类型","slug":"工程与实战/Python中的数据类型","date":"2023-08-15T02:10:06.000Z","updated":"2026-01-01T15:46:05.612Z","comments":true,"path":"cuidDZEK6Bgd97PUqFs0w-F_G.html","permalink":"https://www.hicode365.com/cuidDZEK6Bgd97PUqFs0w-F_G","excerpt":"","text":"Python中的数据类型可变数据类型对变量的值进行修改时，变量对应的内存地址不变，对应的值发生了改变，这种数据类型就称为可变数据类型。 不可变数据类型对变量的进行修改时，变量对应的内存地址发生了改变(变量指向了新的内存)，从而修改了变量的值，而变量对应的原内存的值并没有被改变，这种数据类型就称为可变数据类型。 也就是：不可变数据类型更改后地址发生改变，可变数据类型更改地址不发生改变 常用数据类型 数据类型 是否是可变数据类型 是否有序 None (空) 不可变 - int (整数) 不可变 - float (浮点) 不可变 - bool (布尔) 不可变 - str (字符串) 不可变 - tuple (元组) 不可变 序列类型，有序 list (列表) 可变 序列类型，有序 set (集合) 可变 序列类型，无序，不可重复 dict (字典) 可变 映射类型，v3.6及以后无有序, 前面版本无序 扩展 数据类型 是否是可变数据类型 是否有序 说明 bytes 不可变 - 定义字节：b’hello’,bytes(5) bytearray 可变 - 定义字节数组：bytearray(b’hello’), bytearray(10) complex (复数) 不可变 - 由一个实数和一个虚数组合构成，如：4+3j frozenset (冻结的set) 不可变 无序 冻结的set初始化后不能再添加或删除元素 array (数组) 可变 有序 数组中的元素必须是同一类型 OrderedDict 可变 有序 key有序，setdefault取值key不存在也不报错 defaultdict 可变 有序 取值时Key不存在也不会抛出KeyError异常 deque 可变 有序 高效插入和删除的双向队列列表 常见数据类型的操作和转换list列表[ ]list是**&#x3D;&#x3D;可变&#x3D;&#x3D;、&#x3D;&#x3D;可重复&#x3D;&#x3D;的&#x3D;&#x3D;有序&#x3D;&#x3D;**列表，里面的元素的数据类型也可以不同(也可以是另一个list)。list可根据索引号取其中的数据。 list的生成12345678list1 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]print(&quot;list1: &quot;, list(list1)) # 输出： list1: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]list2 = list(range(0, 10))print(&quot;list2: &quot;, list(list2))list3 = [i*i for i in range(10) if i % 2 == 0]print(&quot;list3: &quot;, list(list3))list4 = (str(i) + j for i in range(0, 10, 2) for j in &quot;xyz&quot;)print(&quot;list4: &quot;, list(list4)) list元素反转、排序和次数统计12345678910list1 = [0, 1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9]list1.reverse() # 元素顺序反转print(&quot;list reverse: &quot;, list1)list1.sort(reverse=False) # 排序print(&quot;list sort: &quot;, list1)list1 = sorted(list1, reverse=True)print(&quot;list sort: &quot;, list1)times = list1.count(5) # 查看list中的元素出现的次数print(&quot;times: &quot;, times) list元素的添加、删除和取值12345678910111213141516list1.append(10)print(&quot;append value: &quot;, list1) # 添加元素list1.insert(1, 10) # 在指定位置添加元素print(&quot;insert value: &quot;, list1)list1.remove(10) # 删除指定value元素(第一个匹配的元素)print(&quot;remove value: &quot;, list1)value = list1.pop(12) # 删除指定index的元素并返回删除的值list1.pop() # 不指定index时默认删除最后一个元素list1.pop(-2) # 删除倒数第二个元素print(&quot;remove index: &quot;, list1)index_value = list1.index(3) # 查找第一个value为100的index值，如果不存在报TypeError异常print(&quot;index_value: &quot;, index_value)print(list1)index_value = list1.index(5, 7, 9) # 指定范围，从第7(包括)个到第9(不包括)个元素之间查找value为5的indexprint(&quot;index_value: &quot;, index_value) list添加多个元素、list的合并12345list2 = [100, 101, 102]# list1 = list1 + list2list1.extend(list2)print(list1)print(&quot;*&quot; * 10) list的遍历1234567891011for value in list1: print(&quot;value: %i&quot; % value)print(&quot;*&quot; * 50)for index in range(len(list1)): print(&quot;index: %i, value: %i&quot; % (index, list1[index]))print(&quot;*&quot; * 50)for index, value in enumerate(list1): print(&quot;index: %i, value: %i&quot; % (index, value))print(&quot;*&quot; * 50)for index, value in enumerate(list1, 100): # index从100开始 print(&quot;index: %i, value: %i&quot; % (index, value)) list中使用切片(slice)取值12345678910111213elements = list1[0:3] # 取第0到3条元素(包括头不包括尾)# elements = list1[:3]print(&quot;slice elements: &quot;, elements)elements = list1[1:] # 取第1到最后一个元素(包括头也包括尾)print(&quot;slice elements: &quot;, elements)elements = list1[-2] # 取倒数第二条print(&quot;slice elements: &quot;, elements)elements = list1[4:-2] # 取第四条到倒数第二条(包括头不包括尾)print(&quot;slice elements: &quot;, elements)elements = list1[0:6:2] # 取第0条到第6条中每2个取一个print(&quot;slice elements: &quot;, elements)elements = list1[:] # 取所有元素print(&quot;slice elements: &quot;, elements) &#x3D;&#x3D;列表、元组和字符串&#x3D;&#x3D;都可以使用切片进行操作 list的深copy和浅copy12345678910111213141516# 浅拷贝只拷贝了引用，没有拷贝内容list2 = list1list2[1] = 1000print(&quot;list1: &quot;, list1)print(&quot;list2: &quot;, list1)print(id(list1), id(list2))# 深拷贝是对于一个对象所有层次的拷贝(递归拷贝)list3 = list1.copy()# import copy# list3 = copy.copy(list1)# list3 = copy.deepcopy(list1)list1[1] = 1print(&quot;list1: &quot;, list1)print(&quot;list3: &quot;, list3)print(id(list1), id(list3)) set集合{ }set是**&#x3D;&#x3D;可变&#x3D;&#x3D;**、&#x3D;&#x3D;不可重复&#x3D;&#x3D;的&#x3D;&#x3D;无序&#x3D;&#x3D;列表。 &#x3D;&#x3D;set中不可以放入可变对象&#x3D;&#x3D;，因为无法判断两个可变对象是否相等而去重。 set的定义123456789101112131415161718192021222324set0 = &#123;0, 0, 1, 2, 3, 4, 5, 4, 5, 6&#125; # 直接定义set集合print(&quot;set0: &quot;, set0) # 输出 set0: &#123;0,1, 2, 3, 4, 5, 6&#125;set1 = set([0, 0, 1, 2, 3, 4, 5, 4, 5, 6]) # 通过list定义setprint(&quot;set1: &quot;, set1)set2 = set((0, 0, 1, 2, 3, 4, 5, 4, 5, 6)) # 通过tuple定义setprint(&quot;set2: &quot;, set2)set3 = set(&#123;&quot;x&quot;: 2, 10: &quot;b&quot;&#125;) # 通过dict定义setprint(&quot;set3: &quot;, set3) # 输出 set3: &#123;10, &#x27;x&#x27;&#125;my_list = [0, 0, 1, 2, 3, 4, 5, 4, 5]set4 = set(my_list) # set中不可以放入可变对象,然而为何放入list却不报错?print(&quot;set4: &quot;, set4) # 输出 set4: &#123;0, 1, 2, 3, 4, 5, 6&#125;# 由下面操作可以得出结论,set是先把list做遍历得到不可变的int对象类型后再放入set中my_list[0] = 10print(&quot;set4 with my_list changed: &quot;, set4) # 输出 &#123;0, 1, 2, 3, 4, 5, 6&#125;my_list.append([10, 20])print(&quot;my_list: &quot;, my_list)set5 = set(my_list) # 在list再放入list,此时将报错print(&quot;set5: &quot;, set5) set元素的添加、删除和取值12345678910111213141516set0 = &#123;0, 0, 1, 2, 3, 4, 5, 4, 5, 6&#125;print(&quot;set0: &quot;, set0)set0.add(&quot;cn&quot;) # 添加单个元素print(&quot;set0: &quot;, set0)set0.update([10, 20, 30]) # 添加多个元素print(&quot;set0: &quot;, set0)set0.add((&quot;com&quot;, &quot;cn&quot;)) # 添加元组(元组是不可变数据类型)print(&quot;set0: &quot;, set0)# set0.add([10, 20]) # 添加list报错,不能添加可变的数据类型(不能添加,但可以使用list创建set)# set0.add(&#123;10, 20&#125;) # 添加set报错,(可是使用不可变的frozenset添加:set0.add(frozenset(&#123;10, 20&#125;)))# set0.add(&#123;&quot;x&quot;: 2, 10: &quot;b&quot;&#125;) # 添加dict报错,不能添加可变的数据类型(不能添加,但可以使用dict创建set)set0.remove(&quot;cn&quot;) # 根据值删除元素(set不能根据索引删除)print(&quot;set0: &quot;, set0) set取并集和交集12345set1 = &#123;&quot;a&quot;, &quot;b&quot;, 4, 6, 100&#125;my_set = set0 | set1 # 取并集print(my_set)my_set = set0 &amp; set1 # 取交集print(my_set) set遍历注：set的遍历同list dict字典{ }dict是**&#x3D;&#x3D;无序&#x3D;&#x3D;，key&#x3D;&#x3D;不可重复&#x3D;&#x3D;、&#x3D;&#x3D;不可变&#x3D;&#x3D;**内容以key-value键值对形式存在的映射 dict中的key只能是不可变对象且唯一, 一个key对应一个value，多次对一个key设置value，后面的值会把前面的冲掉。 dict一般用在需要高速查找的很多地方。dict的key必须是不可变对象，这是因为dict根据key来计算value的存储位置，如果每次计算相同的key得出的结果不同，那dict内部就完全混乱了。这种通过key计算位置的算法称为哈希算法（Hash）。要保证hash的正确性，作为key的对象就不能变。在Python中，字符串、整数等都是不可变的，因此，可以放心地作为key。而list、set是可变的，所以就不能作为key。 dict的创建和增删改查12345678910111213141516dict1 = &#123;&quot;addr&quot;: &quot;北京&quot;, &quot;age&quot;: 18, &quot;gender&quot;: &quot;女&quot;&#125;dict1[&quot;height&quot;] = 1.77 # 添加元素dict1.pop(&quot;age&quot;) # 删除元素输出item_del = dict1.popitem() # 产出dict中的最后一个item并返回print(&quot;item_del: &quot;, item_del)dict1[&quot;addr&quot;] = &quot;深圳&quot; # 修改元素print(&quot;dict1: &quot;, dict1)keys = dict1.keys() # 获取dict的所有keyprint(&quot;keys: &quot;, keys) # dict_keys([&#x27;add&#x27;, &#x27;height&#x27;])addr = dict1.get(&quot;addr&quot;) # 根据key获取value,若key不存在报异常(defaultdict字典不报异常)print(&quot;addr: &quot;, addr)addr = dict1.setdefault(&quot;addr&quot;) # 根据key获取value,若key不存返回None,也可设置默认返回值print(&quot;addr: &quot;, addr)name = dict1.get(&quot;name&quot;, &quot;unknow&quot;) # 根据key获取value,若key不存返回默认值&#x27;unknow&#x27;print(&quot;name: &quot;, name) dict的遍历12345678910# dict的遍历for key in dict1: print(&quot;key: %s, value: %s&quot; % (key, dict1[key]))print(&quot;*&quot; * 50)for value in dict1.values(): print(&quot;value: &quot;, value)print(&quot;*&quot; * 50)for key, value in dict1.items(): print(&quot;key: %s, value: %s&quot; % (key, value)) dict的合并123dict2 = &#123;&quot;mobel&quot;: 15888888888, &quot;postal_code&quot;: 10000&#125; # 合并两个dictdict1.update(dict2)print(&quot;dict1: &quot;, dict1) dict和list的异同 list查找和插入的时间随着元素的增加而增加；占用空间小，浪费内存很少 dict查找和插入的速度极快，不会随着key的增加而变慢；需要占用大量的内存，内存浪费多。所以，dict是用空间来换取时间的一种方法。 dict的排序1234567891011121314151617181920212223# dict排序dict3 = &#123;&#x27;sh&#x27;: 3, &#x27;hz&#x27;: 2, &#x27;tj&#x27;: 1, &#x27;bj&#x27;: 5, &#x27;gz&#x27;: 2, &#x27;sz&#x27;: 4, &#x27;wh&#x27;: 1&#125;# 默认排序，并仅返回keykey_rank1 = sorted(dict3.keys(), reverse=False)print(&quot;key_rank1: &quot;, key_rank1)# 默认排序(以key来排序)，并返回key和valuedict_key_rank1 = sorted(dict3.items(), reverse=False)print(&quot;dict_key_rank1: &quot;, dict(dict_key_rank1))# 以key排序dict_key_rank2 = sorted(dict3.items(), key=lambda item: item[0], reverse=False)print(&quot;dict_key_rank2: &quot;, dict(dict_key_rank2))# 以value排序dict_value_rank1 = sorted(dict3.items(), key=lambda item: item[1], reverse=False)print(&quot;dict_value_rank1: &quot;, dict(dict_value_rank1))# 以value排序dict4 = &#123;&#x27;上海&#x27;: 3, &#x27;杭州&#x27;: 2, &#x27;天津&#x27;: 1, &#x27;北京&#x27;: 5, &#x27;广州&#x27;: 2, &#x27;深圳&#x27;: 4, &#x27;武汉&#x27;: 1&#125;dict_value_rank2 = sorted(dict4.items(), key=lambda item: item[1], reverse=True)print(&quot;dict_value_rank2: &quot;, dict(dict_value_rank2)) tuple元组( )tuple是**&#x3D;&#x3D;不可变&#x3D;&#x3D;、&#x3D;&#x3D;有序&#x3D;&#x3D;**的列表，所以一般在定义tuple时就进行初始化赋值。 注意： 在定义只有一个元素的tuple时其元素后面要加逗号 123tuple0 = () # 创建空元祖tuple0 = (1) # 不是tuple，会当成括号处理tuple0 = (1,) # 正确的tuple tuple虽然不可变但tuple中的元素对象却是可变的 12345my_list = [&quot;x&quot;, &quot;y&quot;]tuple1 = (&#x27;a&#x27;, &#x27;b&#x27;, my_list) # tuple包含list,list变化时,tuple1也就跟着变化print(&quot;tuple1: &quot;, tuple1) # tuple1: (&#x27;a&#x27;, &#x27;b&#x27;, [&#x27;x&#x27;, &#x27;y&#x27;])my_list.append(&quot;z&quot;)print(&quot;tuple1 with my_list changed: &quot;, tuple1) # tuple1变为(&#x27;a&#x27;, &#x27;b&#x27;, [&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;]) tuple的创建123tuple2 = (1, &quot;good&quot;, 2, 3, &quot;good&quot;, True) # 创建元组,里面的元素类型可以不同tuple3 = (&quot;a&quot;, &quot;b&quot;, *tuple2, 4, 5) # 元组引用另一个数组中的所有元素print(&quot;tuple3: &quot;, tuple3) tuple中元素的增删改查1234567891011element = tuple2[4] # 根据索引获取元组中的元素element = tuple2[-2] # 使用索引获取元组中的元素index = tuple2.index(&quot;good&quot;) # 获取第一个匹配给定值的index值del tuple2 # 删除元组# tuple2[4] = &quot;well&quot; # 修改元组的元素,报错tuple4 = (&quot;a&quot;, &quot;b&quot;, 4, 5, [6, 7, 8])print(&quot;tuple4: &quot;, tuple4)# tuple4[-1] = [10, 20, 30] #报错tuple4[-1][0] = 100 # 可以通过修改元组中的list,从而改变元组print(&quot;tuple4: &quot;, tuple4) # tuple4: (&#x27;a&#x27;, &#x27;b&#x27;, 4, 5, [100, 7, 8]) tuple的遍历注：tuple的遍历同list","categories":[{"name":"工程与实战","slug":"工程与实战","permalink":"https://www.hicode365.com/categories/%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"python","slug":"python","permalink":"https://www.hicode365.com/tags/python/"},{"name":"数据类型","slug":"数据类型","permalink":"https://www.hicode365.com/tags/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"}]},{"title":"docker常用命令","slug":"推荐系统/docker常用命令","date":"2022-11-27T14:28:39.000Z","updated":"2026-02-22T07:05:09.517Z","comments":true,"path":"1c41d072cac63725dbd48a1273b2fd97.html","permalink":"https://www.hicode365.com/1c41d072cac63725dbd48a1273b2fd97","excerpt":"","text":"docker常用命令12345678910111213141516docker pull tensorflow/serving #从仓库拉取镜像docker pull tensorflow/serving:latest-gpu #从仓库拉取GPU镜像docker pull tensorflow/serving:2.8.3-gpu #从仓库拉取GPU镜像docker run -it tensorflow/serving #进入到镜像中exit #退出镜像docker run tensorflow/serving #运行某个容器docker ps // 查看所有正在运行容器docker stop containerId // containerId 是容器的IDdocker ps -a // 查看所有容器docker ps -a -q // 查看所有容器IDdocker stop $(docker ps -a -q) // stop停止所有容器docker rm $(docker ps -a -q) // remove删除所有容器docker rm/rmi #删除容器/镜像docker cp local_files containerId:docker_files #本地文件复制到docker tf-serving部署123456789101112-p: 指定主机到docker容器的端口映射--mount: 表示要进行挂载,其中 type=bind: 是选择挂载模式， source: 要部署模型的存储路径，也就是挂载的源（必须是绝对路径）， target: 要挂载的目标位置，模型挂载到docker容器中的位置，也就是docker容器中的目录（放在集装箱的哪里）-t: 指定的是挂载到哪个容器-e: 环境变量 MODEL_NAME: 必须与target指定路径的最后一个文件夹名称相同--per_process_gpu_memory_fraction: 运行时所需的GPU显存资源最大比率的值设定-v: path1:path2 分别指模型在机器种储存的路径（必须是绝对路径），模型在容器中储存的路径（放在集装箱的哪里） 1234docker run -p 8500:8500 \\ --mount type=bind,source=/Users/coreyzhong/workspace/tensorflow/saved_model/,target=/models/test-model \\ -t tensorflow/serving:1.15.0 \\ -e MODEL_NAME=test-model --model_base_path=/models/test-model/ &amp; 12345model_path=&quot;/Users/havorld/jupyter/model_save/&quot;docker run -t --rm -p 8500:8500 -p 8501:8501 \\ -v &quot;$model_path/din:/models/tf_saved_models&quot; \\ -e MODEL_NAME=tf_saved_models \\ tensorflow/serving &amp; 1234567查看TensorFlow-Serving状态： curl http://localhost:8501/v1/models/$&#123;model_name&#125;查看TensorFlow-Serving模型信息： curl http://localhost:8501/v1/models/$&#123;model_name&#125;/metadata查看模型信息: saved_model_cli show --dir=&#x27;./$&#123;model_path&#125;/20220422104620&#x27; --all使用Http请求进行模型预测： curl -d &#x27;&#123;&quot;instances&quot;: [1,2,3,4,5]&#125;&#x27; -X POST http://localhost:8501/v1/models/$&#123;model_name&#125;:predict其中instances的value为模型输入Tensor的字符串形式，矩阵维度需要和Tensor对应。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586docker run -t --rm -p 8501:8501 \\ -v &quot;/home/Personas/havorld/tfserving/tf_saved_models:/models/tf_saved_models&quot; \\ -e MODEL_NAME=tf_saved_models \\ tensorflow/servingdocker run -t --rm -p 8501:8501 \\-v &quot;/Users/haopeng.meng/jupyter/tf_saved_models:/models/tf_saved_models&quot; \\-e MODEL_NAME=tf_saved_models \\tensorflow/servingcat /opt/logs/rec-feed-api/access.log | grep &quot;feed recommend-&gt; uid:55&quot; | grep &quot;id=34174686&quot;sudo docker run -t --rm -p 8501:8501 -p 8500:8500 \\ -v &quot;/home/meng.haopeng/tfserving/tf_saved_models:/models/tf_saved_models&quot; \\ -e MODEL_NAME=tf_saved_models \\ tensorflow/servingdocker run -p 8501:8501 -p 8500:8500 \\ --mount type=bind,source=/Users/haopeng.meng/jupyter/tf_saved_models,target=/models/tf_saved_models \\ -e MODEL_NAME=tf_saved_models \\ -t tensorflow/servingdocker run -p 8500:8500 \\ --mount type=bind,source=./intent/,target=/models/intent_score \\ -e MODEL_NAME=intent_score -t tensorflow/serving:1.10.0docker run -p 8501:8501 -p 8500:8500 --mount type=bind,source=/my/model/path/m,target=/models/m -e MODEL_NAME=m -t tensorflow/serving:2.1.0sudo docker run -t --rm -p 8501:8501 -p 8500:8500 \\ -v &quot;/home/meng.haopeng/tfserving/tf_saved_models:/models/tf_saved_models&quot; \\ -e MODEL_NAME=tf_saved_models \\ tensorflow/serving#docker run -t -p 443:8500 -p 8500:8501 -v &quot;/data/lsj/dmp/SavedModel/:/models/&quot; tensorflow/serving --model_config_file=/models/models.config --model_config_file_poll_wait_seconds=300# run use containerdocker run -t -p 8501:8500 --name=tf_serving_multi_version_01 -v &quot;/data/tf-model/models/:/models/&quot; tensorflow/serving --model_config_file=/models/models.config --model_config_file_poll_wait_seconds=300 --allow_version_labels_for_unavailable_models=true --enable_batching=true --batching_parameters_file=/models/batch.config# tf-serving部署docker run -t --rm -p 8501:8501 \\-v &quot;/Users/haopeng.meng/jupyter/tf_saved_models:/models/tf_saved_models&quot; \\-e MODEL_NAME=tf_saved_models \\tensorflow/servingdocker run -p 8501:8501 -p 8500:8500 \\ --mount type=bind,source=/Users/haopeng.meng/jupyter/tf_saved_models,target=/models/tf_saved_models \\ -e MODEL_NAME=tf_saved_models \\ -t tensorflow/serving sudo docker run -t --rm -p 8501:8501 -p 8500:8500 \\ -v &quot;/home/meng.haopeng/tfserving/tf_saved_models:/models/tf_saved_models&quot; \\ -e MODEL_NAME=tf_saved_models \\ tensorflow/servingsudo docker run --name feed -t --rm -p 8700:8500 -p 8701:8501 \\ -v &quot;/home/meng.haopeng/tfserving/tf_saved_models:/models/tf_saved_models&quot; \\ -e MODEL_NAME=tf_saved_models \\ tensorflow/servingdocker run --name feed -t --rm -p 8701:8501 -p 8700:8500 \\ --mount type=bind,source=/home/Personas/havorld/tfserving/model_save,target=/models/model_save \\ -e MODEL_NAME=model_save \\ -t tensorflow/serving:latest-gpu docker run --name feed -t --rm -p 8700:8500 -p 8701:8501 \\ --mount type=bind,source=/Users/haopeng.meng/Desktop/recommend/rec-alg-feed/model_save/din/serving/,target=/models/serving \\ -e MODEL_NAME=serving \\ -t tensorflow/serving docker run --name feed -t --rm -p 8700:8500 -p 8701:8501 \\ --mount type=bind,source=/Users/haopeng.meng/Desktop/recommend/serving/din/,target=/models/serving \\ -e MODEL_NAME=serving \\ -t tensorflow/serving","categories":[{"name":"docker","slug":"docker","permalink":"https://www.hicode365.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.hicode365.com/tags/docker/"},{"name":"tfserving","slug":"tfserving","permalink":"https://www.hicode365.com/tags/tfserving/"}]},{"title":"git常用命令","slug":"推荐系统/git常用命令","date":"2021-09-28T03:05:35.000Z","updated":"2023-08-15T02:10:06.000Z","comments":true,"path":"cuidrRz6-aJ020Ll6zs2_iuIT.html","permalink":"https://www.hicode365.com/cuidrRz6-aJ020Ll6zs2_iuIT","excerpt":"","text":"git常用命令123456789101112131415161718192021222324252627282930git clone https://github.com/username/project.github.io.git #拉取代码(master/main)git clone -b _dev https://github.com/username/project.github.io.git #拉取分支(非master或main分支)git checkout --track origin/_remote #获取指定的远程分支到本地git branch #查看本地分支git branch -a #查看远程分支git branch -vv#查看分支的绑定信息git branch _local #创建本地分支git branch -d _local #删除本地分支(当前分支不能停留在要删除的分支上)git checkout _local #切换到本地分支git checkout -b _local # 创建并切换到本地创建的分支git push --set-upstream origin _remote #创建远程分支git branch -r -d origin/_remote #删除远程分支(记得push一下 git push origin _remote)git checkout -b _local origin/_remote #创建本地分支绑定远程分支git status #查看修改过代码的类git diff #查看修改的代码git add src/main/java/com/so/alg/RecommendServiceImpl.java #添加修改的代码git commit -m &quot;recommend feed modified&quot; #给修改的代码添加注释git pull origin _remote #从远程更新变动的代码git push origin _local:_remote #提交git merge _remote #合并分支到master上(需要先切换到master分支上,在执行合并)git tag -a 2020071801 -m &#x27;v2.0部署&#x27; #添加taggit show 2020071801 #展示taggit push --tags #提交tag git配置SSH公钥和私钥1234567891011121314151617# 1.生成公钥和私钥(邮箱为github注册邮箱)ssh-keygen -t rsa -C &quot;xxxxxx@gmail.com&quot;# 2.设置公钥私钥key的保存位置(可以直接确认则保存在默认位置)Generating public/private rsa key pair.Enter file in which to save the key (/Users/username/.ssh/id_rsa):# 3.输入、重复输入密钥盐值Enter passphrase (empty for no passphrase):Enter same passphrase again:# 4.复制打印的公钥内容，并在github-&gt;Settings-&gt;SSH and GPG keys-&gt;New SSH Key中设置(title随意起)cat .ssh\\id_rsa.pub# 5.查看密钥是否配置成功(会提示输入盐值)ssh -T git@github.comEnter passphrase for key &#x27;/Users/username/.ssh/id_rsa&#x27;:Hi hicode360! You&#x27;ve successfully authenticated, but GitHub does not provide shell access.# 6.配置全局git信息git config --global user.name &quot;username&quot; # github用户名git config --global user.email &quot;xxxxxx@gmail.com&quot; #github注册邮箱","categories":[{"name":"git","slug":"git","permalink":"https://www.hicode365.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://www.hicode365.com/tags/git/"},{"name":"命令","slug":"命令","permalink":"https://www.hicode365.com/tags/%E5%91%BD%E4%BB%A4/"}]}],"categories":[{"name":"机器学习","slug":"ml","permalink":"https://www.hicode365.com/categories/ml/"},{"name":"工程与实战","slug":"工程与实战","permalink":"https://www.hicode365.com/categories/%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9E%E6%88%98/"},{"name":"docker","slug":"docker","permalink":"https://www.hicode365.com/categories/docker/"},{"name":"git","slug":"git","permalink":"https://www.hicode365.com/categories/git/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"https://www.hicode365.com/tags/SVM/"},{"name":"支持向量积","slug":"支持向量积","permalink":"https://www.hicode365.com/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E7%A7%AF/"},{"name":"核函数","slug":"核函数","permalink":"https://www.hicode365.com/tags/%E6%A0%B8%E5%87%BD%E6%95%B0/"},{"name":"软间隔","slug":"软间隔","permalink":"https://www.hicode365.com/tags/%E8%BD%AF%E9%97%B4%E9%9A%94/"},{"name":"频率学派","slug":"频率学派","permalink":"https://www.hicode365.com/tags/%E9%A2%91%E7%8E%87%E5%AD%A6%E6%B4%BE/"},{"name":"贝叶斯学派","slug":"贝叶斯学派","permalink":"https://www.hicode365.com/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E6%B4%BE/"},{"name":"判别式模型","slug":"判别式模型","permalink":"https://www.hicode365.com/tags/%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B/"},{"name":"生成式模型","slug":"生成式模型","permalink":"https://www.hicode365.com/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B/"},{"name":"回归","slug":"回归","permalink":"https://www.hicode365.com/tags/%E5%9B%9E%E5%BD%92/"},{"name":"分类","slug":"分类","permalink":"https://www.hicode365.com/tags/%E5%88%86%E7%B1%BB/"},{"name":"python","slug":"python","permalink":"https://www.hicode365.com/tags/python/"},{"name":"数据类型","slug":"数据类型","permalink":"https://www.hicode365.com/tags/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"},{"name":"docker","slug":"docker","permalink":"https://www.hicode365.com/tags/docker/"},{"name":"tfserving","slug":"tfserving","permalink":"https://www.hicode365.com/tags/tfserving/"},{"name":"git","slug":"git","permalink":"https://www.hicode365.com/tags/git/"},{"name":"命令","slug":"命令","permalink":"https://www.hicode365.com/tags/%E5%91%BD%E4%BB%A4/"}]}