{"meta":{"title":"hicode","subtitle":"愿你出走半生 归来仍是少年","description":"写下理解留下轨迹，用工程沉淀算法思考对抗复杂，在算法与工程交汇处构建可落地的智能。持续学习，长期主义，做可复利的技术人。","author":"hicode365","url":"https://www.hicode365.com","root":"/"},"pages":[{"title":"about","date":"2021-09-27T09:02:54.000Z","updated":"2026-02-21T10:23:27.271Z","comments":true,"path":"about/index.html","permalink":"https://www.hicode365.com/about/index.html","excerpt":"","text":"写下理解，留下轨迹。 用工程沉淀算法，思考对抗复杂。 在算法与工程交汇处，构建可落地的智能。 持续学习，长期主义，做可复利的技术人。"},{"title":"categories","date":"2021-09-27T08:54:47.000Z","updated":"2023-08-15T02:10:06.000Z","comments":true,"path":"categories/index.html","permalink":"https://www.hicode365.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-09-27T08:55:32.000Z","updated":"2023-08-15T02:10:06.000Z","comments":true,"path":"tags/index.html","permalink":"https://www.hicode365.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"机器学习导论","slug":"机器学习/机器学习导论","date":"2026-02-22T01:35:12.017Z","updated":"2026-02-22T06:59:57.970Z","comments":true,"path":"cuidoJbT3CxBKq6GN_8yRPhUF.html","permalink":"https://www.hicode365.com/cuidoJbT3CxBKq6GN_8yRPhUF","excerpt":"","text":"引言在传统的编程范式中，我们输入数据和规则，通过计算机得到答案。而机器学习（Machine Learning）则不同，我们向计算机输入数据和答案（即最终的结果），让计算机自己去发现其中的学习模型、发现规律，从而构建能够进行预测的数学模型。简而言之，机器学习是一门研究如何让计算机从数据中自动获取规律，并利用这些规律来预测未知、辅助决策的科学。 机器学习是一门交叉学科，根据不同的视角可以被划分为多种范式，如： 从统计推断的哲学思想划分，机器学习模型可以划分为：频率学派与贝叶斯学派。前者认为模型的参数是固定但未知的“常数”，学习的过程就是通过优化方法寻找这个最优值；而后者则认为参数本身服从某种分布，学习的过程是在观察数据后，对先验信念进行更新。 若从建模的最终目标划分，算法模型又可分为判别式模型与生成式模型。判别式模型关注的是寻找不同类别之间的决策边界，直接学习如何划分数据；而生成式模型则试图理解数据本身的产生过程，通过学习联合分布来间接进行预测。 按统计推断划分频率学派核心思想频率派（Frequentist）将概率解释为事件在长期重复试验中发生的频率。在这一框架下，模型参数 $ \\theta $ 被视为一个确定的未知常量，虽然我们不知道它的具体值，但它本身并不具有随机性。我们只能通过观测数据来估计这个常量的值。 最大似然估计给定观测数据集 $ X $，所有样本的联合概率为： $$p(X|\\theta) = \\prod_{i=1}^N p(x_i|\\theta)$$ 这个联合概率被称为似然函数（Likelihood Function），它衡量了在特定参数 $ \\theta $ 下观测到当前数据的可能性。频率派的估计方法就是寻找能使似然函数最大化的参数值，即最大似然估计（Maximum Likelihood Estimation, MLE）： $$\\theta_{MLE} = \\arg\\max_{\\theta} p(X|\\theta) = \\arg\\max_{\\theta} \\sum_{i=1}^N \\log p(x_i|\\theta)$$ 由于对数函数是单调递增的，且能将连乘转化为连加便于计算，实际中通常使用对数似然。 频率派与机器学习频率派的思想深刻影响了统计机器学习的发展。许多经典算法都可以从MLE的角度理解： 线性回归：假设误差服从高斯分布时，MLE等价于最小二乘法 逻辑回归：直接对条件概率 $ p(y|x) $ 进行MLE估计 支持向量机：可以看作是在特定损失函数下的频率派方法 频率派方法的优势在于其理论简洁性、计算效率高，且在大样本条件下具有良好的渐近性质。然而，它也存在明显的局限性：无法融入先验知识，在小样本情况下容易过拟合，且只能给出点估计而非分布估计。 频率学派的参数估计 方法 全称 核心思想 适用场景 MLE 最大似然估计 $\\hat{\\theta}_{MLE} = \\arg\\max P(D \\mid \\theta)$ 寻找使观测数据出现概率最大的参数 最通用。大样本下具有渐近正态性、一致性。对模型假设敏感。 MM 矩估计法 令样本矩等于总体矩：$E[X^k] = \\frac{1}{n}\\sum X_i^k$ 简单快速，不需要知道分布的具体形式。但在小样本下往往不是有效估计量。 LSE 最小二乘估计 $\\min \\sum (y_i - \\hat{y}_i)^2$ 最小化残差平方和 线性回归。不需要概率分布假设（若误差服从正态分布，则等价于 MLE）。 GMM 广义矩估计 利用过定矩条件（矩条件多于参数），最小化二次型距离 经济计量学常用。解决了内生性问题，且不需要对误差分布做强假设。 M-估计 稳健估计 替换平方损失函数为更抗噪的 $\\rho$ 函数（如 Huber 损失） 数据有异常值（Outliers）时。比 MLE 更鲁棒。 EM 期望极大算法 交替进行 E-step（求期望）和 M-step（极大化似然） 含有隐变量（Latent variables）的概率模型，如 GMM 聚类、HMM。 贝叶斯学派核心思想贝叶斯派（Bayesian）将概率解释为对事件发生的不确定性的度量，这种不确定性可以是主观的信念。在这一框架下，参数 $ \\theta $ 被视为一个随机变量，服从某个先验分布 $ p(\\theta) $，这个先验分布反映了我们在看到数据之前对参数的认知。 贝叶斯定理与后验分布当我们观测到数据 $ X $ 后，根据贝叶斯定理，我们可以更新对参数的认知，得到后验分布（Posterior Distribution）： $$p(\\theta|X) = \\frac{p(X|\\theta) \\cdot p(\\theta)}{p(X)} = \\frac{p(X|\\theta) \\cdot p(\\theta)}{\\int p(X|\\theta) \\cdot p(\\theta) \\, d\\theta}$$ 其中： $ p(X|\\theta) $ 是似然函数，与频率派中的定义相同 $ p(\\theta) $ 是先验分布，体现了我们对参数的主观先验知识 $ p(X) = \\int p(X|\\theta)p(\\theta)d\\theta $ 是边缘似然，也称为证据（Evidence），作为归一化常数确保后验概率之和为1 最大后验估计虽然贝叶斯派的完整结果是整个后验分布，但在实际应用中有时也需要一个点估计。这时可以采用最大后验估计（Maximum A Posteriori, MAP）： $$\\theta_{MAP} = \\arg\\max_{\\theta} p(\\theta|X) = \\arg\\max_{\\theta} p(X|\\theta) \\cdot p(\\theta) \\tag{4}$$ MAP估计巧妙地结合了似然函数和先验信息。从优化的角度看，先验分布起到了正则化的作用：例如，高斯先验对应L2正则化，拉普拉斯先验对应L1正则化。 贝叶斯预测贝叶斯方法真正的优势在于其能够进行概率预测。当我们得到后验分布后，对于新样本 $ x_{new} $ 的预测分布可以通过对参数空间进行积分得到： $$p(x_{new}|X) = \\int p(x_{new}|\\theta) \\cdot p(\\theta|X) \\, d\\theta \\tag{5}$$ 这个过程被称为贝叶斯模型平均（Bayesian Model Averaging）。与频率派只使用单一最优参数不同，贝叶斯方法考虑了所有可能的参数值，并依据后验概率进行加权平均，从而自然地体现了模型的不确定性。 贝叶斯派与机器学习贝叶斯派思想催生了**概率图模型（Probabilistic Graphical Models）**这一重要领域，包括： 朴素贝叶斯分类器：最简单的贝叶斯模型，假设特征条件独立 高斯过程：非参数贝叶斯方法，用于回归和分类 贝叶斯神经网络：为神经网络权重赋予先验分布 贝叶斯方法的优势在于：能够自然地融合先验知识、防止过拟合、提供不确定性估计、适用于小样本学习。但同时也面临挑战：后验分布的计算通常涉及高维积分，难以解析求解，需要借助近似方法如马尔可夫链蒙特卡洛（MCMC）、变分推断等。 贝叶斯学派的参数估计 方法 核心思想 输出形式 特点 MAP 最大后验估计 $\\arg\\max P(D \\mid \\theta) P(\\theta)$ 点估计 (单个数值) 相当于 MLE + 先验。在线性回归中增加高斯先验即等价于 L2 正则化 (Ridge)。 贝叶斯推断 计算完整的后验分布 $P(\\theta \\mid D) = \\frac{P(D \\mid \\theta) P(\\theta)}{\\int P(D \\mid \\theta) P(\\theta) d\\theta}$ 后验分布 提供不确定性度量（均值、方差）。当先验是共轭先验时有解析解。 MCMC 马尔可夫链蒙特卡洛采样 (如 Gibbs 采样) 采样样本点集 通过随机采样模拟复杂的后验分布。万能但慢，适用于高维复杂模型。 变分推断 (VI) 将推断问题转化为优化问题 寻找一个简单分布 $q(\\theta)$ 最小化 $KL(q | p$ 近似解析分布 牺牲了一定精度换取速度。是大规模深度贝叶斯学习（如 VAE）的核心。 频率派与贝叶斯派对比尽管存在哲学分歧，两大学派在方法论上有着深刻的联系： MLE与MAP的关系：当先验分布为均匀分布时，MAP估计退化为MLE估计 正则化视角：MLE + 正则化项 等价于 MAP（特定先验） 大样本一致性：当样本量趋于无穷时，后验分布会集中在MLE估计附近，贝叶斯估计渐近等价于频率派估计 现代机器学习中，两大学派的界限正变得越来越模糊。研究者们开始汲取两者的优势： 贝叶斯深度学习：将贝叶斯思想引入深度神经网络，解决过拟合和不确定性估计问题 集成学习：结合多个模型的思想与贝叶斯模型平均有异曲同工之妙 概率编程：提供灵活的框架来表达和求解概率模型 变分自编码器（VAE）：巧妙结合了深度学习与变分推断 应用场景选择在实际问题中，选择哪种方法取决于具体需求： 数据量巨大：频率派方法通常计算更高效 小样本或零样本学习：贝叶斯方法可以利用先验知识 需要不确定性估计：贝叶斯方法自然提供 模型解释性要求高：简单频率派模型更易解释 计算资源有限：频率派方法通常更友好 按建模目标划分判别式 直接对后验概率 P(y | x) 进行建模。 关注点：直接学习决策边界，即不同类别之间的界限。 例子：逻辑回归、神经网络、支持向量机。 比喻：学会直接区分狗和猫的图片（只看区别）。 生成式 间接地对似然 (Likelihood) P(x | y) 和先验 (Prior) P(y) 进行建模。 关注点：为每个类别单独建模其特征的分布。“生成式”是因为一旦学到了 P(x | y)，就可以为任何类别 y 生成新的样本 x。 步骤： 为每个类别 y 假设一个特征分布模型（例如高斯分布）。 从训练数据中估计每个类别分布的参数（如均值、方差）。 利用贝叶斯定理，将学到的 P(x | y) 和 P(y) 转换为最终用于分类的 P(y | x)。 比喻：分别学习“狗看起来是什么样”和“猫看起来是什么样”的完整模型，然后对于一个新动物，看它更符合哪个模型。 判别式与生成式的模型对比判别模型 和 生成模型 的根本区别在于它们解决问题的思路和关注点不同。 判别模型 致力于 “找到区别”。 思路：直接学习不同类别数据之间的决策边界，而不关心单个类别本身的具体样貌。 目标：回答“它更像是猫还是狗？” 好比：一个裁判，他不需要会画画，只需要掌握一个关键标准（比如身长）来快速区分两者。 生成模型 致力于 “理解本质”。 思路：分别学习每一类数据（如猫、狗）的整体特征和内部结构，为每个类别建立一个完整的“概念模型”。 目标：回答“猫&#x2F;狗长什么样子？” 好比：一个艺术家，他需要透彻地了解猫和狗的骨骼、肌肉、毛发等所有细节，才能把它们画出来。 特征 判别模型（Discriminative） 生成模型（Generative） 核心思想 学习类别之间的边界，找到“差异”。 学习数据本身的分布，理解每一类的“本质”。 解决的问题 这是X还是Y 什么是X 学习内容 条件概率p(y|x) （x是数据特征，y是数据标签） 联合概率p(x,y)、分布p(x)（无监督的情况） 能力 主要用于分类和回归，无法生成新数据。 既可以进行分类，也可以生成新的数据（如画一只猫）。 类比 只学会一个投机取巧的判别技巧（如比身长）。 学完后对猫狗有直观认知，能画出它们。 常见算法 逻辑回归、支持向量机、决策树、CRF、神经网络 朴素贝叶斯、高斯混合、隐马尔可夫模型、VAF、生成对抗网络 判别模型因为只专注于区分，因此不具备生成能力；而生成模型因为学会了数据的“本质”，所以具备生成新数据的能力。 ChatGPT、Midjourney这类能创作内容（生成文本、图像）的模型，其核心都是生成模型。而许多用于图像分类、垃圾邮件过滤等任务的模型，则更多是判别模型。 生成与判别式模型的选择 当数据量较少或对数据分布有较强先验知识时，生成学习方法可能更有效。 适用于数据生成、缺失值处理； 在小样本下可能比判别模型（如逻辑回归）更鲁棒。 当数据量充足且计算资源有限时，判别学习方法可能更合适。 按学习目标划分回归（Regression） 回归旨在建立输入特征与连续型输出变量之间的映射关系，回归任务的目标是预测一个连续的数值输出 $y \\in \\mathbb{R}$， 核心思想：找到一条曲线&#x2F;超平面，使预测值 $\\hat{y}$ 尽可能接近真实值 $y$，最小化预测误差: $ \\min_\\theta \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 $ 应用场景：预测房价、售额预测、股票价格等。 常见的回归算法: 线性回归：最简单的回归模型，假设输入与输出呈线性关系 岭回归&#x2F;Lasso：在线性回归基础上加入L2&#x2F;L1正则化，防止过拟合 决策树回归：通过树结构分段拟合数据 随机森林回归：集成多棵决策树，降低方差 梯度提升回归（GBDT、XGBoost）：通过迭代优化残差 支持向量回归（SVR）：利用核方法处理非线性关系 神经网络：能够拟合任意复杂的非线性函数 分类（Classification） 目标是预测离散的类别标签（Y）。 例如：判断邮件是否为垃圾邮件（类别：垃圾邮件&#x2F;非垃圾邮件）、识别图像中的物体类别（类别：猫&#x2F;狗&#x2F;车等）。 预测离散类别标签。根据类别数量可分为： 二分类： 只有两个类别，例如垃圾邮件分类。 多分类：包含多个类别，例如手写数字识别（0-9）。 多标签分类：一个样本可能属于多个类别 目标给定输入特征向量 $\\mathbf{x} \\in \\mathbb{R}^d$，分类问题的目标是预测其所属的离散类别标签 $y \\in \\{1, 2, ..., K\\}$。 从概率视角看，我们需要计算后验概率（posterior probability）：$$P(y &#x3D; k \\mid \\mathbf{x}), \\quad k &#x3D; 1,2,…,K$$ 所有分类模型最终都采用最大后验决策规则（MAP decision rule）进行预测：$$\\hat{y} &#x3D; \\arg\\max_{k} P(y &#x3D; k \\mid \\mathbf{x})$$即选择使后验概率最大的类别作为预测结果。 建模判别式学习建模（Discriminative） 直接建模 $P(y|\\mathbf{x})$ 关注决策边界，不关心数据本身的分布 生成式学习建模（Generative） 间接建模：先学习联合分布 $ P(\\mathbf{x}, y) = P(y)P(\\mathbf{x}|y) $ 通过贝叶斯定理计算后验：$ P(y|\\mathbf{x}) = \\frac{P(y)P(\\mathbf{x}|y)}{\\sum_{k=1}^K P(y=k)P(\\mathbf{x}|y=k)} $ 参数估计无论采用判别式还是生成式方法，模型都包含需要从数据中学习的参数 $\\theta$。参数估计主要有两种方法： 频率派的最大似然估计（MLE）​ 将参数视为确定的未知常量，寻找使训练数据似然函数最大化的参数值：$$\\theta_{MLE} &#x3D; \\arg\\max_\\theta \\prod_{i&#x3D;1}^N P(y_i|\\mathbf{x}_i, \\theta)$$​ 等价于最小化经验风险。 贝叶斯派的最大后验估计（MAP）​ 将参数视为随机变量，在MLE的基础上引入了参数的先验分布$p(\\theta)$，通过贝叶斯定理求使后验概率最大的参数值：$$\\theta_{MAP} &#x3D; \\arg\\max_\\theta \\prod_{i&#x3D;1}^N P(y_i|\\mathbf{x}_i, \\theta) \\cdot p(\\theta)$$ 正则化效应：先验分布 $p(\\theta)$ 等价于对参数施加约束，防止过拟合 高斯先验 $p(\\theta) \\sim \\mathcal{N}(0, \\sigma^2)$ ↔ L2 正则化 拉普拉斯先验 $p(\\theta) \\sim \\text{Laplace}(0, b)$ ↔ L1 正则化 在数据稀缺时，先验知识可提供更稳健的估计 当先验为均匀分布时，MAP 退化为 MLE 完整流程 训练阶段：使用MLE或MAP估计模型参数 $\\hat{\\theta}$ 预测阶段：对于新样本 $\\mathbf{x}_{new}$，计算后验概率：$P(y=k|\\mathbf{x}_{new}, \\hat{\\theta})$ 决策阶段：应用最大后验决策规则：$\\hat{y} = \\arg\\max_k P(y=k|\\mathbf{x}_{new}, \\hat{\\theta})$ 聚类（Clustering）聚类是一种无监督学习任务，目标是将未标注的样本划分为若干个簇（cluster），使得同一簇内的样本尽可能相似，不同簇间的样本尽可能不同。 给定数据集 $\\{\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_N\\}$，聚类算法旨在找到一种划分 $C = \\{C_1, C_2, ..., C_K\\}$，使得： 簇内相似度最大化：$\\max \\sum_{k=1}^K \\sum_{\\mathbf{x}_i \\in C_k} \\text{sim}(\\mathbf{x}_i, \\mu_k)$ 簇间相似度最小化：$\\min \\sum_{k \\lt j} \\text{sim}(\\mu_k, \\mu_j)$，其中 $\\mu_k$ 是第 $k$ 个簇的中心（或代表点），$\\text{sim}(\\cdot)$ 是相似度度量。 降维（Dimensionality Reduction）降维是将高维数据映射到低维空间的过程，同时尽可能保留原始数据的重要结构信息。它是无监督学习的重要分支，也是数据预处理的关键步骤。 给定高维数据 $\\{\\mathbf{x}_i \\in \\mathbb{R}^D\\}_{i=1}^N$，降维算法寻找映射 $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}^d$，其中 $d \\ll D$，使得：$$\\mathbf{z}_i = f(\\mathbf{x}_i) \\in \\mathbb{R}^d$$且 $\\{\\mathbf{z}_i\\}$ 尽可能保留 $\\{\\mathbf{x}_i\\}$ 的重要结构信息。 需要降维的场景: 维度灾难（Curse of Dimensionality）：随着维度增加，数据变得稀疏，距离度量失效，模型复杂度指数级增长 可视化需求：人类只能理解2D或3D空间，降维使高维数据可视化成为可能 计算效率：减少特征数量，降低模型训练和预测的时间复杂度 去噪：丢弃不重要的维度，保留主要信号 特征提取：从原始特征中构造更有代表性的新特征 排序（Ranking）排序任务的目标是学习一个排序函数，能够根据输入特征对项目集合进行排序，使得相关性高的项目排在前面。排序问题是信息检索和推荐系统的核心。 排序的应用场景: 搜索引擎：根据用户查询对网页进行排序 推荐系统：为用户推荐最感兴趣的商品、视频、音乐 问答系统：对候选答案按相关性排序 广告点击率预测：排序广告以提高点击率 社交媒体：排序信息流中的帖子 结语机器学习的发展，本质上是在表示、评估与优化这三个核心要素上的持续演进：选择何种方式表示数据与模型，如何定义优劣，以及通过何种策略寻找最优解。 深度学习的崛起摒弃了人工特征工程，通过分层抽象自动从原始数据中提取层次化特征，使得模型能够感知高维空间中的复杂结构。然而，强大的表示能力也伴随着挑战——海量参数需要海量数据，黑箱特性损害了可解释性。 强化学习的复兴则拓展了学习的范式。它不再满足于拟合静态数据，而是在与环境的交互中通过试错学习最优策略，将机器学习从模式识别推向序贯决策。这种范式更接近生物学习的本质，在机器人、博弈、控制等领域展现出独特价值。 然而，技术的飞速发展机器学习仍面临多方面的挑战： 数据效率：人类可以从少量样本中学习，而当前模型仍然依赖海量数据。如何让机器学习像人类一样“举一反三”，仍是未解难题。 鲁棒性与泛化：分布外泛化、对抗样本、虚假相关——模型在实验室环境外的表现往往不尽如人意。 可解释性与可信赖：随着模型进入医疗、金融、司法等高风险领域，黑箱决策的风险日益凸显。 价值对齐：当模型越来越强大，如何确保其目标与人类价值观一致，成为关乎未来的重要课题。 计算效率与可持续性：大模型的训练消耗惊人，如何在性能与能耗之间取得平衡，既是技术问题，也是环境问题。 常见模型划分一览表 算法模型 频率派 贝叶斯派 判别式 生成式 参数估计&#x2F;核心准则 线性回归 是 是 (贝叶斯线性回归) 是 否 最小二乘 (OLS) &#x2F; MLE 逻辑回归 是 是 (拉普拉斯近似等) 是 否 极大似然估计 (MLE) 决策树&#x2F;随机森林&#x2F;提升树 是 否 是 否 信息增益 &#x2F; 基尼系数 &#x2F; MSE SVM 是 否 是 否 合页损失 + 正则化 (对偶优化) PCA (主成分分析) 是 是 (Probabilistic PCA) 不适用 是 (概率视角下) 方差最大化 &#x2F; 投影误差最小化 LDA (线性判别分析) 是 否 否 是 Fisher 准则 &#x2F; 类内类间散度 高斯判别分析 (GDA) 是 是 否 是 联合概率 $P(x,y)$ 建模 高斯过程 (GP) 否 是 是 否 核函数 + 边际似然最大化 高斯混合模型 (GMM) 是 是 (贝叶斯 GMM) 否 是 EM 算法 朴素贝叶斯 是 是 否 是 MAP (带平滑) &#x2F; MLE 贝叶斯网络 否 是 否 是 结构学习 + 条件概率表 隐马尔可夫 (HMM) 是 是 (变分推断) 否 是 EM 算法 条件随机场 (CRF) 是 否 是 否 极大似然 (梯度上升) 感知机 是 否 是 否 随机梯度下降 (SGD) 深度神经网络 (DNN) 是 是 (BNN) 是 视模型而定 反向传播 (BP) + 优化器","categories":[{"name":"机器学习","slug":"ml","permalink":"https://www.hicode365.com/categories/ml/"}],"tags":[{"name":"频率学派","slug":"频率学派","permalink":"https://www.hicode365.com/tags/%E9%A2%91%E7%8E%87%E5%AD%A6%E6%B4%BE/"},{"name":"贝叶斯学派","slug":"贝叶斯学派","permalink":"https://www.hicode365.com/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E6%B4%BE/"},{"name":"判别式模型","slug":"判别式模型","permalink":"https://www.hicode365.com/tags/%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B/"},{"name":"生成式模型","slug":"生成式模型","permalink":"https://www.hicode365.com/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B/"},{"name":"回归","slug":"回归","permalink":"https://www.hicode365.com/tags/%E5%9B%9E%E5%BD%92/"},{"name":"分类","slug":"分类","permalink":"https://www.hicode365.com/tags/%E5%88%86%E7%B1%BB/"}]},{"title":"python中的数据类型","slug":"工程与实战/Python中的数据类型","date":"2023-08-15T02:10:06.000Z","updated":"2026-01-01T15:46:05.612Z","comments":true,"path":"cuidjjGhKf00NbW7ycIsJFt6m.html","permalink":"https://www.hicode365.com/cuidjjGhKf00NbW7ycIsJFt6m","excerpt":"","text":"Python中的数据类型可变数据类型对变量的值进行修改时，变量对应的内存地址不变，对应的值发生了改变，这种数据类型就称为可变数据类型。 不可变数据类型对变量的进行修改时，变量对应的内存地址发生了改变(变量指向了新的内存)，从而修改了变量的值，而变量对应的原内存的值并没有被改变，这种数据类型就称为可变数据类型。 也就是：不可变数据类型更改后地址发生改变，可变数据类型更改地址不发生改变 常用数据类型 数据类型 是否是可变数据类型 是否有序 None (空) 不可变 - int (整数) 不可变 - float (浮点) 不可变 - bool (布尔) 不可变 - str (字符串) 不可变 - tuple (元组) 不可变 序列类型，有序 list (列表) 可变 序列类型，有序 set (集合) 可变 序列类型，无序，不可重复 dict (字典) 可变 映射类型，v3.6及以后无有序, 前面版本无序 扩展 数据类型 是否是可变数据类型 是否有序 说明 bytes 不可变 - 定义字节：b’hello’,bytes(5) bytearray 可变 - 定义字节数组：bytearray(b’hello’), bytearray(10) complex (复数) 不可变 - 由一个实数和一个虚数组合构成，如：4+3j frozenset (冻结的set) 不可变 无序 冻结的set初始化后不能再添加或删除元素 array (数组) 可变 有序 数组中的元素必须是同一类型 OrderedDict 可变 有序 key有序，setdefault取值key不存在也不报错 defaultdict 可变 有序 取值时Key不存在也不会抛出KeyError异常 deque 可变 有序 高效插入和删除的双向队列列表 常见数据类型的操作和转换list列表[ ]list是**&#x3D;&#x3D;可变&#x3D;&#x3D;、&#x3D;&#x3D;可重复&#x3D;&#x3D;的&#x3D;&#x3D;有序&#x3D;&#x3D;**列表，里面的元素的数据类型也可以不同(也可以是另一个list)。list可根据索引号取其中的数据。 list的生成12345678list1 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]print(&quot;list1: &quot;, list(list1)) # 输出： list1: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]list2 = list(range(0, 10))print(&quot;list2: &quot;, list(list2))list3 = [i*i for i in range(10) if i % 2 == 0]print(&quot;list3: &quot;, list(list3))list4 = (str(i) + j for i in range(0, 10, 2) for j in &quot;xyz&quot;)print(&quot;list4: &quot;, list(list4)) list元素反转、排序和次数统计12345678910list1 = [0, 1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9]list1.reverse() # 元素顺序反转print(&quot;list reverse: &quot;, list1)list1.sort(reverse=False) # 排序print(&quot;list sort: &quot;, list1)list1 = sorted(list1, reverse=True)print(&quot;list sort: &quot;, list1)times = list1.count(5) # 查看list中的元素出现的次数print(&quot;times: &quot;, times) list元素的添加、删除和取值12345678910111213141516list1.append(10)print(&quot;append value: &quot;, list1) # 添加元素list1.insert(1, 10) # 在指定位置添加元素print(&quot;insert value: &quot;, list1)list1.remove(10) # 删除指定value元素(第一个匹配的元素)print(&quot;remove value: &quot;, list1)value = list1.pop(12) # 删除指定index的元素并返回删除的值list1.pop() # 不指定index时默认删除最后一个元素list1.pop(-2) # 删除倒数第二个元素print(&quot;remove index: &quot;, list1)index_value = list1.index(3) # 查找第一个value为100的index值，如果不存在报TypeError异常print(&quot;index_value: &quot;, index_value)print(list1)index_value = list1.index(5, 7, 9) # 指定范围，从第7(包括)个到第9(不包括)个元素之间查找value为5的indexprint(&quot;index_value: &quot;, index_value) list添加多个元素、list的合并12345list2 = [100, 101, 102]# list1 = list1 + list2list1.extend(list2)print(list1)print(&quot;*&quot; * 10) list的遍历1234567891011for value in list1: print(&quot;value: %i&quot; % value)print(&quot;*&quot; * 50)for index in range(len(list1)): print(&quot;index: %i, value: %i&quot; % (index, list1[index]))print(&quot;*&quot; * 50)for index, value in enumerate(list1): print(&quot;index: %i, value: %i&quot; % (index, value))print(&quot;*&quot; * 50)for index, value in enumerate(list1, 100): # index从100开始 print(&quot;index: %i, value: %i&quot; % (index, value)) list中使用切片(slice)取值12345678910111213elements = list1[0:3] # 取第0到3条元素(包括头不包括尾)# elements = list1[:3]print(&quot;slice elements: &quot;, elements)elements = list1[1:] # 取第1到最后一个元素(包括头也包括尾)print(&quot;slice elements: &quot;, elements)elements = list1[-2] # 取倒数第二条print(&quot;slice elements: &quot;, elements)elements = list1[4:-2] # 取第四条到倒数第二条(包括头不包括尾)print(&quot;slice elements: &quot;, elements)elements = list1[0:6:2] # 取第0条到第6条中每2个取一个print(&quot;slice elements: &quot;, elements)elements = list1[:] # 取所有元素print(&quot;slice elements: &quot;, elements) &#x3D;&#x3D;列表、元组和字符串&#x3D;&#x3D;都可以使用切片进行操作 list的深copy和浅copy12345678910111213141516# 浅拷贝只拷贝了引用，没有拷贝内容list2 = list1list2[1] = 1000print(&quot;list1: &quot;, list1)print(&quot;list2: &quot;, list1)print(id(list1), id(list2))# 深拷贝是对于一个对象所有层次的拷贝(递归拷贝)list3 = list1.copy()# import copy# list3 = copy.copy(list1)# list3 = copy.deepcopy(list1)list1[1] = 1print(&quot;list1: &quot;, list1)print(&quot;list3: &quot;, list3)print(id(list1), id(list3)) set集合{ }set是**&#x3D;&#x3D;可变&#x3D;&#x3D;**、&#x3D;&#x3D;不可重复&#x3D;&#x3D;的&#x3D;&#x3D;无序&#x3D;&#x3D;列表。 &#x3D;&#x3D;set中不可以放入可变对象&#x3D;&#x3D;，因为无法判断两个可变对象是否相等而去重。 set的定义123456789101112131415161718192021222324set0 = &#123;0, 0, 1, 2, 3, 4, 5, 4, 5, 6&#125; # 直接定义set集合print(&quot;set0: &quot;, set0) # 输出 set0: &#123;0,1, 2, 3, 4, 5, 6&#125;set1 = set([0, 0, 1, 2, 3, 4, 5, 4, 5, 6]) # 通过list定义setprint(&quot;set1: &quot;, set1)set2 = set((0, 0, 1, 2, 3, 4, 5, 4, 5, 6)) # 通过tuple定义setprint(&quot;set2: &quot;, set2)set3 = set(&#123;&quot;x&quot;: 2, 10: &quot;b&quot;&#125;) # 通过dict定义setprint(&quot;set3: &quot;, set3) # 输出 set3: &#123;10, &#x27;x&#x27;&#125;my_list = [0, 0, 1, 2, 3, 4, 5, 4, 5]set4 = set(my_list) # set中不可以放入可变对象,然而为何放入list却不报错?print(&quot;set4: &quot;, set4) # 输出 set4: &#123;0, 1, 2, 3, 4, 5, 6&#125;# 由下面操作可以得出结论,set是先把list做遍历得到不可变的int对象类型后再放入set中my_list[0] = 10print(&quot;set4 with my_list changed: &quot;, set4) # 输出 &#123;0, 1, 2, 3, 4, 5, 6&#125;my_list.append([10, 20])print(&quot;my_list: &quot;, my_list)set5 = set(my_list) # 在list再放入list,此时将报错print(&quot;set5: &quot;, set5) set元素的添加、删除和取值12345678910111213141516set0 = &#123;0, 0, 1, 2, 3, 4, 5, 4, 5, 6&#125;print(&quot;set0: &quot;, set0)set0.add(&quot;cn&quot;) # 添加单个元素print(&quot;set0: &quot;, set0)set0.update([10, 20, 30]) # 添加多个元素print(&quot;set0: &quot;, set0)set0.add((&quot;com&quot;, &quot;cn&quot;)) # 添加元组(元组是不可变数据类型)print(&quot;set0: &quot;, set0)# set0.add([10, 20]) # 添加list报错,不能添加可变的数据类型(不能添加,但可以使用list创建set)# set0.add(&#123;10, 20&#125;) # 添加set报错,(可是使用不可变的frozenset添加:set0.add(frozenset(&#123;10, 20&#125;)))# set0.add(&#123;&quot;x&quot;: 2, 10: &quot;b&quot;&#125;) # 添加dict报错,不能添加可变的数据类型(不能添加,但可以使用dict创建set)set0.remove(&quot;cn&quot;) # 根据值删除元素(set不能根据索引删除)print(&quot;set0: &quot;, set0) set取并集和交集12345set1 = &#123;&quot;a&quot;, &quot;b&quot;, 4, 6, 100&#125;my_set = set0 | set1 # 取并集print(my_set)my_set = set0 &amp; set1 # 取交集print(my_set) set遍历注：set的遍历同list dict字典{ }dict是**&#x3D;&#x3D;无序&#x3D;&#x3D;，key&#x3D;&#x3D;不可重复&#x3D;&#x3D;、&#x3D;&#x3D;不可变&#x3D;&#x3D;**内容以key-value键值对形式存在的映射 dict中的key只能是不可变对象且唯一, 一个key对应一个value，多次对一个key设置value，后面的值会把前面的冲掉。 dict一般用在需要高速查找的很多地方。dict的key必须是不可变对象，这是因为dict根据key来计算value的存储位置，如果每次计算相同的key得出的结果不同，那dict内部就完全混乱了。这种通过key计算位置的算法称为哈希算法（Hash）。要保证hash的正确性，作为key的对象就不能变。在Python中，字符串、整数等都是不可变的，因此，可以放心地作为key。而list、set是可变的，所以就不能作为key。 dict的创建和增删改查12345678910111213141516dict1 = &#123;&quot;addr&quot;: &quot;北京&quot;, &quot;age&quot;: 18, &quot;gender&quot;: &quot;女&quot;&#125;dict1[&quot;height&quot;] = 1.77 # 添加元素dict1.pop(&quot;age&quot;) # 删除元素输出item_del = dict1.popitem() # 产出dict中的最后一个item并返回print(&quot;item_del: &quot;, item_del)dict1[&quot;addr&quot;] = &quot;深圳&quot; # 修改元素print(&quot;dict1: &quot;, dict1)keys = dict1.keys() # 获取dict的所有keyprint(&quot;keys: &quot;, keys) # dict_keys([&#x27;add&#x27;, &#x27;height&#x27;])addr = dict1.get(&quot;addr&quot;) # 根据key获取value,若key不存在报异常(defaultdict字典不报异常)print(&quot;addr: &quot;, addr)addr = dict1.setdefault(&quot;addr&quot;) # 根据key获取value,若key不存返回None,也可设置默认返回值print(&quot;addr: &quot;, addr)name = dict1.get(&quot;name&quot;, &quot;unknow&quot;) # 根据key获取value,若key不存返回默认值&#x27;unknow&#x27;print(&quot;name: &quot;, name) dict的遍历12345678910# dict的遍历for key in dict1: print(&quot;key: %s, value: %s&quot; % (key, dict1[key]))print(&quot;*&quot; * 50)for value in dict1.values(): print(&quot;value: &quot;, value)print(&quot;*&quot; * 50)for key, value in dict1.items(): print(&quot;key: %s, value: %s&quot; % (key, value)) dict的合并123dict2 = &#123;&quot;mobel&quot;: 15888888888, &quot;postal_code&quot;: 10000&#125; # 合并两个dictdict1.update(dict2)print(&quot;dict1: &quot;, dict1) dict和list的异同 list查找和插入的时间随着元素的增加而增加；占用空间小，浪费内存很少 dict查找和插入的速度极快，不会随着key的增加而变慢；需要占用大量的内存，内存浪费多。所以，dict是用空间来换取时间的一种方法。 dict的排序1234567891011121314151617181920212223# dict排序dict3 = &#123;&#x27;sh&#x27;: 3, &#x27;hz&#x27;: 2, &#x27;tj&#x27;: 1, &#x27;bj&#x27;: 5, &#x27;gz&#x27;: 2, &#x27;sz&#x27;: 4, &#x27;wh&#x27;: 1&#125;# 默认排序，并仅返回keykey_rank1 = sorted(dict3.keys(), reverse=False)print(&quot;key_rank1: &quot;, key_rank1)# 默认排序(以key来排序)，并返回key和valuedict_key_rank1 = sorted(dict3.items(), reverse=False)print(&quot;dict_key_rank1: &quot;, dict(dict_key_rank1))# 以key排序dict_key_rank2 = sorted(dict3.items(), key=lambda item: item[0], reverse=False)print(&quot;dict_key_rank2: &quot;, dict(dict_key_rank2))# 以value排序dict_value_rank1 = sorted(dict3.items(), key=lambda item: item[1], reverse=False)print(&quot;dict_value_rank1: &quot;, dict(dict_value_rank1))# 以value排序dict4 = &#123;&#x27;上海&#x27;: 3, &#x27;杭州&#x27;: 2, &#x27;天津&#x27;: 1, &#x27;北京&#x27;: 5, &#x27;广州&#x27;: 2, &#x27;深圳&#x27;: 4, &#x27;武汉&#x27;: 1&#125;dict_value_rank2 = sorted(dict4.items(), key=lambda item: item[1], reverse=True)print(&quot;dict_value_rank2: &quot;, dict(dict_value_rank2)) tuple元组( )tuple是**&#x3D;&#x3D;不可变&#x3D;&#x3D;、&#x3D;&#x3D;有序&#x3D;&#x3D;**的列表，所以一般在定义tuple时就进行初始化赋值。 注意： 在定义只有一个元素的tuple时其元素后面要加逗号 123tuple0 = () # 创建空元祖tuple0 = (1) # 不是tuple，会当成括号处理tuple0 = (1,) # 正确的tuple tuple虽然不可变但tuple中的元素对象却是可变的 12345my_list = [&quot;x&quot;, &quot;y&quot;]tuple1 = (&#x27;a&#x27;, &#x27;b&#x27;, my_list) # tuple包含list,list变化时,tuple1也就跟着变化print(&quot;tuple1: &quot;, tuple1) # tuple1: (&#x27;a&#x27;, &#x27;b&#x27;, [&#x27;x&#x27;, &#x27;y&#x27;])my_list.append(&quot;z&quot;)print(&quot;tuple1 with my_list changed: &quot;, tuple1) # tuple1变为(&#x27;a&#x27;, &#x27;b&#x27;, [&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;]) tuple的创建123tuple2 = (1, &quot;good&quot;, 2, 3, &quot;good&quot;, True) # 创建元组,里面的元素类型可以不同tuple3 = (&quot;a&quot;, &quot;b&quot;, *tuple2, 4, 5) # 元组引用另一个数组中的所有元素print(&quot;tuple3: &quot;, tuple3) tuple中元素的增删改查1234567891011element = tuple2[4] # 根据索引获取元组中的元素element = tuple2[-2] # 使用索引获取元组中的元素index = tuple2.index(&quot;good&quot;) # 获取第一个匹配给定值的index值del tuple2 # 删除元组# tuple2[4] = &quot;well&quot; # 修改元组的元素,报错tuple4 = (&quot;a&quot;, &quot;b&quot;, 4, 5, [6, 7, 8])print(&quot;tuple4: &quot;, tuple4)# tuple4[-1] = [10, 20, 30] #报错tuple4[-1][0] = 100 # 可以通过修改元组中的list,从而改变元组print(&quot;tuple4: &quot;, tuple4) # tuple4: (&#x27;a&#x27;, &#x27;b&#x27;, 4, 5, [100, 7, 8]) tuple的遍历注：tuple的遍历同list","categories":[{"name":"工程与实战","slug":"工程与实战","permalink":"https://www.hicode365.com/categories/%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"python","slug":"python","permalink":"https://www.hicode365.com/tags/python/"},{"name":"数据类型","slug":"数据类型","permalink":"https://www.hicode365.com/tags/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"}]},{"title":"docker常用命令","slug":"git&sql&docker/docker常用命令","date":"2022-11-27T14:28:39.000Z","updated":"2023-08-15T02:10:06.000Z","comments":true,"path":"1c41d072cac63725dbd48a1273b2fd97.html","permalink":"https://www.hicode365.com/1c41d072cac63725dbd48a1273b2fd97","excerpt":"","text":"docker常用命令12345678910111213141516docker pull tensorflow/serving #从仓库拉取镜像docker pull tensorflow/serving:latest-gpu #从仓库拉取GPU镜像docker pull tensorflow/serving:2.8.3-gpu #从仓库拉取GPU镜像docker run -it tensorflow/serving #进入到镜像中exit #退出镜像docker run tensorflow/serving #运行某个容器docker ps // 查看所有正在运行容器docker stop containerId // containerId 是容器的IDdocker ps -a // 查看所有容器docker ps -a -q // 查看所有容器IDdocker stop $(docker ps -a -q) // stop停止所有容器docker rm $(docker ps -a -q) // remove删除所有容器docker rm/rmi #删除容器/镜像docker cp local_files containerId:docker_files #本地文件复制到docker tf-serving部署123456789101112-p: 指定主机到docker容器的端口映射--mount: 表示要进行挂载,其中 type=bind: 是选择挂载模式， source: 要部署模型的存储路径，也就是挂载的源（必须是绝对路径）， target: 要挂载的目标位置，模型挂载到docker容器中的位置，也就是docker容器中的目录（放在集装箱的哪里）-t: 指定的是挂载到哪个容器-e: 环境变量 MODEL_NAME: 必须与target指定路径的最后一个文件夹名称相同--per_process_gpu_memory_fraction: 运行时所需的GPU显存资源最大比率的值设定-v: path1:path2 分别指模型在机器种储存的路径（必须是绝对路径），模型在容器中储存的路径（放在集装箱的哪里） 1234docker run -p 8500:8500 \\ --mount type=bind,source=/Users/coreyzhong/workspace/tensorflow/saved_model/,target=/models/test-model \\ -t tensorflow/serving:1.15.0 \\ -e MODEL_NAME=test-model --model_base_path=/models/test-model/ &amp; 12345model_path=&quot;/Users/havorld/jupyter/model_save/&quot;docker run -t --rm -p 8500:8500 -p 8501:8501 \\ -v &quot;$model_path/din:/models/tf_saved_models&quot; \\ -e MODEL_NAME=tf_saved_models \\ tensorflow/serving &amp; 1234567查看TensorFlow-Serving状态： curl http://localhost:8501/v1/models/$&#123;model_name&#125;查看TensorFlow-Serving模型信息： curl http://localhost:8501/v1/models/$&#123;model_name&#125;/metadata查看模型信息: saved_model_cli show --dir=&#x27;./$&#123;model_path&#125;/20220422104620&#x27; --all使用Http请求进行模型预测： curl -d &#x27;&#123;&quot;instances&quot;: [1,2,3,4,5]&#125;&#x27; -X POST http://localhost:8501/v1/models/$&#123;model_name&#125;:predict其中instances的value为模型输入Tensor的字符串形式，矩阵维度需要和Tensor对应。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586docker run -t --rm -p 8501:8501 \\ -v &quot;/home/Personas/havorld/tfserving/tf_saved_models:/models/tf_saved_models&quot; \\ -e MODEL_NAME=tf_saved_models \\ tensorflow/servingdocker run -t --rm -p 8501:8501 \\-v &quot;/Users/haopeng.meng/jupyter/tf_saved_models:/models/tf_saved_models&quot; \\-e MODEL_NAME=tf_saved_models \\tensorflow/servingcat /opt/logs/rec-feed-api/access.log | grep &quot;feed recommend-&gt; uid:55&quot; | grep &quot;id=34174686&quot;sudo docker run -t --rm -p 8501:8501 -p 8500:8500 \\ -v &quot;/home/meng.haopeng/tfserving/tf_saved_models:/models/tf_saved_models&quot; \\ -e MODEL_NAME=tf_saved_models \\ tensorflow/servingdocker run -p 8501:8501 -p 8500:8500 \\ --mount type=bind,source=/Users/haopeng.meng/jupyter/tf_saved_models,target=/models/tf_saved_models \\ -e MODEL_NAME=tf_saved_models \\ -t tensorflow/servingdocker run -p 8500:8500 \\ --mount type=bind,source=./intent/,target=/models/intent_score \\ -e MODEL_NAME=intent_score -t tensorflow/serving:1.10.0docker run -p 8501:8501 -p 8500:8500 --mount type=bind,source=/my/model/path/m,target=/models/m -e MODEL_NAME=m -t tensorflow/serving:2.1.0sudo docker run -t --rm -p 8501:8501 -p 8500:8500 \\ -v &quot;/home/meng.haopeng/tfserving/tf_saved_models:/models/tf_saved_models&quot; \\ -e MODEL_NAME=tf_saved_models \\ tensorflow/serving#docker run -t -p 443:8500 -p 8500:8501 -v &quot;/data/lsj/dmp/SavedModel/:/models/&quot; tensorflow/serving --model_config_file=/models/models.config --model_config_file_poll_wait_seconds=300# run use containerdocker run -t -p 8501:8500 --name=tf_serving_multi_version_01 -v &quot;/data/tf-model/models/:/models/&quot; tensorflow/serving --model_config_file=/models/models.config --model_config_file_poll_wait_seconds=300 --allow_version_labels_for_unavailable_models=true --enable_batching=true --batching_parameters_file=/models/batch.config# tf-serving部署docker run -t --rm -p 8501:8501 \\-v &quot;/Users/haopeng.meng/jupyter/tf_saved_models:/models/tf_saved_models&quot; \\-e MODEL_NAME=tf_saved_models \\tensorflow/servingdocker run -p 8501:8501 -p 8500:8500 \\ --mount type=bind,source=/Users/haopeng.meng/jupyter/tf_saved_models,target=/models/tf_saved_models \\ -e MODEL_NAME=tf_saved_models \\ -t tensorflow/serving sudo docker run -t --rm -p 8501:8501 -p 8500:8500 \\ -v &quot;/home/meng.haopeng/tfserving/tf_saved_models:/models/tf_saved_models&quot; \\ -e MODEL_NAME=tf_saved_models \\ tensorflow/servingsudo docker run --name feed -t --rm -p 8700:8500 -p 8701:8501 \\ -v &quot;/home/meng.haopeng/tfserving/tf_saved_models:/models/tf_saved_models&quot; \\ -e MODEL_NAME=tf_saved_models \\ tensorflow/servingdocker run --name feed -t --rm -p 8701:8501 -p 8700:8500 \\ --mount type=bind,source=/home/Personas/havorld/tfserving/model_save,target=/models/model_save \\ -e MODEL_NAME=model_save \\ -t tensorflow/serving:latest-gpu docker run --name feed -t --rm -p 8700:8500 -p 8701:8501 \\ --mount type=bind,source=/Users/haopeng.meng/Desktop/recommend/rec-alg-feed/model_save/din/serving/,target=/models/serving \\ -e MODEL_NAME=serving \\ -t tensorflow/serving docker run --name feed -t --rm -p 8700:8500 -p 8701:8501 \\ --mount type=bind,source=/Users/haopeng.meng/Desktop/recommend/serving/din/,target=/models/serving \\ -e MODEL_NAME=serving \\ -t tensorflow/serving","categories":[{"name":"docker","slug":"docker","permalink":"https://www.hicode365.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.hicode365.com/tags/docker/"},{"name":"tfserving","slug":"tfserving","permalink":"https://www.hicode365.com/tags/tfserving/"}]},{"title":"git常用命令","slug":"git&sql&docker/git常用命令","date":"2021-09-28T03:05:35.000Z","updated":"2023-08-15T02:10:06.000Z","comments":true,"path":"cuidUDNcydk4MBYkxZe1gJsQH.html","permalink":"https://www.hicode365.com/cuidUDNcydk4MBYkxZe1gJsQH","excerpt":"","text":"git常用命令123456789101112131415161718192021222324252627282930git clone https://github.com/username/project.github.io.git #拉取代码(master/main)git clone -b _dev https://github.com/username/project.github.io.git #拉取分支(非master或main分支)git checkout --track origin/_remote #获取指定的远程分支到本地git branch #查看本地分支git branch -a #查看远程分支git branch -vv#查看分支的绑定信息git branch _local #创建本地分支git branch -d _local #删除本地分支(当前分支不能停留在要删除的分支上)git checkout _local #切换到本地分支git checkout -b _local # 创建并切换到本地创建的分支git push --set-upstream origin _remote #创建远程分支git branch -r -d origin/_remote #删除远程分支(记得push一下 git push origin _remote)git checkout -b _local origin/_remote #创建本地分支绑定远程分支git status #查看修改过代码的类git diff #查看修改的代码git add src/main/java/com/so/alg/RecommendServiceImpl.java #添加修改的代码git commit -m &quot;recommend feed modified&quot; #给修改的代码添加注释git pull origin _remote #从远程更新变动的代码git push origin _local:_remote #提交git merge _remote #合并分支到master上(需要先切换到master分支上,在执行合并)git tag -a 2020071801 -m &#x27;v2.0部署&#x27; #添加taggit show 2020071801 #展示taggit push --tags #提交tag git配置SSH公钥和私钥1234567891011121314151617# 1.生成公钥和私钥(邮箱为github注册邮箱)ssh-keygen -t rsa -C &quot;xxxxxx@gmail.com&quot;# 2.设置公钥私钥key的保存位置(可以直接确认则保存在默认位置)Generating public/private rsa key pair.Enter file in which to save the key (/Users/username/.ssh/id_rsa):# 3.输入、重复输入密钥盐值Enter passphrase (empty for no passphrase):Enter same passphrase again:# 4.复制打印的公钥内容，并在github-&gt;Settings-&gt;SSH and GPG keys-&gt;New SSH Key中设置(title随意起)cat .ssh\\id_rsa.pub# 5.查看密钥是否配置成功(会提示输入盐值)ssh -T git@github.comEnter passphrase for key &#x27;/Users/username/.ssh/id_rsa&#x27;:Hi hicode360! You&#x27;ve successfully authenticated, but GitHub does not provide shell access.# 6.配置全局git信息git config --global user.name &quot;username&quot; # github用户名git config --global user.email &quot;xxxxxx@gmail.com&quot; #github注册邮箱","categories":[{"name":"git","slug":"git","permalink":"https://www.hicode365.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://www.hicode365.com/tags/git/"},{"name":"命令","slug":"命令","permalink":"https://www.hicode365.com/tags/%E5%91%BD%E4%BB%A4/"}]}],"categories":[{"name":"机器学习","slug":"ml","permalink":"https://www.hicode365.com/categories/ml/"},{"name":"工程与实战","slug":"工程与实战","permalink":"https://www.hicode365.com/categories/%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9E%E6%88%98/"},{"name":"docker","slug":"docker","permalink":"https://www.hicode365.com/categories/docker/"},{"name":"git","slug":"git","permalink":"https://www.hicode365.com/categories/git/"}],"tags":[{"name":"频率学派","slug":"频率学派","permalink":"https://www.hicode365.com/tags/%E9%A2%91%E7%8E%87%E5%AD%A6%E6%B4%BE/"},{"name":"贝叶斯学派","slug":"贝叶斯学派","permalink":"https://www.hicode365.com/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E6%B4%BE/"},{"name":"判别式模型","slug":"判别式模型","permalink":"https://www.hicode365.com/tags/%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B/"},{"name":"生成式模型","slug":"生成式模型","permalink":"https://www.hicode365.com/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B/"},{"name":"回归","slug":"回归","permalink":"https://www.hicode365.com/tags/%E5%9B%9E%E5%BD%92/"},{"name":"分类","slug":"分类","permalink":"https://www.hicode365.com/tags/%E5%88%86%E7%B1%BB/"},{"name":"python","slug":"python","permalink":"https://www.hicode365.com/tags/python/"},{"name":"数据类型","slug":"数据类型","permalink":"https://www.hicode365.com/tags/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"},{"name":"docker","slug":"docker","permalink":"https://www.hicode365.com/tags/docker/"},{"name":"tfserving","slug":"tfserving","permalink":"https://www.hicode365.com/tags/tfserving/"},{"name":"git","slug":"git","permalink":"https://www.hicode365.com/tags/git/"},{"name":"命令","slug":"命令","permalink":"https://www.hicode365.com/tags/%E5%91%BD%E4%BB%A4/"}]}