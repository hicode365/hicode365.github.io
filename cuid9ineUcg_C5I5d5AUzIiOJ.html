<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>SVM支持向量机 | hicode365</title>
  <meta name="description" content="引言支持向量机（Support Vector Machine, SVM）是一种经典的二分类模型，它的核心思想是：在特征空间中寻找一个决策边界（超平面），使得两类样本被正确分开，并且离决策边界最近的样本点（即支持向量）到该边界的距离尽可能大。这个距离被称为间隔（Margin），SVM因此也被称为最大间隔分类器。 决策函数$$ \left\{ (x_1^{(1)}, x_2^{(1)}, \ldots">
<meta property="og:type" content="article">
<meta property="og:title" content="SVM支持向量机">
<meta property="og:url" content="https://www.hicode365.com/cuid9ineUcg_C5I5d5AUzIiOJ">
<meta property="og:site_name" content="hicode">
<meta property="og:description" content="引言支持向量机（Support Vector Machine, SVM）是一种经典的二分类模型，它的核心思想是：在特征空间中寻找一个决策边界（超平面），使得两类样本被正确分开，并且离决策边界最近的样本点（即支持向量）到该边界的距离尽可能大。这个距离被称为间隔（Margin），SVM因此也被称为最大间隔分类器。 决策函数$$ \left\{ (x_1^{(1)}, x_2^{(1)}, \ldots">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.hicode365.com/images/v2-319cb9929efe5025d7ee029ac7e60098_r.png">
<meta property="og:image" content="https://www.hicode365.com/images/ScreenShot_2025-10-25_161439_348.png">
<meta property="og:image" content="https://www.hicode365.com/images/ScreenShot_2026-02-22_185554_909.png">
<meta property="article:published_time" content="2026-02-22T09:48:39.749Z">
<meta property="article:modified_time" content="2026-02-22T12:07:05.830Z">
<meta property="article:author" content="hicode365">
<meta property="article:tag" content="SVM">
<meta property="article:tag" content="支持向量积">
<meta property="article:tag" content="核函数">
<meta property="article:tag" content="软间隔">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.hicode365.com/images/v2-319cb9929efe5025d7ee029ac7e60098_r.png">
  <!-- Canonical links -->
  <link rel="canonical" href="https://www.hicode365.com/cuid9ineUcg_C5I5d5AUzIiOJ.html">
  
    <link rel="alternate" href="/atom.xml" title="hicode" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
    <link href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.css" rel="stylesheet">
  
  
<meta name="generator" content="Hexo 8.1.1"></head>


<body class="main-center theme-black# 主题颜色 theme-black theme-blue theme-green theme-purple" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://www.hicode365.com/" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.png" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">hicode</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">Algorithms Engineer</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Peking, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav menu-highlight">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">关于</span>
          </a>
        </li>
        
      </ul>
      
    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/docker/">docker</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/git/">git</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9E%E6%88%98/">工程与实战</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ml/">机器学习</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/SVM/" style="font-size: 13px;">SVM</a> <a href="/tags/docker/" style="font-size: 13px;">docker</a> <a href="/tags/git/" style="font-size: 13px;">git</a> <a href="/tags/python/" style="font-size: 13px;">python</a> <a href="/tags/tfserving/" style="font-size: 13px;">tfserving</a> <a href="/tags/%E5%88%86%E7%B1%BB/" style="font-size: 13px;">分类</a> <a href="/tags/%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B/" style="font-size: 13px;">判别式模型</a> <a href="/tags/%E5%91%BD%E4%BB%A4/" style="font-size: 13px;">命令</a> <a href="/tags/%E5%9B%9E%E5%BD%92/" style="font-size: 13px;">回归</a> <a href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E7%A7%AF/" style="font-size: 13px;">支持向量积</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/" style="font-size: 13px;">数据类型</a> <a href="/tags/%E6%A0%B8%E5%87%BD%E6%95%B0/" style="font-size: 13px;">核函数</a> <a href="/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B/" style="font-size: 13px;">生成式模型</a> <a href="/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E6%B4%BE/" style="font-size: 13px;">贝叶斯学派</a> <a href="/tags/%E8%BD%AF%E9%97%B4%E9%9A%94/" style="font-size: 13px;">软间隔</a> <a href="/tags/%E9%A2%91%E7%8E%87%E5%AD%A6%E6%B4%BE/" style="font-size: 13px;">频率学派</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled ">
        
          <li>
            
            <div class="item-thumb">
              <a href="/cuid9ineUcg_C5I5d5AUzIiOJ" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/ml/">机器学习</a>
              </p>
              <p class="item-title">
                <a href="/cuid9ineUcg_C5I5d5AUzIiOJ" class="title">SVM支持向量机</a>
              </p>
              <p class="item-date" style="display:none;">
                <time datetime="2026-02-22T09:48:39.749Z" itemprop="datePublished">2026-02-22</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-thumb">
              <a href="/cuidh5iGsdDS3Iq6RawHOWB5J" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/ml/">机器学习</a>
              </p>
              <p class="item-title">
                <a href="/cuidh5iGsdDS3Iq6RawHOWB5J" class="title">机器学习导论</a>
              </p>
              <p class="item-date" style="display:none;">
                <time datetime="2026-02-22T01:35:12.017Z" itemprop="datePublished">2026-02-22</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-thumb">
              <a href="/cuidDZEK6Bgd97PUqFs0w-F_G" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9E%E6%88%98/">工程与实战</a>
              </p>
              <p class="item-title">
                <a href="/cuidDZEK6Bgd97PUqFs0w-F_G" class="title">python中的数据类型</a>
              </p>
              <p class="item-date" style="display:none;">
                <time datetime="2023-08-15T02:10:06.000Z" itemprop="datePublished">2023-08-15</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-thumb">
              <a href="/1c41d072cac63725dbd48a1273b2fd97" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/docker/">docker</a>
              </p>
              <p class="item-title">
                <a href="/1c41d072cac63725dbd48a1273b2fd97" class="title">docker常用命令</a>
              </p>
              <p class="item-date" style="display:none;">
                <time datetime="2022-11-27T14:28:39.000Z" itemprop="datePublished">2022-11-27</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-thumb">
              <a href="/cuidrRz6-aJ020Ll6zs2_iuIT" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/git/">git</a>
              </p>
              <p class="item-title">
                <a href="/cuidrRz6-aJ020Ll6zs2_iuIT" class="title">git常用命令</a>
              </p>
              <p class="item-date" style="display:none;">
                <time datetime="2021-09-28T03:05:35.000Z" itemprop="datePublished">2021-09-28</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
  <aside class="sidebar sidebar-toc collapse   in  " id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    <nav id="toc" class="article-toc">
      <h3 class="toc-title">文章目录</h3>
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E5%87%BD%E6%95%B0"><span class="toc-number">2.</span> <span class="toc-text">决策函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%97%B4%E9%9A%94%EF%BC%88Margin%EF%BC%89%E4%B8%8E%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="toc-number">3.</span> <span class="toc-text">间隔（Margin）与优化目标</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97"><span class="toc-number">3.1.</span> <span class="toc-text">距离计算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%82%B9%E5%88%B0%E7%9B%B4%E7%BA%BF%E7%9A%84%E8%B7%9D%E7%A6%BB"><span class="toc-number">3.1.1.</span> <span class="toc-text">点到直线的距离</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%82%B9%E5%88%B0%E5%B9%B3%E9%9D%A2%E7%9A%84%E8%B7%9D%E7%A6%BB"><span class="toc-number">3.1.2.</span> <span class="toc-text">点到平面的距离</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E9%97%B4%E9%9A%94"><span class="toc-number">3.2.</span> <span class="toc-text">函数间隔</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%A0%E4%BD%95%E9%97%B4%E9%9A%94%EF%BC%88%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87%EF%BC%89"><span class="toc-number">3.3.</span> <span class="toc-text">几何间隔（优化目标）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E9%97%B4%E9%9A%94VS%E5%87%A0%E4%BD%95%E9%97%B4%E9%9A%94"><span class="toc-number">3.4.</span> <span class="toc-text">函数间隔VS几何间隔</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E9%97%B4%E9%9A%94%E6%80%9D%E6%83%B3"><span class="toc-number">3.5.</span> <span class="toc-text">最大间隔思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E4%B8%8E%E5%8C%96%E7%AE%80"><span class="toc-number">3.6.</span> <span class="toc-text">目标函数与化简</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A1%AC%E9%97%B4%E9%9A%94%E4%B8%8E%E8%BD%AF%E9%97%B4%E9%9A%94"><span class="toc-number">3.7.</span> <span class="toc-text">硬间隔与软间隔</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A1%AC%E9%97%B4%E9%9A%94"><span class="toc-number">3.7.1.</span> <span class="toc-text">硬间隔</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AF%E9%97%B4%E9%9A%94"><span class="toc-number">3.7.2.</span> <span class="toc-text">软间隔</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%B1%82%E8%A7%A3%EF%BC%88%E7%A1%AC%E9%97%B4%E9%9A%94%EF%BC%89"><span class="toc-number">4.</span> <span class="toc-text">拉格朗日对偶求解（硬间隔）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%87%BD%E6%95%B0"><span class="toc-number">4.1.</span> <span class="toc-text">拉格朗日函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KKT%E6%9D%A1%E4%BB%B6%E4%B8%8E%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F"><span class="toc-number">4.2.</span> <span class="toc-text">KKT条件与支持向量</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E4%B8%8E%E6%B1%82%E8%A7%A3%EF%BC%88%E8%BD%AF%E9%97%B4%E9%9A%94%EF%BC%89"><span class="toc-number">5.</span> <span class="toc-text">拉格朗日对偶与求解（软间隔）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%BE%E5%BC%9B%E5%8F%98%E9%87%8F%E4%B8%8E%E6%83%A9%E7%BD%9A%E5%8F%82%E6%95%B0"><span class="toc-number">5.1.</span> <span class="toc-text">松弛变量与惩罚参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KKT%E6%9D%A1%E4%BB%B6%E4%B8%8E%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F-1"><span class="toc-number">5.2.</span> <span class="toc-text">KKT条件与支持向量</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%80%E7%BB%88%E7%9A%84%E5%86%B3%E7%AD%96%E5%87%BD%E6%95%B0%EF%BC%88%E8%BD%AF%E9%97%B4%E9%9A%94%E7%89%88%E6%9C%AC%EF%BC%89"><span class="toc-number">6.</span> <span class="toc-text">最终的决策函数（软间隔版本）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BD%AF%E9%97%B4%E9%9A%94-vs-%E7%A1%AC%E9%97%B4%E9%9A%94"><span class="toc-number">6.1.</span> <span class="toc-text">软间隔 vs.硬间隔</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0%EF%BC%88Kernel-Function%EF%BC%89"><span class="toc-number">7.</span> <span class="toc-text">核函数（Kernel Function）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%8E%E7%BB%B4%E4%B8%8D%E5%8F%AF%E5%88%86%E4%B8%8E%E9%AB%98%E7%BB%B4%E6%98%A0%E5%B0%84"><span class="toc-number">7.1.</span> <span class="toc-text">低维不可分与高维映射</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E6%8A%80%E5%B7%A7%EF%BC%88Kernel-Trick%EF%BC%89"><span class="toc-number">7.2.</span> <span class="toc-text">核技巧（Kernel Trick）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="toc-number">7.3.</span> <span class="toc-text">常用核函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E5%8F%96%E5%8E%9F%E5%88%99"><span class="toc-number">7.4.</span> <span class="toc-text">核函数的选取原则</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A1%AC%E9%97%B4%E9%9A%94%E3%80%81%E8%BD%AF%E9%97%B4%E9%9A%94%E3%80%81%E6%A0%B8%E5%87%BD%E6%95%B0%E5%AF%B9%E6%AF%94"><span class="toc-number">8.</span> <span class="toc-text">硬间隔、软间隔、核函数对比</span></a></li></ol>
    </nav>
  </div>
</aside>

<main class="main" role="main">
  <div class="content">
  <article id="post-机器学习/SVM支持向量机" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      SVM支持向量机
    </h1>
  

      
      <div class="article-meta">
        <!-- <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/cuid9ineUcg_C5I5d5AUzIiOJ" class="article-date">
	  <time datetime="2026-02-22T09:48:39.749Z" itemprop="datePublished">2026-02-22</time>
	</a>
</span> -->
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/ml/">机器学习</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/SVM/" rel="tag">SVM</a>, <a class="article-tag-link-link" href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E7%A7%AF/" rel="tag">支持向量积</a>, <a class="article-tag-link-link" href="/tags/%E6%A0%B8%E5%87%BD%E6%95%B0/" rel="tag">核函数</a>, <a class="article-tag-link-link" href="/tags/%E8%BD%AF%E9%97%B4%E9%9A%94/" rel="tag">软间隔</a>
  </span>


        

        <!-- <span class="post-comment"><i class="icon icon-comment"></i> <a href="/cuid9ineUcg_C5I5d5AUzIiOJ#comments" class="article-comment-link">评论</a></span> -->
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>支持向量机（Support Vector Machine, SVM）是一种经典的二分类模型，它的核心思想是：<strong>在特征空间中寻找一个决策边界（超平面），使得两类样本被正确分开，并且离决策边界最近的样本点（即支持向量）到该边界的距离尽可能大</strong>。这个距离被称为<strong>间隔（Margin）</strong>，SVM因此也被称为<strong>最大间隔分类器</strong>。</p>
<h1 id="决策函数"><a href="#决策函数" class="headerlink" title="决策函数"></a>决策函数</h1><p>$$
\left\{
(x_1^{(1)}, x_2^{(1)}, \ldots, x_n^{(1)}),
(x_1^{(2)}, x_2^{(2)}, \ldots, x_n^{(2)}),
\ldots,
(x_1^{(m)}, x_2^{(m)}, \ldots, x_n^{(m)})
\right\}
$$</p>
<p>这是一个样本集，包含 $ m $ 个样本，每个样本有 $ n $ 个特征。</p>
<ul>
<li>第 $ i $ 个样本为：$(x_1^{(i)}, x_2^{(i)}, \ldots, x_n^{(i)})$</li>
<li>上标 $(i)$ 表示第 $ i $ 个样本</li>
<li>下标 $ j $ 表示该样本的第 $ j $ 个特征</li>
<li>所以 $ x_j^{(i)} $ 表示 <strong>第 $ i $ 个样本的第 $ j $ 个特征值</strong></li>
</ul>
<p>矩阵表示法(每一行是一个样本，每一列是一个特征)：<br>$$
\mathbf{X} =
\begin{bmatrix}
x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\
x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\
\vdots   & \vdots   & \ddots & \vdots   \\
x_1^{(m)} & x_2^{(m)} & \cdots & x_n^{(m)}
\end{bmatrix}
$$</p>
<p>SVM的决策函数为线性函数：</p>
<p>$$
f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x} + b
$$</p>
<p>其中 $\mathbf{w} \in \mathbb{R}^n$ 是权重向量（法向量），$b \in \mathbb{R}$ 是偏置项。决策边界（超平面）由方程 $\mathbf{w}^\top \mathbf{x} + b = 0$ 定义。</p>
<p>对于二维情况，决策边界是一条直线；对于三维情况，它是一个平面；更高维则称为超平面，如：</p>
<p>当样本有两个特征时（即2维），决策边界为直线：</p>
<img src="/images/v2-319cb9929efe5025d7ee029ac7e60098_r.png" alt="v2-319cb9929efe5025d7ee029ac7e60098_r" style="zoom:40%;" />
$$
w_1x_1 + w_2x_2 + b = 0
$$

<p>当三个（或多个）特征时，决策边界为平面：<br>$$
w_1x_1 + w_2x_2 + w_3x_3 + b = 0
$$</p>
<p>分类规则：把 $x^{(i)}$ 代入决策方程：</p>
<ul>
<li><p>得到的值$\mathbf{w}^\top \mathbf{x} + b > 0$：  $y^{(i)}$ 为正例，$y^{(i)} = +1$； </p>
</li>
<li><p>得到的值$\mathbf{w}^\top \mathbf{x} + b < 0$ ：  $y^{(i)}$ 为负例，$y^{(i)} = -1$；</p>
</li>
</ul>
<p>因此，正确分类的样本满足 $y^{(i)}(\mathbf{w}^\top \mathbf{x}^{(i)} + b) > 0$。</p>
<h1 id="间隔（Margin）与优化目标"><a href="#间隔（Margin）与优化目标" class="headerlink" title="间隔（Margin）与优化目标"></a>间隔（Margin）与优化目标</h1><h2 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h2><h3 id="点到直线的距离"><a href="#点到直线的距离" class="headerlink" title="点到直线的距离"></a>点到直线的距离</h3><p>在二维空间中，假设有一条直线的方程为 $ ax + by + c = 0 $，以及一个点 $P(x_0, y_0) $。该点到直线的距离 $d $ 可以通过以下公式计算：</p>
<p>$$
d = \frac{|ax_0 + by_0 + c|}{\sqrt{a^2 + b^2}}
$$<br>这里，$a, b, c $ 是直线方程的系数，而 $x_0, y_0 $ 是点的坐标。</p>
<h3 id="点到平面的距离"><a href="#点到平面的距离" class="headerlink" title="点到平面的距离"></a>点到平面的距离</h3><p>在三维空间中，假设有一个平面的方程为 $Ax + By + Cz + D = 0 $，以及一个点 $P(x_0, y_0, z_0) $。该点到平面的距离 $d $ 可以通过以下公式计算：</p>
<p>$$
d = \frac{|Ax_0 + By_0 + Cz_0 + D|}{\sqrt{A^2 + B^2 + C^2}}
$$<br>这里，$A, B, C, D $ 是平面方程的系数，而 $x_0, y_0, z_0 $ 是点的坐标。</p>
<p>注：</p>
<ul>
<li>在这两个公式中，分子部分表示的是点代入直线或平面方程后的值的绝对值，它反映了点相对于直线或平面的位置。</li>
<li>分母部分是对直线或平面法向量长度的计算（即向量 $(a, b) $ 或 $(A, B, C) $ 的模长），用于标准化距离。</li>
</ul>
<h2 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h2><p>函数间隔（Functional Margin）的定义</p>
<p>对于一个样本 $(\mathbf{x}^{(i)}, y^{(i)})$，其中 $y^{(i)} \in \{+1, -1\}$，我们定义其<strong>函数间隔</strong>为：</p>
<p>$$
\hat{\gamma}_i = y^{(i)} (\mathbf{w}^\top \mathbf{x}^{(i)} + b)
$$</p>
<ul>
<li>$\mathbf{w}^\top \mathbf{x}^{(i)} + b$ 是模型对样本 $\mathbf{x}^{(i)}$ 的<strong>原始打分（score）</strong>。</li>
<li>如果分类正确$ y^{(i)}$ 与 $ \mathbf{w}^\top \mathbf{x}^{(i)} + b$ 同号，它的绝对值越大，说明分类的置信度越高。</li>
</ul>
<h2 id="几何间隔（优化目标）"><a href="#几何间隔（优化目标）" class="headerlink" title="几何间隔（优化目标）"></a>几何间隔（优化目标）</h2><p>我们以二维决策边界为例：<br>$$
w_1x_1 + w_2x_2 + b = 0，其中 \mathbf{w}=[w_1,w_2]
$$<br>点到决策边界直线的距离为：<br>$$
\gamma_i = d_i= \frac{|w_1x_1^{(i)} + w_2x_1^{(i)} + b|}{\sqrt{w_1^2 + w_2^2}} = y^{(i)} \frac{w_1x_1^{(i)} + w_2x_1^{(i)} + b}{\sqrt{w_1^2 + w_2^2}} = y^{(i)} \frac{\mathbf{wx} + b}{\sqrt{w_1^2 + w_2^2}} =  = y^{(i)} \frac{\mathbf{wx} + b}{\|\mathbf{w}\|}
$$</p>
<p>$\|\mathbf{w}\|$ 为<strong>L2范数</strong>（也称为欧几里得范数）定义为:  $\|\mathbf{w}\| = \sqrt{w_1^2 + w_2^2}$ ， 表示的是<strong>权重向量 $ \mathbf{w} $ 的长度。</strong></p>
<p>所以几何间隔为：<br>$$
\gamma_i = y^{(i)} \frac{\mathbf{wx} + b}{\|\mathbf{w}\|} = \frac{1}{\|\mathbf{w}\|}  y^{(i)}{(\mathbf{wx} + b)} = \frac{1}{\|\mathbf{w}\|} \hat{\gamma}_i
$$<br>我们的优化目标是最大化距离决策边界最小的样本的间隔值。</p>
<h2 id="函数间隔VS几何间隔"><a href="#函数间隔VS几何间隔" class="headerlink" title="函数间隔VS几何间隔"></a>函数间隔VS几何间隔</h2><p>$$
\gamma_i = \frac{\hat{\gamma}_i}{\|\mathbf{w}\|}
$$</p>
<table>
<thead>
<tr>
<th>特性</th>
<th>函数间隔 $\hat{\gamma}_i$</th>
<th>几何间隔 $\gamma_i$</th>
</tr>
</thead>
<tbody><tr>
<td>是否依赖 $\mathbf{w}$ 的尺度</td>
<td>是（可任意缩放）</td>
<td>否（尺度不变）</td>
</tr>
<tr>
<td>是否表示真实距离</td>
<td>否</td>
<td>是</td>
</tr>
<tr>
<td>优化目标</td>
<td>不直接用于最大化（因可缩放）</td>
<td>用于 SVM 的核心目标：<strong>最大化最小几何间隔</strong></td>
</tr>
</tbody></table>
<p><strong>示例</strong>：</p>
<p>假设 $\mathbf{w} = (2, 2), b = -2$，对某个样本 $\mathbf{x} = (1,1), y=+1$：</p>
<ul>
<li>函数间隔：$\hat{\gamma} = 1 \cdot (2*1 + 2*1 - 2) = 2$</li>
<li>几何间隔：$\gamma = 2 / \sqrt{2^2 + 2^2} = 2 / \sqrt{8} = \frac{1}{\sqrt{2}}$</li>
</ul>
<p>现在把 $\mathbf{w}, b$ 都乘以 10：</p>
<ul>
<li>$\mathbf{w}' = (20,20), b' = -20$</li>
<li>函数间隔变为：$20$（变大了！）</li>
<li>但几何间隔仍是：$20 / \sqrt{800} = 20 / (10\sqrt{8}) = 2 / \sqrt{8} = \frac{1}{\sqrt{2}}$</li>
</ul>
<p><strong>几何间隔不变，函数间隔随参数缩放而变</strong>。</p>
<h2 id="最大间隔思想"><a href="#最大间隔思想" class="headerlink" title="最大间隔思想"></a>最大间隔思想</h2><p>SVM的目标是：<strong>找到一个超平面 $(\mathbf{w},b)$，使得所有样本中离它最近的那个样本的几何间隔尽可能大</strong>。这个最近的样本点就是<strong>支持向量（Support Vector）</strong>。</p>
<p>整个数据集的几何间隔定义为：</p>
<p>$$
\gamma = \min_{i=1,\dots,m} \gamma_i
$$</p>
<p>优化问题即为：</p>
<p>$$
\max_{\mathbf{w},b} \gamma = \max_{\mathbf{w},b} \min_{i} \frac{y^{(i)}(\mathbf{w}^\top \mathbf{x}^{(i)} + b)}{\|\mathbf{w}\|}
$$</p>
<h2 id="目标函数与化简"><a href="#目标函数与化简" class="headerlink" title="目标函数与化简"></a>目标函数与化简</h2><p>SVM 的核心思想是：<strong>找到一个决策边界，使得所有样本中离它最近的那个样本（即支持向量）的距离尽可能大</strong>。</p>
<ul>
<li><p>单个样本的几何间隔：<br>$$
  \gamma_i = \frac{1}{\|\mathbf{w}\|} y^{(i)}(\mathbf{w}^\top \mathbf{x}_i + b)
  $$</p>
</li>
<li><p>整个数据集的几何间隔是所有样本中<strong>最小的那个</strong>：<br>$$
   \gamma = \min_{i=1,\dots,m} \gamma_i 
  $$</p>
</li>
<li><p>我们的目标是找到能最大化这个最小几何间隔的参数：<br>$$
  \max_{\mathbf{w}, b} \gamma = \max_{\mathbf{w}, b} \min_{i=1,\dots,m} \frac{1}{\|\mathbf{w}\|} y^{(i)}(\mathbf{w}^\top \mathbf{x}_i + b)
  $$</p>
</li>
<li><p>对间隔进行<strong>缩放</strong>：</p>
<p>性质：对同一超平面，对参数（$\mathbf{w}$ 和 $b$ ）同时进行缩放，几何间隔保持不变，函数间隔发生变化。</p>
</li>
</ul>
<p>$$
\gamma_i = \frac{1}{\|\mathbf{w}\|} y^{(i)}(\mathbf{w}^\top \mathbf{x}_i + b)
$$</p>
<ul>
<li><p>设对任意正实数 $c>0$，把参数变为 $(\mathbf w',b')=(c\mathbf w,c b)$。</p>
<ul>
<li><p>超平面方程由 $\mathbf w^\top \mathbf x + b =0$ 变为 $(c\mathbf w)^\top \mathbf x + c b = c(\mathbf w^\top \mathbf x + b)$。零点集不变，因此<strong>超平面本身不变</strong>（同一条直线&#x2F;平面）。</p>
</li>
<li><p>函数边距变为<br>$$
    \hat\gamma_i' = y_i((c\mathbf w)^\top \mathbf x_i + c b) = c, y_i(\mathbf w^\top \mathbf x_i + b) = c,\hat\gamma_i.
    $$<br>所以函数边距按同样因子 (c) 缩放。</p>
</li>
<li><p>$\mathbf w 的范数变成 |\mathbf w'|=|c\mathbf w| = c|\mathbf w|$。</p>
</li>
<li><p>几何边距变为<br>$$
    \gamma_i' = \frac{\hat\gamma_i'}{|\mathbf w'|} = \frac{c\hat\gamma_i}{c|\mathbf w|} = \frac{\hat\gamma_i}{|\mathbf w|} = \gamma_i.
    $$</p>
</li>
<li><p>也就是说，<strong>几何边距不变，函数间隔变为原来的c倍</strong>。</p>
</li>
<li><p>我们对 $\max_{\mathbf{w}, b} \gamma = \max_{\mathbf{w}, b} \min_{i=1,\dots,m} \frac{1}{\|\mathbf{w}\|} y^{(i)}(\mathbf{w}^\top \mathbf{x}_i + b)$ 中的$\frac{1}{\|\mathbf{w}\|} y^{(i)}(\mathbf{w}^\top \mathbf{x}_i + b)$  进行缩放（不会改变原来的结果值）,缩放到 $y^{(i)}(\mathbf{w}^\top \mathbf{x}_i + b) \ge 1$，此时：<br>$$
    \begin{aligned}
    &\max_{\mathbf{w}, b} \gamma = \max_{\mathbf{w}, b} \min_{i=1,\dots,m} \frac{1}{\|\mathbf{w}\|} y^{(i)}(\mathbf{w}^\top \mathbf{x}_i + b) =  \max_{\mathbf{w}, b} \min_{i=1,\dots,m} \frac{1}{\|\mathbf{w}\|} \cdot 1 = \max_{\mathbf{w}, b}\frac{1}{\|\mathbf{w}\|} = \min_{\mathbf{w}, b}{\|\mathbf{w}\|} =  \min_{\mathbf{w}, b}\frac{1}{2}{\|\mathbf{w}\|}^2 \\
    &\text{s.t.}\quad y_i(\mathbf w^\top \mathbf x_i + b) \ge 1,\quad i=1,\dots,m.
    \end{aligned}
    $$<br>所以，<strong>SVM优化目标的的原始形式为：</strong><br>$$
    \begin{aligned}
    &\min_{\mathbf{w}, b}\frac{1}{2}{\|\mathbf{w}\|}^2 \\
    &\text{s.t.}\quad y_i(\mathbf w^\top \mathbf x_i + b) \ge 1,\quad i=1,\dots,m.
    \end{aligned}
    $$<br>加入软间隔，在目标里加上松弛项并<strong>引入松弛变量 $\xi_i$ 的优化目标形式为</strong>：<br>$$
    \begin{aligned}
    &\min \tfrac{1}{2}|\mathbf w|^2 + C\sum_{i=1}^n\xi_i \\
    &\text{s.t.}\quad y_i(\mathbf w^\top \mathbf x_i + b) \ge 1-\xi_i,\ \xi_i\ge 0, \quad i=1,\dots,m.
    \end{aligned}
    $$</p>
</li>
</ul>
</li>
</ul>
<h2 id="硬间隔与软间隔"><a href="#硬间隔与软间隔" class="headerlink" title="硬间隔与软间隔"></a>硬间隔与软间隔</h2><img src="/images/ScreenShot_2025-10-25_161439_348.png" alt="ScreenShot_2025-10-25_161439_348" style="zoom: 25%;" />

<h3 id="硬间隔"><a href="#硬间隔" class="headerlink" title="硬间隔"></a>硬间隔</h3><ul>
<li><strong>前提</strong>：数据完全线性可分。</li>
<li><strong>目标</strong>：找到一个超平面，使得所有样本被正确分类，且间隔最大。</li>
<li><strong>数学形式</strong>：</li>
</ul>
<p>$$
\begin{aligned}
  &\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \\
  &\text{s.t. } y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1,\quad i=1,\dots,m
  \end{aligned}
$$</p>
<p>  其中 $y_i \in \{-1, +1\}$</p>
<h3 id="软间隔"><a href="#软间隔" class="headerlink" title="软间隔"></a>软间隔</h3><ul>
<li><p><strong>现实情况</strong>：数据可能含噪声或轻微不可分。</p>
</li>
<li><p>引入<strong>松弛变量 $\xi_i \geq 0$</strong>，允许部分样本违反约束。</p>
</li>
<li><p><strong>目标函数加入正则项</strong>（控制间隔与误分类的权衡）：<br>$$
  \begin{aligned}
  &\min_{\mathbf{w}, b, \xi} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^m \xi_i \\
  &\text{s.t. } y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i,\quad \xi_i \geq 0
  \end{aligned}
  $$</p>
<ul>
<li>$C$ 是<strong>惩罚系数</strong>：$C$ 越大，对误分类越敏感（趋向硬间隔）；$C$ 小则容忍更多误分类。</li>
<li>如果 $ ξ_i = 0 $，点被正确分类且在间隔之外。</li>
<li>如果 $ 0 < ξ_i <= 1 $，点被正确分类但在间隔之内。</li>
<li>如果 $ ξ_i > 1 $，点被错误分类。</li>
</ul>
</li>
</ul>
<h1 id="拉格朗日对偶求解（硬间隔）"><a href="#拉格朗日对偶求解（硬间隔）" class="headerlink" title="拉格朗日对偶求解（硬间隔）"></a>拉格朗日对偶求解（硬间隔）</h1><h2 id="拉格朗日函数"><a href="#拉格朗日函数" class="headerlink" title="拉格朗日函数"></a>拉格朗日函数</h2><p>上述我们已经得到SVM的优化目标函数为:<br>$$
\begin{aligned}
  &\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \\
  &\text{s.t. } y_i(\mathbf{w}^\top \mathbf{x}_i + b) - 1 \geq 0,\quad i=1,\dots,m
  \end{aligned}
$$<br>目标函数是一个带不等式约束的凸二次规划（QP）问题。我们可以用拉格朗日乘子法将其转化为一个更易于求解的对偶问题。</p>
<p><strong>步骤 1： 引入拉格朗日函数</strong></p>
<p>为每个不等式约束引入一个拉格朗日乘子 $\alpha_i \ge 0$，构建拉格朗日函数：<br>$$
\mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha}) = \frac{1}{2} \|\mathbf{w}\|^2 - \sum_{i=1}^m \alpha_i \left[ y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 \right]
$$</p>
<ul>
<li>$\boldsymbol{\alpha} = (\alpha_1, \dots, \alpha_m)^T$</li>
<li>从上述式子不难看出：$\mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha}) \leq \frac{1}{2} \|\mathbf{w}\|^2$</li>
</ul>
<p>根据拉格朗日乘子法的性质，原始问题等价于以下<strong>无约束的极小极大问题</strong>，所以目标函数转换为：<br>$$
\min_{\mathbf{w}, b} \max_{\boldsymbol{\alpha} \ge 0} \mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha})
$$<br><strong>步骤 2： 转化为对偶问题（Dual Problem）</strong></p>
<p>原始问题是 $\min_{\mathbf{w}, b} \max_{\boldsymbol{\alpha} \ge 0} \mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha})$。在满足KKT条件的情况下，我们可以等价地求解它的对偶问题：<br>$$
\min_{\mathbf{w}, b} \max_{\boldsymbol{\alpha} \ge 0} \mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha})
\ → \
\max_{\boldsymbol{\alpha} \ge 0} \min_{\mathbf{w}, b} \mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha})
$$</p>
<p><strong>首先，求解内层的最小化问题：</strong> $\min_{\mathbf{w}, b} \mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha})$</p>
<p>分别对 $\mathbf{w}$ 和 $b$ 求偏导并令其为零：<br>$$
\nabla_{\mathbf{w}} \mathcal{L} = \mathbf{w} - \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i = 0 \quad \Rightarrow \quad \mathbf{w} = \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i
$$</p>
<p>$$
\nabla_{\mathbf{b}} \mathcal{L} = \frac{\partial \mathcal{L}}{\partial b} = - \sum_{i=1}^m \alpha_i y_i = 0 \quad \Rightarrow \quad \sum_{i=1}^m \alpha_i y_i = 0
$$</p>
<p>将这两个结果代回拉格朗日函数 $\mathcal{L}$：</p>
<p>$$
\begin{aligned}
\mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha}) &= \frac{1}{2} \left( \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i \right)^T \left( \sum_{j=1}^m \alpha_j y_j \mathbf{x}_j \right) - \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i^T \left( \sum_{j=1}^m \alpha_j y_j \mathbf{x}_j \right) - b \sum_{i=1}^m \alpha_i y_i + \sum_{i=1}^m \alpha_i \\
&= \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j - \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j + \sum_{i=1}^m \alpha_i \\
&= -\frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j + \sum_{i=1}^m \alpha_i
\end{aligned}
$$<br>（注意：中间项 $- b \sum_{i=1}^m \alpha_i y_i$ 因为 $\sum \alpha_i y_i = 0$ 而消失了。）</p>
<p><strong>步骤 3： 得到对偶问题</strong></p>
<p>现在，我们的目标变成了最大化上面这个结果，同时要满足约束：<br>$$
\begin{aligned}
& \max_{\boldsymbol{\alpha}} \quad -\frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j + \sum_{i=1}^m \alpha_i \\
& \text{s.t.} \quad \sum_{i=1}^m \alpha_i y_i = 0, \\
& \quad \quad \alpha_i \ge 0, \quad i = 1, \dots, m
\end{aligned}
$$<br>通常我们将最大化转换为最小化，得到SVM标准对偶问题的最终形式：</p>
<p>$$
\begin{aligned}
& \min_{\boldsymbol{\alpha}} \quad \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j - \sum_{i=1}^m \alpha_i \\
& \text{s.t.} \quad \sum_{i=1}^m \alpha_i y_i = 0, \\
& \quad \quad \alpha_i \ge 0, \quad i = 1, \dots, m
\end{aligned}
$$</p>
<p>然后就是根据所有样本的值带入上式（还有约束条件），求得 $\alpha$，再根据 $w$ 与$\alpha$的关系求得 $w$值。</p>
<p>在根据 $y = \mathbf{w}^\top \mathbf{x}_i + b$  随边找一个样本点带入求得 $b$ 值。</p>
<p><strong>$\alpha$的意义：</strong></p>
<p>所有$\alpha_i$ 不为零的点就是支持向量，也就是位于边界上的点，非边界上的点$\alpha = 0$ </p>
<h2 id="KKT条件与支持向量"><a href="#KKT条件与支持向量" class="headerlink" title="KKT条件与支持向量"></a>KKT条件与支持向量</h2><p>求解上述对偶问题后，我们得到最优的拉格朗日乘子 $\boldsymbol{\alpha}^*$。此时，KKT条件（最优解必须满足的条件）起着关键作用，特别是其中的<strong>互补松弛条件</strong>：<br>$$
\alpha_i^* \left[ y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 \right] = 0, \quad \forall i
$$<br>这个条件意味着：</p>
<ol>
<li>如果 $\alpha_i^* = 0$，那么对应的样本点不会对 $\mathbf{w}$ 的计算产生影响（见 $\mathbf{w} = \sum \alpha_i y_i \mathbf{x}_i$）。</li>
<li>如果 $\alpha_i^* > 0$，那么必然有 $y_i (\mathbf{w}^T \mathbf{x}_i + b) = 1$。这意味着这个样本点<strong>正好在最大间隔边界上</strong>！</li>
</ol>
<p>这些 $\alpha_i^* > 0$ 所对应的样本点，就是<strong>支持向量（Support Vectors）</strong>。它们决定了最终的超平面。</p>
<p><strong>求解 $\mathbf{w}$ 和 $b$：</strong></p>
<ul>
<li><p>$\mathbf{w}$： 我们已经从导数为零的条件得到了：<br>$$
  \mathbf{w}^* = \sum_{i=1}^m \alpha_i^* y_i \mathbf{x}_i
  $$<br>实际上，我们只需要支持向量（即 $\alpha_i^* > 0$ 的点）来计算：<br>$$
  \mathbf{w}^* = \sum_{i \in SV} \alpha_i^* y_i \mathbf{x}_i
  $$</p>
</li>
<li><p>$b$： 利用任何一个支持向量 $(\mathbf{x}_s, y_s)$（即满足 $\alpha_s^* > 0$ 的点），根据互补松弛条件：<br>$$
  y_s (\mathbf{w}^{*T} \mathbf{x}_s + b^*) = 1
  $$<br>可以解出：<br>$$
  b^* = y_s - \mathbf{w}^{*T} \mathbf{x}_s
  $$<br>为了数值稳定性，通常使用所有支持向量计算出的 $b$ 的平均值。</p>
</li>
</ul>
<h1 id="拉格朗日对偶与求解（软间隔）"><a href="#拉格朗日对偶与求解（软间隔）" class="headerlink" title="拉格朗日对偶与求解（软间隔）"></a>拉格朗日对偶与求解（软间隔）</h1><h2 id="松弛变量与惩罚参数"><a href="#松弛变量与惩罚参数" class="headerlink" title="松弛变量与惩罚参数"></a>松弛变量与惩罚参数</h2><p><strong>优化问题（原始问题，Primal Problem with Soft Margin）：</strong></p>
<p>由于数据可能不是线性可分的，我们引入<strong>松弛变量（Slack Variables）</strong> $\xi_i \ge 0$ 来允许一些样本犯错误。同时，在目标函数中加入对这些错误的惩罚项。优化问题变为：</p>
<p>$$
\begin{aligned}
& \min_{\mathbf{w}, b, \boldsymbol{\xi}} \quad \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^m \xi_i \\
& \text{s.t.} \quad y_i (\mathbf{w}^T \mathbf{x}_i + b) \ge 1 - \xi_i, \quad i = 1, \dots, m \\
& \quad \quad \xi_i \ge 0, \quad i = 1, \dots, m
\end{aligned}
$$</p>
<p>这里：</p>
<ul>
<li>$\xi_i$ 是松弛变量。$\xi_i = 0$ 表示样本 $x_i$ 被正确分类且位于间隔边界之外；$0 < \xi_i \le 1$ 表示样本位于间隔内部，但在正确的一侧；$\xi_i > 1$ 表示样本被错误分类。</li>
<li>$C > 0$ 是<strong>惩罚参数</strong>，由用户指定。它控制了我们对分类错误的容忍度：<ul>
<li>$C$ 越大，对错误的惩罚越重，间隔带越“硬”，可能过拟合。</li>
<li>$C$ 越小，对错误的惩罚越轻，间隔带越“宽软”，可能欠拟合。</li>
</ul>
</li>
</ul>
<p><strong>构建拉格朗日函数：</strong></p>
<p>我们现在有两个不等式约束，因此引入两组拉格朗日乘子：$\alpha_i \ge 0$ 和 $\mu_i \ge 0$。</p>
<p>$$
\mathcal{L}(\mathbf{w}, b, \boldsymbol{\xi}, \boldsymbol{\alpha}, \boldsymbol{\mu}) = \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^m \xi_i - \sum_{i=1}^m \alpha_i \left[ y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 + \xi_i \right] - \sum_{i=1}^m \mu_i \xi_i
$$</p>
<p><strong>转化为对偶问题：</strong></p>
<p>同样，我们求解 $\min_{\mathbf{w}, b, \boldsymbol{\xi}} \max_{\boldsymbol{\alpha}, \boldsymbol{\mu} \ge 0} \mathcal{L}$ 的对偶问题 $\max_{\boldsymbol{\alpha}, \boldsymbol{\mu} \ge 0} \min_{\mathbf{w}, b, \boldsymbol{\xi}} \mathcal{L}$。</p>
<ol>
<li><p><strong>对 $\mathbf{w}, b, \xi_i$ 求偏导并令为零：</strong><br>$$
   \nabla_{\mathbf{w}} \mathcal{L} = \mathbf{w} - \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i = 0 \quad \Rightarrow \quad \mathbf{w} = \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i
   $$</p>
<p>$$
   \frac{\partial \mathcal{L}}{\partial b} = - \sum_{i=1}^m \alpha_i y_i = 0 \quad \Rightarrow \quad \sum_{i=1}^m \alpha_i y_i = 0
   $$</p>
<p>$$
   \frac{\partial \mathcal{L}}{\partial \xi_i} = C - \alpha_i - \mu_i = 0 \quad \Rightarrow \quad \alpha_i + \mu_i = C
   $$</p>
</li>
<li><p><strong>将结果代回拉格朗日函数：</strong><br>将 $\mathbf{w} = \sum \alpha_i y_i \mathbf{x}_i$， $\mu_i = C - \alpha_i$ 代入 $\mathcal{L}$。经过与硬间隔类似的化简过程（注意包含 $\xi_i$ 的项会相互抵消），我们得到：</p>
</li>
</ol>
<p><strong>对偶问题（Dual Problem with Soft Margin）：</strong><br>$$
\boxed{
\begin{aligned}
& \min_{\boldsymbol{\alpha}} \quad \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j - \sum_{i=1}^m \alpha_i \\
& \text{s.t.} \quad \sum_{i=1}^m \alpha_i y_i = 0, \\
& \quad \quad 0 \le \alpha_i \le C, \quad i = 1, \dots, m
\end{aligned}
}
$$</p>
<p><strong>关键变化：</strong> 与硬间隔相比，软间隔的对偶问题唯一的变化是约束条件从 $\alpha_i \ge 0$ 变成了 $0 \le \alpha_i \le C$。这个上界 $C$ 来自于关系式 $\alpha_i = C - \mu_i$ 和 $\mu_i \ge 0$。</p>
<hr>
<h2 id="KKT条件与支持向量-1"><a href="#KKT条件与支持向量-1" class="headerlink" title="KKT条件与支持向量"></a>KKT条件与支持向量</h2><p>对于软间隔SVM，KKT条件（最优解必须满足的条件）变得更加丰富，它们完整地描述了支持向量的不同类型。</p>
<p><strong>KKT条件：</strong></p>
<ol>
<li><p><strong>平稳性：</strong> $\mathbf{w} = \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i$, $\sum_{i=1}^m \alpha_i y_i = 0$, $\alpha_i + \mu_i = C$.</p>
</li>
<li><p><strong>原始可行性：</strong> $y_i (\mathbf{w}^T \mathbf{x}_i + b) \ge 1 - \xi_i$, $\xi_i \ge 0$.</p>
</li>
<li><p><strong>对偶可行性：</strong> $\alpha_i \ge 0$, $\mu_i \ge 0$.</p>
</li>
<li><p><strong>互补松弛性：</strong><br>$$
   \alpha_i \left[ y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 + \xi_i \right] = 0
   $$</p>
<p>$$
   \mu_i \xi_i = 0
   $$</p>
</li>
</ol>
<p>根据这些KKT条件，我们可以将样本点分为三类：</p>
<table>
<thead>
<tr>
<th align="left">$\alpha_i$ 的值</th>
<th align="left">$\xi_i$ 的值</th>
<th align="left">几何位置</th>
<th align="left">类型</th>
</tr>
</thead>
<tbody><tr>
<td align="left">$\alpha_i = 0$</td>
<td align="left">$\xi_i = 0$</td>
<td align="left">被正确分类，<strong>在间隔边界之外</strong></td>
<td align="left"><strong>非支持向量</strong></td>
</tr>
<tr>
<td align="left">$0 < \alpha_i < C$</td>
<td align="left">$\xi_i = 0$</td>
<td align="left">恰好落在<strong>间隔边界</strong>上 ($y_i(\mathbf{w}^T\mathbf{x}_i + b) = 1$)</td>
<td align="left"><strong>标准支持向量</strong></td>
</tr>
<tr>
<td align="left">$\alpha_i = C$</td>
<td align="left">$0 < \xi_i \le 1$</td>
<td align="left">位于<strong>间隔内部</strong>，但在正确的一侧</td>
<td align="left"><strong>边界支持向量</strong></td>
</tr>
<tr>
<td align="left">$\alpha_i = C$</td>
<td align="left">$\xi_i > 1$</td>
<td align="left">在<strong>错误的一侧</strong>，被误分类</td>
<td align="left"><strong>误分类支持向量</strong></td>
</tr>
</tbody></table>
<p><strong>求解 $\mathbf{w}$ 和 $b$：</strong></p>
<ul>
<li><p>$\mathbf{w}^*$ 的求解与硬间隔相同，只依赖于支持向量 ($\alpha_i > 0$)：<br>$$
  \mathbf{w}^* = \sum_{i=1}^m \alpha_i^* y_i \mathbf{x}_i = \sum_{i \in SV} \alpha_i^* y_i \mathbf{x}_i
  $$</p>
</li>
<li><p>$b^*$ 的求解：在硬间隔中，我们使用任意一个支持向量。在软间隔中，为了数值稳定性，我们通常使用<strong>所有标准支持向量</strong> ($0 < \alpha_i < C$) 来计算 $b$ 的平均值。<br>对于任何一个标准支持向量，由于 $\xi_i = 0$ 且 $0 < \alpha_i < C$，根据互补松弛条件有：<br>$$
  y_i (\mathbf{w}^{*T} \mathbf{x}_i + b^*) = 1
  $$<br>因此，<br>$$
  b^* = \frac{1}{|S|} \sum_{i \in S} (y_i - \mathbf{w}^{*T} \mathbf{x}_i)
  $$<br>其中 $S = \{ i | 0 < \alpha_i < C \}$ 是所有标准支持向量的集合。</p>
</li>
</ul>
<hr>
<h1 id="最终的决策函数（软间隔版本）"><a href="#最终的决策函数（软间隔版本）" class="headerlink" title="最终的决策函数（软间隔版本）"></a>最终的决策函数（软间隔版本）</h1><p>软间隔SVM的最终决策函数在形式上与硬间隔完全相同：</p>
<p>$$
f(\mathbf{x}) = \text{sign}(\mathbf{w}^{*T} \mathbf{x} + b^*)
$$</p>
<p>将 $\mathbf{w}^* = \sum_{i \in SV} \alpha_i^* y_i \mathbf{x}_i$ 代入：</p>
<p>$$
\boxed{
f(\mathbf{x}) = \text{sign}\left( \sum_{i \in SV} \alpha_i^* y_i \mathbf{x}_i^T \mathbf{x} + b^* \right)
}
$$</p>
<p><strong>关键点：</strong></p>
<ol>
<li><strong>形式不变</strong>：决策函数仍然是支持向量的线性组合。</li>
<li><strong>支持向量更多样</strong>：支持向量集合 $SV$ 现在包含了所有 $\alpha_i > 0$ 的样本，即标准支持向量和那些被允许“犯错”的边界&#x2F;误分类支持向量。正是这些“犯错”的样本使得模型获得了鲁棒性。</li>
<li><strong>核函数应用</strong>：同样，我们可以将内积 $\mathbf{x}_i^T \mathbf{x}$ 替换为核函数 $K(\mathbf{x}_i, \mathbf{x})$，从而将软间隔SVM应用于非线性分类问题，这被称为<strong>非线性软间隔SVM</strong>。</li>
</ol>
<h2 id="软间隔-vs-硬间隔"><a href="#软间隔-vs-硬间隔" class="headerlink" title="软间隔 vs.硬间隔"></a>软间隔 vs.硬间隔</h2><table>
<thead>
<tr>
<th align="left">特性</th>
<th align="left">硬间隔SVM</th>
<th align="left">软间隔SVM</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>适用场景</strong></td>
<td align="left">数据严格线性可分</td>
<td align="left">数据近似线性可分或存在噪声</td>
</tr>
<tr>
<td align="left"><strong>优化目标</strong></td>
<td align="left">$\min \frac{1}{2}\|\mathbf{w}\|^2$</td>
<td align="left">$\min \frac{1}{2}\|\mathbf{w}\|^2 + C \sum \xi_i$</td>
</tr>
<tr>
<td align="left"><strong>对偶约束</strong></td>
<td align="left">$0 \le \alpha_i$</td>
<td align="left">$0 \le \alpha_i \le C$</td>
</tr>
<tr>
<td align="left"><strong>支持向量</strong></td>
<td align="left">都在间隔边界上</td>
<td align="left">在间隔边界上、内部或误分点</td>
</tr>
<tr>
<td align="left"><strong>参数</strong></td>
<td align="left">无</td>
<td align="left">惩罚参数 $C$</td>
</tr>
</tbody></table>
<h1 id="核函数（Kernel-Function）"><a href="#核函数（Kernel-Function）" class="headerlink" title="核函数（Kernel Function）"></a>核函数（Kernel Function）</h1><h2 id="低维不可分与高维映射"><a href="#低维不可分与高维映射" class="headerlink" title="低维不可分与高维映射"></a>低维不可分与高维映射</h2><img src="/images/ScreenShot_2026-02-22_185554_909.png" alt="ScreenShot_2026-02-22_185554_909" style="zoom:40%;" />

<p>当数据在原始特征空间中线性不可分时，一个自然的想法是将数据映射到更高维的空间，使其变得线性可分。设映射函数为 $\phi(\mathbf{x})$，则在高维空间中的决策函数为：<br>$$
f(\mathbf{x}) = \mathbf{w}^\top \phi(\mathbf{x}) + b
$$</p>
<h2 id="核技巧（Kernel-Trick）"><a href="#核技巧（Kernel-Trick）" class="headerlink" title="核技巧（Kernel Trick）"></a>核技巧（Kernel Trick）</h2><p>直接计算 $\phi(\mathbf{x})$ 可能非常复杂，甚至无限维。但注意到在对偶问题中，所有涉及样本的地方都是以内积形式出现：$(\mathbf{x}^{(i)})^\top \mathbf{x}^{(j)}$。如果存在一个函数 $K(\cdot,\cdot)$，使得：</p>
<p>$$
K(\mathbf{x}^{(i)}, \mathbf{x}^{(j)}) = \phi(\mathbf{x}^{(i)})^\top \phi(\mathbf{x}^{(j)})
$$</p>
<p>那么我们就可以<strong>在低维空间中计算核函数 $K$</strong>，而无需显式地计算高维映射 $\phi$，从而大大降低计算量。这就是<strong>核技巧</strong>。</p>
<h2 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h2><ul>
<li><strong>线性核</strong>：$K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^\top \mathbf{x}_j$（即无映射，用于线性可分情况）</li>
<li><strong>多项式核</strong>：$K(\mathbf{x}_i, \mathbf{x}_j) = (\gamma \mathbf{x}_i^\top \mathbf{x}_j + r)^d$</li>
<li><strong>高斯核（RBF核）</strong>：$K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2)$（最常用，可将特征映射到无穷维）</li>
<li><strong>Sigmoid核</strong>：$K(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\gamma \mathbf{x}_i^\top \mathbf{x}_j + r)$</li>
</ul>
<p>核函数的选择通常依赖领域知识或交叉验证。</p>
<h2 id="核函数的选取原则"><a href="#核函数的选取原则" class="headerlink" title="核函数的选取原则"></a>核函数的选取原则</h2><ul>
<li>若特征数 $n$ 很大，而样本数 $m$ 较小，通常使用线性核（避免过拟合）；</li>
<li>若 $n$ 适中，$m$ 也适中，可尝试高斯核；</li>
<li>若 $n$ 很小，$m$ 很大，需要先手工增加特征，再考虑线性核。</li>
</ul>
<hr>
<h1 id="硬间隔、软间隔、核函数对比"><a href="#硬间隔、软间隔、核函数对比" class="headerlink" title="硬间隔、软间隔、核函数对比"></a>硬间隔、软间隔、核函数对比</h1><table>
<thead>
<tr>
<th>特性</th>
<th>硬间隔SVM</th>
<th>软间隔SVM</th>
<th>核SVM</th>
</tr>
</thead>
<tbody><tr>
<td>适用场景</td>
<td>数据严格线性可分</td>
<td>数据近似线性可分或含噪声</td>
<td>非线性可分数据</td>
</tr>
<tr>
<td>优化目标</td>
<td>$\min \frac12\|\mathbf{w}\|^2$</td>
<td>$\min \frac12\|\mathbf{w}\|^2 + C\sum\xi_i$</td>
<td>在核空间中间接优化</td>
</tr>
<tr>
<td>对偶约束</td>
<td>$\alpha_i \ge 0$</td>
<td>$0 \le \alpha_i \le C$</td>
<td>同上，但内积换为核函数</td>
</tr>
<tr>
<td>支持向量</td>
<td>仅在间隔边界上</td>
<td>包括边界、内部、误分类点</td>
<td>核空间中的支持向量</td>
</tr>
<tr>
<td>关键参数</td>
<td>无</td>
<td>惩罚参数 $C$</td>
<td>核参数（如 $\gamma$）及 $C$</td>
</tr>
</tbody></table>
<p>SVM通过最大化间隔获得良好的泛化能力，引入软间隔和核函数后能处理各种复杂数据，是机器学习中非常经典的算法。理解其背后的对偶推导和KKT条件，有助于深入掌握支持向量机的精髓。</p>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="https://www.hicode365.com/cuid9ineUcg_C5I5d5AUzIiOJ" title="SVM支持向量机" target="_blank" rel="external">https://www.hicode365.com/cuid9ineUcg_C5I5d5AUzIiOJ</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://www.hicode365.com/" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.png" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://www.hicode365.com/" target="_blank"><span class="text-dark">hicode</span><small class="ml-1x">Algorithms Engineer</small></a></h3>
        <div>Open shared and free</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    
    <li class="next">
      <a href="/cuidh5iGsdDS3Iq6RawHOWB5J" title="机器学习导论"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
    <li class="toggle-toc">
      <a class="toggle-btn " data-toggle="collapse" href="#collapseToc" aria-expanded="false" title="文章目录" role="button">    <span>[&nbsp;</span><span>文章目录</span>
        <i class="text-collapsed icon icon-anchor"></i>
        <i class="text-in icon icon-close"></i>
        <span>]</span>
      </a>
    </li>
    
  </ul>
  
  
  <!-- Button trigger modal -->
  <button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button>
  <!-- <div class="wave-icon wave-icon-danger btn-donate" data-toggle="modal" data-target="#donateModal">
    <div class="wave-circle"><span class="icon"><i class="icon icon-bill"></i></span></div>
  </div> -->
  
  
  <div class="bar-right">
    
  </div>
  </div>
</nav>
  
<!-- Modal -->
<div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog">
  <div class="modal-dialog" role="document">
    <div class="modal-content donate">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
      <div class="modal-body">
        <div class="donate-box">
          <div class="donate-head">
            <p>感谢您的支持，我会继续努力的!</p>
          </div>
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade active in" id="alipay">
              <div class="donate-payimg">
                <img src="/images/donate/alipay.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p>
            </div>
            <div role="tabpanel" class="tab-pane fade" id="wechatpay">
              <div class="donate-payimg">
                <img src="/images/donate/wechatpay.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p>
            </div>
          </div>
          <div class="donate-footer">
            <ul class="nav nav-tabs nav-justified" role="tablist">
              <li role="presentation" class="active">
                <a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a>
              </li>
              <li role="presentation" class="">
                <a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>



</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
    <div class="copyright">
    	
        <!-- 隐藏主题版权信息 -->
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>



    <script>
(function ($) {
    $('.search-form').on('submit', function (e) {
        var keyword = $('.search-form-input[name="wd"]').val();
        window.location = 'https://www.baidu.com/s?wd=site:www.hicode365.com ' + keyword;
        return false;
    });
})(jQuery);
</script>




   




   


   <script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$']],
    displayMath: [['$$', '$$']],
    processEscapes: true,
    tags: 'ams'
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
};
</script>
<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



  <script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.js"></script>
  <script>
  //利用 FancyBox 实现点击图片放大
  $(document).ready(function() {
    $('article img').not('[hidden]').not('.panel-body img').each(function() {
      var $image = $(this);
      var imageCaption = $image.attr('alt');
      var $imageWrapLink = $image.parent('a');
      if ($imageWrapLink.length < 1) {
        var src = this.getAttribute('src');
        var idx = src.lastIndexOf('?');
        if (idx != -1) {
          src = src.substring(0, idx);
        }
        $imageWrapLink = $image.wrap('<a href="' + src + '"></a>').parent('a');
      }
      $imageWrapLink.attr('data-fancybox', 'images');
      if (imageCaption) {
        $imageWrapLink.attr('data-caption', imageCaption);
      }
    });
    $().fancybox({
      selector: '[data-fancybox="images"]',
      hash: false,
      loop: false,
    });
  });
  </script>





</body>
</html>